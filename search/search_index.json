{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 This repository provides a set of Cloud-Native Network Functions ( CNF ) test cases and the framework to add more test cases. CNF The app (containers/pods/operators) we want to certify according Telco partner/Red Hat\u2019s best practices. TNF /Certification Test Suite The tool we use to certify a CNF . The purpose of the tests and the framework is to test the interaction of CNF with OpenShift Container Platform ( OCP ). Info This test suite is provided for the CNF Developers to test their CNFs readiness for certification. Please see \u201c CNF Developers\u201d for more information. Features The framework generates a report (named claim.json ) as the end of test execution. The catalog of the existing test cases and test building blocks are available in CATALOG.md Architecture \u00b6 There are 3 building blocks in the above framework. the CNF represents the CNF to be certified. The certification suite identifies the resources (containers/pods/operators etc) belonging to the CNF via labels or static data entries in the config file the Certification container/exec is the certification test suite running on the platform or in a container. The executable verifies the CNF under test configuration and its interactions with openshift the Debug pods are part of a Kubernetes daemonset responsible to run various privileged commands on kubernetes nodes. Debug pods are useful to run platform tests and test commands (e.g. ping) in container namespaces without changing the container image content. The debug daemonset is instantiated via the cnf-certification-test-partner repository.","title":"Overview"},{"location":"#overview","text":"This repository provides a set of Cloud-Native Network Functions ( CNF ) test cases and the framework to add more test cases. CNF The app (containers/pods/operators) we want to certify according Telco partner/Red Hat\u2019s best practices. TNF /Certification Test Suite The tool we use to certify a CNF . The purpose of the tests and the framework is to test the interaction of CNF with OpenShift Container Platform ( OCP ). Info This test suite is provided for the CNF Developers to test their CNFs readiness for certification. Please see \u201c CNF Developers\u201d for more information. Features The framework generates a report (named claim.json ) as the end of test execution. The catalog of the existing test cases and test building blocks are available in CATALOG.md","title":"Overview"},{"location":"#architecture","text":"There are 3 building blocks in the above framework. the CNF represents the CNF to be certified. The certification suite identifies the resources (containers/pods/operators etc) belonging to the CNF via labels or static data entries in the config file the Certification container/exec is the certification test suite running on the platform or in a container. The executable verifies the CNF under test configuration and its interactions with openshift the Debug pods are part of a Kubernetes daemonset responsible to run various privileged commands on kubernetes nodes. Debug pods are useful to run platform tests and test commands (e.g. ping) in container namespaces without changing the container image content. The debug daemonset is instantiated via the cnf-certification-test-partner repository.","title":"Architecture"},{"location":"cnf-developers/","text":"CNF Developers Guidelines \u00b6 Developers of CNFs , particularly those targeting CNF Certification with Red Hat on OpenShift , can use this suite to test the interaction of their CNF with OpenShift. If interested in CNF Certification please contact Red Hat . Requirements OpenShift 4.10 installation to run the CNFs At least one extra machine to host the test suite To add private test cases \u00b6 Refer this documention https://github.com/test-network-function/cnfextensions Reference cnf-certification-test-partner repository provides sample example to model the test setup.","title":"CNF Developers"},{"location":"cnf-developers/#cnf-developers-guidelines","text":"Developers of CNFs , particularly those targeting CNF Certification with Red Hat on OpenShift , can use this suite to test the interaction of their CNF with OpenShift. If interested in CNF Certification please contact Red Hat . Requirements OpenShift 4.10 installation to run the CNFs At least one extra machine to host the test suite","title":"CNF Developers Guidelines"},{"location":"cnf-developers/#to-add-private-test-cases","text":"Refer this documention https://github.com/test-network-function/cnfextensions Reference cnf-certification-test-partner repository provides sample example to model the test setup.","title":"To add private test cases"},{"location":"configuration/","text":"Test configuration \u00b6 The certification test suite supports autodiscovery using labels and annotations. These can be configured through the following config file. - tnf_config.yml Sample As per the requirement the following fields can be changed. targetNameSpaces \u00b6 Multiple namespaces can be specified to deploy partner pods for testing through targetNameSpaces in the config file. targetNameSpaces : - name : firstnamespace - name : secondnamespace targetPodLabels \u00b6 The goal of this section is to specify the labels to be used to identify the CNF resources under test. Highly recommended The labels should be defined in pod definition rather than added after pod is created, as labels added later on will be lost in case the pod gets rescheduled. In case of pods defined as part of a deployment, it\u2019s best to use the same label as the one defined in the spec.selector.matchLabels section of the deployment yaml. The prefix field can be used to avoid naming collision with other labels. targetPodLabels : - prefix : test-network-function.com name : generic value : target The corresponding pod label used to match pods is: test-network-function.com/generic : target Once the pods are found, all of their containers are also added to the target container list. A target deployment list will also be created with all the deployments which the test pods belong to. targetCrds \u00b6 In order to autodiscover the CRDs to be tested, an array of search filters can be set under the \u201ctargetCrdFilters\u201d label. The autodiscovery mechanism will iterate through all the filters to look for all the CRDs that match it. Currently, filters only work by name suffix. targetCrdFilters : - nameSuffix : \"group1.tnf.com\" - nameSuffix : \"anydomain.com\" The autodiscovery mechanism will create a list of all CRD names in the cluster whose names have the suffix group1.tnf.com or anydomain.com , e.g. crd1.group1.tnf.com or mycrd.mygroup.anydomain.com . testTarget \u00b6 podsUnderTest / containersUnderTest \u00b6 The autodiscovery mechanism will attempt to identify the default network device and all the IP addresses of the pods it needs for network connectivity tests, though that information can be explicitly set using annotations if needed. Pod IPs \u00b6 The k8s.v1.cni.cncf.io/networks-status annotation is checked and all IPs from it are used. This annotation is automatically managed in OpenShift but may not be present in K8s. If it is not present, then only known IPs associated with the pod are used (the pod .status.ips field). Network Interfaces \u00b6 The k8s.v1.cni.cncf.io/networks-status annotation is checked and the interface from the first entry found with \"default\"=true is used. This annotation is automatically managed in OpenShift but may not be present in K8s. The label test-network-function.com/skip_connectivity_tests excludes pods from all connectivity tests. The label value is not important, only its presence. The label test-network-function.com/skip_multus_connectivity_tests excludes pods from Multus connectivity tests. Tests on default interface are still done. The label value is not important, but its presence. AffinityRequired \u00b6 For CNF workloads that require pods to use Pod or Node Affinity rules, the label AffinityRequired: true must be included on the Pod YAML. This will prevent any tests for anti-affinity to fail as well as test your workloads for affinity rules that support your CNF \u2018s use-case. certifiedcontainerinfo \u00b6 The certifiedcontainerinfo section contains information about CNFs containers that are to be checked for certification status on Red Hat catalogs. Operators \u00b6 The CSV of the installed Operators can be tested by the operator and affiliated-certification specs are identified with the test-network-function.com/operator=target label. Any value is permitted here but target is used here for consistency with the other specs. AllowedProtocolNames \u00b6 This name of protocols that allowed. If we want to add another name, we just need to write the name in the yaml file. for example: if we want to add new protocol - \u201chttp4\u201d, we add in \u201ctnf_config.yml\u201d below \u201cvalidProtocolNames\u201d and then this protocol (\u201chttp4\u201d) add to map allowedProtocolNames and finally \u201chttp4\u201d will be allow protocol. skipScalingTestDeployments and skipScalingTestStatefulSetNames \u00b6 This section of the TNF config allows the user to skip the scaling tests that potentially cause known problems with workloads that do not like being scaled up and scaled down. Example: skipScalingTestDeployments : - name : \"deployment1\" namespace : \"tnf\" skipScalingTestStatefulSetNames : - name : \"statefulset1\" namespace : \"tnf\"","title":"Test Configuration"},{"location":"configuration/#test-configuration","text":"The certification test suite supports autodiscovery using labels and annotations. These can be configured through the following config file. - tnf_config.yml Sample As per the requirement the following fields can be changed.","title":"Test configuration"},{"location":"configuration/#targetnamespaces","text":"Multiple namespaces can be specified to deploy partner pods for testing through targetNameSpaces in the config file. targetNameSpaces : - name : firstnamespace - name : secondnamespace","title":"targetNameSpaces"},{"location":"configuration/#targetpodlabels","text":"The goal of this section is to specify the labels to be used to identify the CNF resources under test. Highly recommended The labels should be defined in pod definition rather than added after pod is created, as labels added later on will be lost in case the pod gets rescheduled. In case of pods defined as part of a deployment, it\u2019s best to use the same label as the one defined in the spec.selector.matchLabels section of the deployment yaml. The prefix field can be used to avoid naming collision with other labels. targetPodLabels : - prefix : test-network-function.com name : generic value : target The corresponding pod label used to match pods is: test-network-function.com/generic : target Once the pods are found, all of their containers are also added to the target container list. A target deployment list will also be created with all the deployments which the test pods belong to.","title":"targetPodLabels"},{"location":"configuration/#targetcrds","text":"In order to autodiscover the CRDs to be tested, an array of search filters can be set under the \u201ctargetCrdFilters\u201d label. The autodiscovery mechanism will iterate through all the filters to look for all the CRDs that match it. Currently, filters only work by name suffix. targetCrdFilters : - nameSuffix : \"group1.tnf.com\" - nameSuffix : \"anydomain.com\" The autodiscovery mechanism will create a list of all CRD names in the cluster whose names have the suffix group1.tnf.com or anydomain.com , e.g. crd1.group1.tnf.com or mycrd.mygroup.anydomain.com .","title":"targetCrds"},{"location":"configuration/#testtarget","text":"","title":"testTarget"},{"location":"configuration/#podsundertest-containersundertest","text":"The autodiscovery mechanism will attempt to identify the default network device and all the IP addresses of the pods it needs for network connectivity tests, though that information can be explicitly set using annotations if needed.","title":"podsUnderTest / containersUnderTest"},{"location":"configuration/#pod-ips","text":"The k8s.v1.cni.cncf.io/networks-status annotation is checked and all IPs from it are used. This annotation is automatically managed in OpenShift but may not be present in K8s. If it is not present, then only known IPs associated with the pod are used (the pod .status.ips field).","title":"Pod IPs"},{"location":"configuration/#network-interfaces","text":"The k8s.v1.cni.cncf.io/networks-status annotation is checked and the interface from the first entry found with \"default\"=true is used. This annotation is automatically managed in OpenShift but may not be present in K8s. The label test-network-function.com/skip_connectivity_tests excludes pods from all connectivity tests. The label value is not important, only its presence. The label test-network-function.com/skip_multus_connectivity_tests excludes pods from Multus connectivity tests. Tests on default interface are still done. The label value is not important, but its presence.","title":"Network Interfaces"},{"location":"configuration/#affinityrequired","text":"For CNF workloads that require pods to use Pod or Node Affinity rules, the label AffinityRequired: true must be included on the Pod YAML. This will prevent any tests for anti-affinity to fail as well as test your workloads for affinity rules that support your CNF \u2018s use-case.","title":"AffinityRequired"},{"location":"configuration/#certifiedcontainerinfo","text":"The certifiedcontainerinfo section contains information about CNFs containers that are to be checked for certification status on Red Hat catalogs.","title":"certifiedcontainerinfo"},{"location":"configuration/#operators","text":"The CSV of the installed Operators can be tested by the operator and affiliated-certification specs are identified with the test-network-function.com/operator=target label. Any value is permitted here but target is used here for consistency with the other specs.","title":"Operators"},{"location":"configuration/#allowedprotocolnames","text":"This name of protocols that allowed. If we want to add another name, we just need to write the name in the yaml file. for example: if we want to add new protocol - \u201chttp4\u201d, we add in \u201ctnf_config.yml\u201d below \u201cvalidProtocolNames\u201d and then this protocol (\u201chttp4\u201d) add to map allowedProtocolNames and finally \u201chttp4\u201d will be allow protocol.","title":"AllowedProtocolNames"},{"location":"configuration/#skipscalingtestdeployments-and-skipscalingteststatefulsetnames","text":"This section of the TNF config allows the user to skip the scaling tests that potentially cause known problems with workloads that do not like being scaled up and scaled down. Example: skipScalingTestDeployments : - name : \"deployment1\" namespace : \"tnf\" skipScalingTestStatefulSetNames : - name : \"statefulset1\" namespace : \"tnf\"","title":"skipScalingTestDeployments and skipScalingTestStatefulSetNames"},{"location":"developers/","text":"Steps \u00b6 To test the newly added test / existing tests locally, follow the steps Clone the repo Set runtime environment variables, as per the requirement. For example, to deploy partner deployments in a custom namespace in the test config. targetNameSpaces : - name : mynamespace Also, skip intrusive tests export TNF_NON_INTRUSIVE_ONLY = true Set K8s config of the cluster where test pods are running export KUBECONFIG = <<mypath/.kube/config>> Execute test suite, which would build and run the suite For example, to run networking tests ./script/development.sh networking","title":"Developers"},{"location":"developers/#steps","text":"To test the newly added test / existing tests locally, follow the steps Clone the repo Set runtime environment variables, as per the requirement. For example, to deploy partner deployments in a custom namespace in the test config. targetNameSpaces : - name : mynamespace Also, skip intrusive tests export TNF_NON_INTRUSIVE_ONLY = true Set K8s config of the cluster where test pods are running export KUBECONFIG = <<mypath/.kube/config>> Execute test suite, which would build and run the suite For example, to run networking tests ./script/development.sh networking","title":"Steps"},{"location":"exception/","text":"Exception Process \u00b6 There may exist some test cases which needs to fail always. The exception raised by the failed tests is published to Red Hat website for that partner. CATALOG provides the details of such exception.","title":"Exception Process"},{"location":"exception/#exception-process","text":"There may exist some test cases which needs to fail always. The exception raised by the failed tests is published to Red Hat website for that partner. CATALOG provides the details of such exception.","title":"Exception Process"},{"location":"reference/","text":"Helpful Links \u00b6 Contribution Guidelines CATALOG Best Practices Document v1.3","title":"Helpful Links"},{"location":"reference/#helpful-links","text":"Contribution Guidelines CATALOG Best Practices Document v1.3","title":"Helpful Links"},{"location":"runtime-env/","text":"Runtime environment variables \u00b6 To run the test suite, some runtime environment variables are to be set. OCP >=4.12 Labels \u00b6 The following labels need to be added to your default namespace in your cluster if you are running OCP >=4.12: pod-security.kubernetes.io/enforce: privileged pod-security.kubernetes.io/enforce-version: latest You can manually label the namespace with: oc label namespace/default pod-security.kubernetes.io/enforce = privileged oc label namespace/default pod-security.kubernetes.io/enforce-version = latest Disable intrusive tests \u00b6 To skip intrusive tests which may disrupt cluster operations, issue the following: export TNF_NON_INTRUSIVE_ONLY = true Likewise, to enable intrusive tests, set the following: export TNF_NON_INTRUSIVE_ONLY = false Disconnected environment \u00b6 In a disconnected environment, only specific versions of images are mirrored to the local repo. For those environments, the partner pod image quay.io/testnetworkfunction/cnf-test-partner and debug pod image quay.io/testnetworkfunction/debug-partner should be mirrored and TNF_PARTNER_REPO should be set to the local repo, e.g.: export TNF_PARTNER_REPO = \"registry.dfwt5g.lab:5000/testnetworkfunction\" Note that you can also specify the debug pod image to use with SUPPORT_IMAGE environment variable, default to debug-partner:latest .","title":"Runtime environment variables"},{"location":"runtime-env/#runtime-environment-variables","text":"To run the test suite, some runtime environment variables are to be set.","title":"Runtime environment variables"},{"location":"runtime-env/#ocp-412-labels","text":"The following labels need to be added to your default namespace in your cluster if you are running OCP >=4.12: pod-security.kubernetes.io/enforce: privileged pod-security.kubernetes.io/enforce-version: latest You can manually label the namespace with: oc label namespace/default pod-security.kubernetes.io/enforce = privileged oc label namespace/default pod-security.kubernetes.io/enforce-version = latest","title":"OCP &gt;=4.12 Labels"},{"location":"runtime-env/#disable-intrusive-tests","text":"To skip intrusive tests which may disrupt cluster operations, issue the following: export TNF_NON_INTRUSIVE_ONLY = true Likewise, to enable intrusive tests, set the following: export TNF_NON_INTRUSIVE_ONLY = false","title":"Disable intrusive tests"},{"location":"runtime-env/#disconnected-environment","text":"In a disconnected environment, only specific versions of images are mirrored to the local repo. For those environments, the partner pod image quay.io/testnetworkfunction/cnf-test-partner and debug pod image quay.io/testnetworkfunction/debug-partner should be mirrored and TNF_PARTNER_REPO should be set to the local repo, e.g.: export TNF_PARTNER_REPO = \"registry.dfwt5g.lab:5000/testnetworkfunction\" Note that you can also specify the debug pod image to use with SUPPORT_IMAGE environment variable, default to debug-partner:latest .","title":"Disconnected environment"},{"location":"test-container/","text":"The tests can be run within a prebuilt container in the OCP cluster. Prerequisites for the OCP cluster The cluster should have enough resources to drain nodes and reschedule pods. If that is not the case, then lifecycle-pod-recreation test should be skipped. With quay test container image \u00b6 Pull test image \u00b6 The test image is available at this repository in quay.io and can be pulled using The image can be pulled using : podman pull quay.io/testnetworkfunction/cnf-certification-test Check cluster resources \u00b6 Some tests suites such as platform-alteration require node access to get node configuration like hugepage . In order to get the required information, the test suite does not ssh into nodes, but instead rely on oc debug tools . This tool makes it easier to fetch information from nodes and also to debug running pods. oc debug tool will launch a new container ending with -debug suffix, and the container will be destroyed once the debug session is done. Ensure that the cluster should have enough resources to create debug pod, otherwise those tests would fail. Note It\u2019s recommended to clean up disk space and make sure there\u2019s enough resources to deploy another container image in every node before starting the tests. Run the tests \u00b6 ./run-tnf-container.sh Required arguments -t to provide the path of the local directory that contains tnf config files -o to provide the path of the local directory that the test results will be available after the container exits. Warning This directory must exist in order for the claim file to be written. Optional arguments -l to list the labels to be run. See Ginkgo Spec Labels for more information on how to filter tests with labels. Note If -l is not specified, the tnf will run in \u2018diagnostic\u2019 mode. In this mode, no test case will run: it will only get information from the cluster (PUTs, CRDs, nodes info, etc\u2026) to save it in the claim file. This can be used to make sure the configuration was properly set and the autodiscovery found the right pods/crds\u2026 -i to provide a name to a custom TNF container image. Supports local images, as well as images from external registries. -k to set a path to one or more kubeconfig files to be used by the container to authenticate with the cluster. Paths must be separated by a colon. Note If -k is not specified, autodiscovery is performed. The autodiscovery first looks for paths in the $KUBECONFIG environment variable on the host system, and if the variable is not set or is empty, the default configuration stored in $HOME/.kube/config is checked. -n to give the network mode of the container. Defaults set to host , which requires selinux to be disabled. Alternatively, bridge mode can be used with selinux if TNF_CONTAINER_CLIENT is set to docker or running the test as root. Note See the docker run \u2013network parameter reference for more information on how to configure network settings. -b to set an external offline DB that will be used to verify the certification status of containers, helm charts and operators. Defaults to the DB included in the TNF container image. Note See the OCT tool for more information on how to create this DB. Command to run ./run-tnf-container.sh -k ~/.kube/config -t ~/tnf/config -o ~/tnf/output -l \"networking,access-control\" See General tests for a list of available keywords. Run with docker \u00b6 By default, run-container.sh utilizes podman . However, an alternate container virtualization client using TNF_CONTAINER_CLIENT can be configured. This is particularly useful for operating systems that do not readily support podman . In order to configure the test harness to use docker , issue the following prior to run-tnf-container.sh : export TNF_CONTAINER_CLIENT = docker With local test container image \u00b6 Build locally \u00b6 podman build -t cnf-certification-test:v4.1.2 \\ --build-arg TNF_VERSION = v4.1.2 \\ --build-arg OPENSHIFT_VERSION = 4 .7.55 . TNF_VERSION value is set to a branch, a tag, or a hash of a commit that will be installed into the image OPENSHIFT_VERSION value points to the OCP version of the cluster in which the workloads to be tested are deployed. Build from an unofficial source \u00b6 The unofficial source could be a fork of the TNF repository. Use the TNF_SRC_URL build argument to override the URL to a source repository. podman build -t cnf-certification-test:v4.1.2 \\ --build-arg TNF_VERSION = v1.0.5 \\ --build-arg TNF_SRC_URL = https://github.com/test-network-function/cnf-certification-test \\ --build-arg OPENSHIFT_VERSION = 4 .7.55 . Run the tests \u00b6 Specify the custom TNF image using the -i parameter. ./run-tnf-container.sh -i cnf-certification-test:v4.1.2 -t ~/tnf/config -o ~/tnf/output -l \"networking,access-control\" Note: see General tests for a list of available keywords.","title":"Prebuilt container"},{"location":"test-container/#with-quay-test-container-image","text":"","title":"With quay test container image"},{"location":"test-container/#pull-test-image","text":"The test image is available at this repository in quay.io and can be pulled using The image can be pulled using : podman pull quay.io/testnetworkfunction/cnf-certification-test","title":"Pull test image"},{"location":"test-container/#check-cluster-resources","text":"Some tests suites such as platform-alteration require node access to get node configuration like hugepage . In order to get the required information, the test suite does not ssh into nodes, but instead rely on oc debug tools . This tool makes it easier to fetch information from nodes and also to debug running pods. oc debug tool will launch a new container ending with -debug suffix, and the container will be destroyed once the debug session is done. Ensure that the cluster should have enough resources to create debug pod, otherwise those tests would fail. Note It\u2019s recommended to clean up disk space and make sure there\u2019s enough resources to deploy another container image in every node before starting the tests.","title":"Check cluster resources"},{"location":"test-container/#run-the-tests","text":"./run-tnf-container.sh Required arguments -t to provide the path of the local directory that contains tnf config files -o to provide the path of the local directory that the test results will be available after the container exits. Warning This directory must exist in order for the claim file to be written. Optional arguments -l to list the labels to be run. See Ginkgo Spec Labels for more information on how to filter tests with labels. Note If -l is not specified, the tnf will run in \u2018diagnostic\u2019 mode. In this mode, no test case will run: it will only get information from the cluster (PUTs, CRDs, nodes info, etc\u2026) to save it in the claim file. This can be used to make sure the configuration was properly set and the autodiscovery found the right pods/crds\u2026 -i to provide a name to a custom TNF container image. Supports local images, as well as images from external registries. -k to set a path to one or more kubeconfig files to be used by the container to authenticate with the cluster. Paths must be separated by a colon. Note If -k is not specified, autodiscovery is performed. The autodiscovery first looks for paths in the $KUBECONFIG environment variable on the host system, and if the variable is not set or is empty, the default configuration stored in $HOME/.kube/config is checked. -n to give the network mode of the container. Defaults set to host , which requires selinux to be disabled. Alternatively, bridge mode can be used with selinux if TNF_CONTAINER_CLIENT is set to docker or running the test as root. Note See the docker run \u2013network parameter reference for more information on how to configure network settings. -b to set an external offline DB that will be used to verify the certification status of containers, helm charts and operators. Defaults to the DB included in the TNF container image. Note See the OCT tool for more information on how to create this DB. Command to run ./run-tnf-container.sh -k ~/.kube/config -t ~/tnf/config -o ~/tnf/output -l \"networking,access-control\" See General tests for a list of available keywords.","title":"Run the tests"},{"location":"test-container/#run-with-docker","text":"By default, run-container.sh utilizes podman . However, an alternate container virtualization client using TNF_CONTAINER_CLIENT can be configured. This is particularly useful for operating systems that do not readily support podman . In order to configure the test harness to use docker , issue the following prior to run-tnf-container.sh : export TNF_CONTAINER_CLIENT = docker","title":"Run with docker"},{"location":"test-container/#with-local-test-container-image","text":"","title":"With local test container image"},{"location":"test-container/#build-locally","text":"podman build -t cnf-certification-test:v4.1.2 \\ --build-arg TNF_VERSION = v4.1.2 \\ --build-arg OPENSHIFT_VERSION = 4 .7.55 . TNF_VERSION value is set to a branch, a tag, or a hash of a commit that will be installed into the image OPENSHIFT_VERSION value points to the OCP version of the cluster in which the workloads to be tested are deployed.","title":"Build locally"},{"location":"test-container/#build-from-an-unofficial-source","text":"The unofficial source could be a fork of the TNF repository. Use the TNF_SRC_URL build argument to override the URL to a source repository. podman build -t cnf-certification-test:v4.1.2 \\ --build-arg TNF_VERSION = v1.0.5 \\ --build-arg TNF_SRC_URL = https://github.com/test-network-function/cnf-certification-test \\ --build-arg OPENSHIFT_VERSION = 4 .7.55 .","title":"Build from an unofficial source"},{"location":"test-container/#run-the-tests_1","text":"Specify the custom TNF image using the -i parameter. ./run-tnf-container.sh -i cnf-certification-test:v4.1.2 -t ~/tnf/config -o ~/tnf/output -l \"networking,access-control\" Note: see General tests for a list of available keywords.","title":"Run the tests"},{"location":"test-output/","text":"Test Output \u00b6 Claim File \u00b6 The test suite generates an output file, named claim file. This file is considered as the proof of CNFs test run, evaluated by Red Hat when certified status is considered. This file describes the following The system(s) under test The tests that are executed The outcome of the executed / skipped tests Files that need to be submitted for certification When submitting results back to Red Hat for certification, please include the above mentioned claim file, the JUnit file, and any available console logs. How to add a CNF platform test result to the existing claim file? go run cmd / tools / cmd / main . go claim - add -- claimfile = claim . json -- reportdir = / home / $ USER / reports Args : --claimfile is an existing claim.json file --repordir :path to test results that you want to include. The tests result files from the given report dir will be appended under the result section of the claim file using file name as the key/value pair. The tool will ignore the test result, if the key name is already present under result section of the claim file. \"results\" : { \"cnf-certification-tests_junit\" : { \"testsuite\" : { \"-errors\" : \"0\" , \"-failures\" : \"2\" , \"-name\" : \"CNF Certification Test Suite\" , \"-tests\" : \"14\" , ... Reference For more details on the contents of the claim file schema . Guide .","title":"Test Output"},{"location":"test-output/#test-output","text":"","title":"Test Output"},{"location":"test-output/#claim-file","text":"The test suite generates an output file, named claim file. This file is considered as the proof of CNFs test run, evaluated by Red Hat when certified status is considered. This file describes the following The system(s) under test The tests that are executed The outcome of the executed / skipped tests Files that need to be submitted for certification When submitting results back to Red Hat for certification, please include the above mentioned claim file, the JUnit file, and any available console logs. How to add a CNF platform test result to the existing claim file? go run cmd / tools / cmd / main . go claim - add -- claimfile = claim . json -- reportdir = / home / $ USER / reports Args : --claimfile is an existing claim.json file --repordir :path to test results that you want to include. The tests result files from the given report dir will be appended under the result section of the claim file using file name as the key/value pair. The tool will ignore the test result, if the key name is already present under result section of the claim file. \"results\" : { \"cnf-certification-tests_junit\" : { \"testsuite\" : { \"-errors\" : \"0\" , \"-failures\" : \"2\" , \"-name\" : \"CNF Certification Test Suite\" , \"-tests\" : \"14\" , ... Reference For more details on the contents of the claim file schema . Guide .","title":"Claim File"},{"location":"test-spec/","text":"Test Specifications \u00b6 Available Test Specs \u00b6 There are two categories for CNF tests. General These tests are designed to test any commodity CNF running on OpenShift, and include specifications such as Default network connectivity. CNF -Specific These tests are designed to test some unique aspects of the CNF under test are behaving correctly. This could include specifications such as issuing a GET request to a web server, or passing traffic through an IPSEC tunnel. General tests \u00b6 These tests belong to multiple suites that can be run in any combination as is appropriate for the CNFs under test. Info Test suites group tests by the topic areas. Suite Test Spec Description Minimum OpenShift Version access-control The access-control test suite is used to test service account, namespace and cluster/pod role binding for the pods under test. It also tests the pods/containers configuration. 4.6.0 affiliated-certification The affiliated-certification test suite verifies that the containers and operators discovered or listed in the configuration file are certified by Redhat 4.6.0 lifecycle The lifecycle test suite verifies the pods deployment, creation, shutdown and survivability. 4.6.0 networking The networking test suite contains tests that check connectivity and networking config related best practices. 4.6.0 operator The operator test suite is designed to test basic Kubernetes Operator functionality. 4.6.0 platform-alteration verifies that key platform configuration is not modified by the CNF under test 4.6.0 observability the observability test suite contains tests that check CNF logging is following best practices and that CRDs have status fields 4.6.0 Info Please refer CATALOG.md for more details. CNF -specific tests \u00b6 TODO","title":"Available Test Specs"},{"location":"test-spec/#test-specifications","text":"","title":"Test Specifications"},{"location":"test-spec/#available-test-specs","text":"There are two categories for CNF tests. General These tests are designed to test any commodity CNF running on OpenShift, and include specifications such as Default network connectivity. CNF -Specific These tests are designed to test some unique aspects of the CNF under test are behaving correctly. This could include specifications such as issuing a GET request to a web server, or passing traffic through an IPSEC tunnel.","title":"Available Test Specs"},{"location":"test-spec/#general-tests","text":"These tests belong to multiple suites that can be run in any combination as is appropriate for the CNFs under test. Info Test suites group tests by the topic areas. Suite Test Spec Description Minimum OpenShift Version access-control The access-control test suite is used to test service account, namespace and cluster/pod role binding for the pods under test. It also tests the pods/containers configuration. 4.6.0 affiliated-certification The affiliated-certification test suite verifies that the containers and operators discovered or listed in the configuration file are certified by Redhat 4.6.0 lifecycle The lifecycle test suite verifies the pods deployment, creation, shutdown and survivability. 4.6.0 networking The networking test suite contains tests that check connectivity and networking config related best practices. 4.6.0 operator The operator test suite is designed to test basic Kubernetes Operator functionality. 4.6.0 platform-alteration verifies that key platform configuration is not modified by the CNF under test 4.6.0 observability the observability test suite contains tests that check CNF logging is following best practices and that CRDs have status fields 4.6.0 Info Please refer CATALOG.md for more details.","title":"General tests"},{"location":"test-spec/#cnf-specific-tests","text":"TODO","title":"CNF-specific tests"},{"location":"test-standalone/","text":"Standalone test executable \u00b6 Prerequisites The repo is cloned and all the commands should be run from the cloned repo. mkdir ~/workspace cd ~/workspace git clone git@github.com:test-network-function/cnf-certification-test.git cd cnf-certification-test Note By default, cnf-certification-test emits results to cnf-certification-test/cnf-certification-tests_junit.xml . 1. Install dependencies \u00b6 Run the following command to install the following dependencies. make install-tools Dependency Minimum Version GoLang 1.19 golangci-lint 1.50.1 jq 1.6 OpenShift Client 4.11 Other binary dependencies required to run tests can be installed using the following command: Note You must also make sure that $GOBIN (default $GOPATH/bin ) is on your $PATH . Efforts to containerise this offering are considered a work in progress. 2. Build the Test Suite \u00b6 In order to build the test executable, first make sure you have satisfied the dependencies . make build-cnf-tests Gotcha: The make build* commands run unit tests where appropriate. They do NOT test the CNF . 3. Test a CNF \u00b6 A CNF is tested by specifying which suites to run using the run-cnf-suites.sh helper script. Run any combination of the suites keywords listed at in the General tests section, e.g. ./run-cnf-suites.sh -l \"lifecycle\" ./run-cnf-suites.sh -l \"networking,lifecycle\" ./run-cnf-suites.sh -l \"operator,networking\" ./run-cnf-suites.sh -l \"networking,platform-alteration\" ./run-cnf-suites.sh -l \"networking,lifecycle,affiliated-certification,operator\" Note As with \u201crun-tnf-container.sh\u201d, if -l is not specified here, the tnf will run in \u2018diagnostic\u2019 mode. By default the claim file will be output into the same location as the test executable. The -o argument for run-cnf-suites.sh can be used to provide a new location that the output files will be saved to. For more detailed control over the outputs, see the output of cnf-certification-test.test --help . cd cnf-certification-test && ./cnf-certification-test.test --help Run a single test \u00b6 All tests have unique labels, which can be used to filter which tests are to be run. This is useful when debugging a single test. To select the test to be executed when running run-cnf-suites.sh with the following command-line: ./run-cnf-suites.sh -l operator-install-source Note The test labels work the same as the suite labels, so you can select more than one test with the filtering mechanism shown before. Run a subset \u00b6 You can find all the labels attached to the tests by running the following command: ./run-cnf-suites.sh --list You can also check the CATALOG.md to find all test labels. Labels for offline environments \u00b6 Some tests do require connectivity to Red Hat servers to validate certification status. To run the tests in an offline environment, skip the tests using the l option. ./run-cnf-suites.sh -l '!online' Alternatively, if an offline DB for containers, helm charts and operators is available, there is no need to skip those tests if the environment variable TNF_OFFLINE_DB is set to the DB location. This DB can be generated using the OCT tool . Build + Test a CNF \u00b6 Refer Developers\u2019 Guide","title":"Standalone test executable"},{"location":"test-standalone/#standalone-test-executable","text":"Prerequisites The repo is cloned and all the commands should be run from the cloned repo. mkdir ~/workspace cd ~/workspace git clone git@github.com:test-network-function/cnf-certification-test.git cd cnf-certification-test Note By default, cnf-certification-test emits results to cnf-certification-test/cnf-certification-tests_junit.xml .","title":"Standalone test executable"},{"location":"test-standalone/#1-install-dependencies","text":"Run the following command to install the following dependencies. make install-tools Dependency Minimum Version GoLang 1.19 golangci-lint 1.50.1 jq 1.6 OpenShift Client 4.11 Other binary dependencies required to run tests can be installed using the following command: Note You must also make sure that $GOBIN (default $GOPATH/bin ) is on your $PATH . Efforts to containerise this offering are considered a work in progress.","title":"1. Install dependencies"},{"location":"test-standalone/#2-build-the-test-suite","text":"In order to build the test executable, first make sure you have satisfied the dependencies . make build-cnf-tests Gotcha: The make build* commands run unit tests where appropriate. They do NOT test the CNF .","title":"2. Build the Test Suite"},{"location":"test-standalone/#3-test-a-cnf","text":"A CNF is tested by specifying which suites to run using the run-cnf-suites.sh helper script. Run any combination of the suites keywords listed at in the General tests section, e.g. ./run-cnf-suites.sh -l \"lifecycle\" ./run-cnf-suites.sh -l \"networking,lifecycle\" ./run-cnf-suites.sh -l \"operator,networking\" ./run-cnf-suites.sh -l \"networking,platform-alteration\" ./run-cnf-suites.sh -l \"networking,lifecycle,affiliated-certification,operator\" Note As with \u201crun-tnf-container.sh\u201d, if -l is not specified here, the tnf will run in \u2018diagnostic\u2019 mode. By default the claim file will be output into the same location as the test executable. The -o argument for run-cnf-suites.sh can be used to provide a new location that the output files will be saved to. For more detailed control over the outputs, see the output of cnf-certification-test.test --help . cd cnf-certification-test && ./cnf-certification-test.test --help","title":"3. Test a CNF"},{"location":"test-standalone/#run-a-single-test","text":"All tests have unique labels, which can be used to filter which tests are to be run. This is useful when debugging a single test. To select the test to be executed when running run-cnf-suites.sh with the following command-line: ./run-cnf-suites.sh -l operator-install-source Note The test labels work the same as the suite labels, so you can select more than one test with the filtering mechanism shown before.","title":"Run a single test"},{"location":"test-standalone/#run-a-subset","text":"You can find all the labels attached to the tests by running the following command: ./run-cnf-suites.sh --list You can also check the CATALOG.md to find all test labels.","title":"Run a subset"},{"location":"test-standalone/#labels-for-offline-environments","text":"Some tests do require connectivity to Red Hat servers to validate certification status. To run the tests in an offline environment, skip the tests using the l option. ./run-cnf-suites.sh -l '!online' Alternatively, if an offline DB for containers, helm charts and operators is available, there is no need to skip those tests if the environment variable TNF_OFFLINE_DB is set to the DB location. This DB can be generated using the OCT tool .","title":"Labels for offline environments"},{"location":"test-standalone/#build-test-a-cnf","text":"Refer Developers\u2019 Guide","title":"Build + Test a CNF"}]}