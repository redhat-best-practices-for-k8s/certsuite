{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>This repository provides a set of test cases to verify the conformance of a workload with the Red Hat Best Practices for Kubernetes.</p> <p>Workload</p> <p>The app (containers/pods/operators) we want to certify according Telco partner/Red Hat\u2019s best practices.</p> <p>Red Hat Best Practices Test Suite for Kubernetes</p> <p>The tool we use to certify a workload.</p> <p>The purpose of the tests and the framework is to test the interaction of the workload with OpenShift Container Platform (OCP).  </p> <p>Info</p> <p>This test suite is provided for the workload developers to test their workload\u2019s readiness for certification. Please see the Developers\u2019 Guide for more information.</p> <p>Features</p> <ul> <li> <p>The test suite generates a report (<code>claim.json</code>) and saves the test execution log (<code>certsuite.log</code>) in a configurable output directory.</p> </li> <li> <p>The catalog of the existing test cases and test building blocks are available in CATALOG.md</p> </li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>There are 3 building blocks in the above framework.</p> <ul> <li> <p>the <code>CNF</code> represents the workload to be certified. The Test Suite identifies the resources (containers/pods/operators etc) belonging to the workload via labels or static data entries in the Config File</p> </li> <li> <p>the <code>Certification container/exec</code> is the Test Suite running on the platform or in a container. The executable verifies the workload under test configuration and its interactions with OpenShift</p> </li> <li> <p>the <code>Debug</code> pods are part of a Kubernetes daemonset responsible to run various privileged commands on Kubernetes nodes. Debug pods are useful to run platform tests and test commands (e.g. ping) in container namespaces without changing the container image content. The debug daemonset is instantiated via the privileged-daemonset repository.</p> </li> </ul>"},{"location":"configuration/","title":"Test Configuration","text":""},{"location":"configuration/#red-hat-best-practices-test-suite-for-kubernetes-configuration","title":"Red Hat Best Practices Test Suite for Kubernetes configuration","text":"<p>The Red Hat Best Practices Test Suite for Kubernetes uses a YAML configuration file to certify a specific workload. This file specifies the workload\u2019s resources to be certified, as well as any exceptions or other general configuration options.</p> <p>By default a file named tnf_config.yml will be used. Here\u2019s an example of the Config File. For a description of each config option see the section Config File options.</p>"},{"location":"configuration/#config-generator","title":"Config Generator","text":"<p>The Config File can be created using the Config Generator, which is part of the certsuite tool shipped with the Test Suite. The purpose of this particular tool is to help users configuring the Test Suite providing a logical structure of the available options as well as the information required to make use of them. The result is a Config File in YAML format that will be parsed to adapt the verification process to a specific workload.</p> <p>To compile the certsuite tool:</p> <pre><code>make build-certsuite-tool\n</code></pre> <p>To launch the Config Generator:</p> <pre><code>./certsuite generate config\n</code></pre> <p>Here\u2019s an example of how to use the tool:</p>"},{"location":"configuration/#config-file-options","title":"Config File options","text":""},{"location":"configuration/#workload-resources","title":"Workload resources","text":"<p>These options allow configuring the workload resources to be verified. Only the resources that the workload uses are required to be configured. The rest can be left empty. Usually a basic configuration includes Namespaces and Pods at least.</p> <p>Note</p> <p>Using the number of labels to determine how to get the resources under test.  If there are labels defined, we get the list of pods, statefulsets, deployments, csvs, by fetching the resources matching the labels. Otherwise, if the labels are not defined, we only test the resources that are in the namespaces under test (defined in tnf_config.yml).</p>"},{"location":"configuration/#targetnamespaces","title":"targetNameSpaces","text":"<p>The namespaces in which the workload under test will be deployed.</p> <pre><code>targetNameSpaces:\n  - name: tnf\n</code></pre>"},{"location":"configuration/#podsundertestlabels","title":"podsUnderTestLabels","text":"<p>The labels that each Pod of the workload under test must have to be verified by the Test Suite.</p> <p>Highly recommended</p> <p>The labels should be defined in Pod definition rather than added after the Pod is created, as labels added later on will be lost in case the Pod gets rescheduled. In the case of Pods defined as part of a Deployment, it\u2019s best to use the same label as the one defined in the spec.selector.matchLabels section of the Deployment YAML. The prefix field can be used to avoid naming collision with other labels.</p> <pre><code>podsUnderTestLabels:\n  - \"test-network-function.com/generic: target\"\n</code></pre>"},{"location":"configuration/#operatorsundertestlabels","title":"operatorsUnderTestLabels","text":"<p>The labels that each operator\u2019s CSV of the workload under test must have to be verified by the Test Suite.</p> <p>If a new label is used for this purpose make sure it is added to the workload operator\u2019s CSVs.</p> <pre><code>operatorsUnderTestLabels:\n  - \"test-network-function.com/operator: target\" \n</code></pre>"},{"location":"configuration/#targetcrdfilters","title":"targetCrdFilters","text":"<p>The CRD name suffix used to filter the workload\u2019s CRDs among all the CRDs present in the cluster. For each CRD it can also be specified if it\u2019s scalable or not in order to avoid some lifecycle test cases.</p> <pre><code>targetCrdFilters:\n - nameSuffix: \"group1.tnf.com\"\n   scalable: false\n - nameSuffix: \"anydomain.com\"\n   scalable: true\n</code></pre> <p>With the config show above, all CRD names in the cluster whose names have the suffix group1.tnf.com or anydomain.com ( e.g. crd1.group1.tnf.com or mycrd.mygroup.anydomain.com) will be tested.</p>"},{"location":"configuration/#manageddeployments-managedstatefulsets","title":"managedDeployments / managedStatefulSets","text":"<p>The Deployments/StatefulSets managed by a Custom Resource whose scaling is controlled using the \u201cscale\u201d subresource of the CR.</p> <p>The CRD defining that CR should be included in the CRD filters with the scalable property set to true. If so, the test case lifecycle-{deployment/statefulset}-scaling will be skipped, otherwise it will fail.</p> <pre><code>managedDeployments:\n  - name: jack\nmanagedStatefulsets:\n  - name: jack\n</code></pre>"},{"location":"configuration/#junit-xml-file-creation","title":"JUnit XML File Creation","text":"<p>The test suite has the ability to create the JUNit XML File output containing the test ID and the corresponding test result.</p> <p>To enable this, set:</p> <pre><code>export TNF_ENABLE_XML_CREATION=true\n</code></pre> <p>This will create a file named <code>cnf-certification-test/cnf-certification-tests_junit.xml</code>.</p>"},{"location":"configuration/#enable-running-container-against-openshift-local","title":"Enable running container against OpenShift Local","text":"<p>While running the test suite as a container, you can enable the container to be able to reach the local CRC instance by setting:</p> <pre><code>export TNF_ENABLE_CRC_TESTING=true\n</code></pre> <p>This utilizes the <code>--add-host</code> flag in Docker to be able to point <code>api.crc.testing</code> to the host gateway.</p>"},{"location":"configuration/#exceptions","title":"Exceptions","text":"<p>These options allow adding exceptions to skip several checks for different resources. The exceptions must be justified in order to satisfy the Red Hat Best Practices for Kubernetes.</p>"},{"location":"configuration/#acceptedkerneltaints","title":"acceptedKernelTaints","text":"<p>The list of kernel modules loaded by the workload that make the Linux kernel mark itself as tainted but that should skip verification.</p> <p>Test cases affected: platform-alteration-tainted-node-kernel.</p> <pre><code>acceptedKernelTaints:\n  - module: vboxsf\n  - module: vboxguest\n</code></pre>"},{"location":"configuration/#skiphelmchartlist","title":"skipHelmChartList","text":"<p>The list of Helm charts that the workload uses whose certification status will not be verified.</p> <p>If no exception is configured, the certification status for all Helm charts will be checked in the OpenShift Helms Charts repository.</p> <p>Test cases affected: affiliated-certification-helmchart-is-certified.</p> <pre><code>skipHelmChartList:\n  - name: coredns\n</code></pre>"},{"location":"configuration/#validprotocolnames","title":"validProtocolNames","text":"<p>The list of allowed protocol names to be used for container port names.</p> <p>The name field of a container port must be of the form protocol[-suffix] where protocol must be allowed by default or added to this list. The optional suffix can be chosen by the application. Protocol names allowed by default: grpc, grpc-web, http, http2, tcp, udp.</p> <p>Test cases affected: manageability-container-port-name-format.</p> <pre><code>validProtocolNames:\n  - \"http3\"\n  - \"sctp\"\n</code></pre>"},{"location":"configuration/#servicesignorelist","title":"servicesIgnoreList","text":"<p>The list of Services that will skip verification.</p> <p>Services included in this list will be filtered out at the autodiscovery stage and will not be subject to checks in any test case.</p> <p>Tests cases affected: networking-dual-stack-service, access-control-service-type.</p> <pre><code>servicesignorelist:\n  - \"hazelcast-platform-controller-manager-service\"\n  - \"hazelcast-platform-webhook-service\"\n  - \"new-pro-controller-manager-metrics-service\"\n</code></pre>"},{"location":"configuration/#skipscalingtestdeployments-skipscalingteststatefulsets","title":"skipScalingTestDeployments / skipScalingTestStatefulSets","text":"<p>The list of Deployments/StatefulSets that do not support scale in/out operations.</p> <p>Deployments/StatefulSets included in this list will skip any scaling operation check.</p> <p>Test cases affected: lifecycle-deployment-scaling, lifecycle-statefulset-scaling.</p> <pre><code>skipScalingTestDeployments:\n  - name: deployment1\n    namespace: tnf\nskipScalingTestStatefulSetNames:\n  - name: statefulset1\n    namespace: tnf\n</code></pre>"},{"location":"configuration/#red-hat-best-practices-test-suite-settings","title":"Red Hat Best Practices Test Suite settings","text":""},{"location":"configuration/#debugdaemonsetnamespace","title":"debugDaemonSetNamespace","text":"<p>This is an optional field with the name of the namespace where a privileged DaemonSet will be deployed. The namespace will be created in case it does not exist. In case this field is not set, the default namespace for this DaemonSet is cnf-suite.</p> <pre><code>debugDaemonSetNamespace: cnf-cert\n</code></pre> <p>This DaemonSet, called tnf-debug is deployed and used internally by the Test Suite tool to issue some shell commands that are needed in certain test cases. Some of these test cases might fail or be skipped in case it wasn\u2019t deployed correctly.</p>"},{"location":"configuration/#other-settings","title":"Other settings","text":"<p>The autodiscovery mechanism will attempt to identify the default network device and all the IP addresses of the Pods it needs for network connectivity tests, though that information can be explicitly set using annotations if needed.</p>"},{"location":"configuration/#pod-ips","title":"Pod IPs","text":"<ul> <li>The k8s.v1.cni.cncf.io/networks-status annotation is checked and all IPs from it are used. This annotation is automatically managed in OpenShift but may not be present in K8s.</li> <li>If it is not present, then only known IPs associated with the Pod are used (the Pod .status.ips field).</li> </ul>"},{"location":"configuration/#network-interfaces","title":"Network Interfaces","text":"<ul> <li>The k8s.v1.cni.cncf.io/networks-status annotation is checked and the interface from the first entry found with \u201cdefault\u201d=true is used. This annotation is automatically managed in OpenShift but may not be present in K8s.</li> </ul> <p>The label test-network-function.com/skip_connectivity_tests excludes Pods from all connectivity tests.</p> <p>The label test-network-function.com/skip_multus_connectivity_tests excludes Pods from Multus connectivity tests. Tests on the default interface are still run.</p>"},{"location":"configuration/#affinity-requirements","title":"Affinity requirements","text":"<p>For workloads that require Pods to use Pod or Node Affinity rules, the label AffinityRequired: true must be included on the Pod YAML. This will ensure that the affinity best practices are tested and prevent any test cases for anti-affinity to fail.</p>"},{"location":"data-collection/","title":"Share Test Suite Results with Red Hat","text":"<p>Your test suite results can be collected and stored in our Collector database.</p>"},{"location":"data-collection/#what-information-is-shared","title":"What information is shared","text":"<ul> <li>Only partner name and pass/fail/skip status per test case is stored</li> <li>Protected by password, partners can access only the data they have submitted. Red Hat team can access data from all partners.</li> </ul>"},{"location":"data-collection/#why-should-i-store-my-data-in-the-collector","title":"Why should I store my data in the Collector?","text":"<ul> <li>Keep track of your test suite results over time.</li> <li>Contribute to our statistics and analysis, to improve Red Hat best practices test suite for Kubernetes.</li> </ul>"},{"location":"developers/","title":"Developers","text":""},{"location":"developers/#steps","title":"Steps","text":"<p>To test the newly added test / existing tests locally, follow the steps</p> <ul> <li>Clone the repo</li> <li> <p>Set runtime environment variables, as per the requirement.</p> <p>For example, to deploy partner deployments in a custom namespace in the test config.</p> <pre><code>targetNameSpaces:\n  - name: mynamespace\n</code></pre> </li> <li> <p>Also, skip intrusive tests</p> </li> </ul> <pre><code>export TNF_NON_INTRUSIVE_ONLY=true\n</code></pre> <ul> <li> <p>Set K8s config of the cluster where test pods are running</p> <pre><code>export KUBECONFIG=&lt;&lt;mypath/.kube/config&gt;&gt;\n</code></pre> </li> <li> <p>Execute test suite, which would build and run the suite</p> <p>For example, to run <code>networking</code> tests</p> <pre><code>./certsuite run -l networking\n</code></pre> </li> </ul>"},{"location":"developers/#dependencies-on-other-pr","title":"Dependencies on other PR","text":"<p>If you have dependencies on other Pull Requests, you can add a comment like that:</p> <pre><code>Depends-On: &lt;url of the PR&gt;\n</code></pre> <p>and the dependent PR will automatically be extracted and injected in your change during the GitHub Action CI jobs and the DCI jobs.</p>"},{"location":"exception/","title":"Exception Process","text":""},{"location":"exception/#exception-process","title":"Exception Process","text":"<p>There may exist some test cases which needs to fail always. The exception raised by the failed tests is published to Red Hat website for that partner.</p> <p>CATALOG provides the details of such exception.</p>"},{"location":"reference/","title":"Helpful Links","text":"<ul> <li>Contribution Guidelines</li> <li>CATALOG</li> <li>Best Practices Document v1.3</li> </ul>"},{"location":"runtime-env/","title":"Runtime environment variables","text":""},{"location":"runtime-env/#runtime-environment-variables","title":"Runtime environment variables","text":"<p>To run the test suite, some runtime environment variables are to be set.</p>"},{"location":"runtime-env/#ocp-412-labels","title":"OCP &gt;=4.12 Labels","text":"<p>The following labels need to be added to your default namespace in your cluster if you are running OCP &gt;=4.12:</p> <pre><code>pod-security.kubernetes.io/enforce: privileged\npod-security.kubernetes.io/enforce-version: latest\n</code></pre> <p>You can manually label the namespace with:</p> <pre><code>oc label namespace/default pod-security.kubernetes.io/enforce=privileged\noc label namespace/default pod-security.kubernetes.io/enforce-version=latest\n</code></pre>"},{"location":"runtime-env/#disable-intrusive-tests","title":"Disable intrusive tests","text":"<p>To skip intrusive tests which may disrupt cluster operations, issue the following:</p> <pre><code>export TNF_NON_INTRUSIVE_ONLY=true\n</code></pre> <p>The intrusive test cases are:</p> <ul> <li>lifecycle-deployment-scaling</li> <li>lifecycle-statefulset-scaling</li> <li>lifecycle-crd-scaling</li> <li>lifecycle-pod-recreation</li> </ul> <p>Likewise, to enable intrusive tests, set the following:</p> <pre><code>export TNF_NON_INTRUSIVE_ONLY=false\n</code></pre> <p>Intrusive tests are enabled by default.</p>"},{"location":"runtime-env/#preflight-integration","title":"Preflight Integration","text":"<p>When running the <code>preflight</code> suite of tests, there are a few environment variables that will need to be set:</p> <p><code>PFLT_DOCKERCONFIG</code> is a required variable for running the preflight test suite. This provides credentials to the underlying preflight library for being able to pull/manipulate images and image bundles for testing.</p> <p>When running as a container, the docker config is mounted to the container via volume mount.</p> <p>When running as a standalone binary, the environment variables are consumed directly from your local machine.</p> <p>See more about this variable here.</p> <p><code>TNF_ALLOW_PREFLIGHT_INSECURE</code> (default: false) is required set to <code>true</code> if you are running against a private container registry that has self-signed certificates.</p> <p>Note that you can also specify the debug pod image to use with <code>SUPPORT_IMAGE</code> environment variable, default to <code>k8s-best-practices-debug:v0.0.2</code>.</p>"},{"location":"test-output/","title":"Test Output","text":""},{"location":"test-output/#test-output","title":"Test Output","text":""},{"location":"test-output/#claim-file","title":"Claim File","text":"<p>The test suite generates an output file, named claim file. This file is considered as the proof of the workload\u2019s test run, evaluated by Red Hat when certified status is considered.</p> <p>This file describes the following</p> <ul> <li>The system(s) under test</li> <li>The tests that are executed</li> <li>The outcome of the executed / skipped tests</li> </ul> <p>Files that need to be submitted for certification</p> <p>When submitting results back to Red Hat for certification, please include the above mentioned claim file, the JUnit file, and any available console logs.</p> <p>How to add a workload platform test result to the existing claim file?</p> <pre><code>go run cmd/tools/cmd/main.go claim-add --claimfile=claim.json\n--reportdir=/home/$USER/reports\n</code></pre> <p>Args: <code>--claimfile is an existing claim.json file</code> <code>--repordir :path to test results that you want to include.</code></p> <p>The tests result files from the given report dir will be appended under the result section of the claim file using file name as the key/value pair.  The tool will ignore the test result, if the key name is already present under result section of the claim file.</p> <pre><code> \"results\": {\n \"cnf-certification-tests_junit\": {\n \"testsuite\": {\n \"-errors\": \"0\",\n \"-failures\": \"2\",\n \"-name\": \"CNF Certification Test Suite\",\n \"-tests\": \"14\",\n ...\n</code></pre> <p>Reference</p> <p>For more details on the contents of the claim file</p> <ul> <li>Guide.</li> </ul>"},{"location":"test-output/#execution-logs","title":"Execution logs","text":"<p>The test suite also saves a copy of the execution logs at [test output directory]/certsuite.log</p>"},{"location":"test-output/#results-artifacts-zip-file","title":"Results artifacts zip file","text":"<p>After running all the test cases, a compressed file will be created with all the results files and web artifacts to review them. The file has a UTC date-time prefix and looks like this:</p> <p>20230620-110654-cnf-test-results.tar.gz</p> <p>The \u201c20230620-110654\u201d sample prefix means \u201cJune-20th 2023, 11:06:54\u201d</p> <p>This is the content of the tar.gz file:</p> <ul> <li>claim.json</li> <li>cnf-certification-tests_junit.xml (Only if enabled via <code>TNF_ENABLE_XML_CREATION</code> environment variable)</li> <li>claimjson.js</li> <li>classification.js</li> <li>results.html</li> </ul> <p>This file serves two different purposes:</p> <ol> <li>Make it easier to store and send the test results for review.</li> <li>View the results in the html web page. In addition, the web page (either results-embed.html or results.html) has a selector for workload type and allows the partner to introduce feedback for each of the failing test cases for later review from Red Hat. It\u2019s important to note that this web page needs the <code>claimjson.js</code> and <code>classification.js</code> files to be in the same folder as the html files to work properly.</li> </ol>"},{"location":"test-output/#show-results-after-running-the-test-code","title":"Show Results after running the test code","text":"<p>A standalone HTML page is available to decode the results. For more details, see: https://github.com/test-network-function/parser</p>"},{"location":"test-output/#compare-claim-files-from-two-different-test-suite-runs","title":"Compare claim files from two different Test Suite runs","text":"<p>Partners can use the <code>tnf claim compare</code> tool in order to compare two claim files. The differences are shown in a table per section. This tool can be helpful when the result of some test cases is different between two (consecutive) runs, as it shows configuration differences in both the Test Suite config and the cluster nodes that could be the root cause for some of the test cases results discrepancy.</p> <p>All the compared sections, except the test cases results are compared blindly, traversing the whole json tree and sub-trees to get a list of all the fields and their values. Three tables are shown:</p> <ul> <li>Differences: same fields with different values.</li> <li>Fields in claim 1 only: json fields in claim file 1 that don\u2019t exist in claim 2.</li> <li>Fields in claim 2 only: json fields in claim file 2 that don\u2019t exist in claim 1.</li> </ul> <p>Let\u2019s say one of the nodes of the claim.json file contains this struct:</p> <pre><code>{\n  \"field1\": \"value1\",\n  \"field2\": {\n    \"field3\": \"value2\",\n    \"field4\": {\n      \"field5\": \"value3\",\n      \"field6\": \"value4\"\n    }\n  }\n}\n</code></pre> <p>When parsing that json struct fields, it will produce a list of fields like this:</p> <pre><code>/field1=value1\n/field2/field3=value2\n/field2/field4/field5=value3\n/field2/field4/field6=finalvalue2\n</code></pre> <p>Once this list of field\u2019s path+value strings has been obtained from both claim files, it is compared in order to find the differences or the fields that only exist on each file.</p> <p>This is a fake example of a node \u201cclus0-0\u201d whose first CNI (index 0) has a different cniVersion and the ipMask flag of its first plugin (also index 0) has changed to false in the second run. Also, the plugin has another \u201cnewFakeFlag\u201d config flag in claim 2 that didn\u2019t exist in clam file 1.</p> <pre><code>...\nCNIs: Differences\nFIELD                           CLAIM 1      CLAIM 2\n/clus0-0/0/cniVersion           1.0.0        1.0.1\n/clus0-1/0/plugins/0/ipMasq     true         false\n\nCNIs: Only in CLAIM 1\n&lt;none&gt;\n\nCNIs: Only in CLAIM 2\n/clus0-1/0/plugins/0/newFakeFlag=true\n...\n</code></pre> <p>Currently, the following sections are compared, in this order:</p> <ul> <li>claim.versions</li> <li>claim.Results</li> <li>claim.configurations.Config</li> <li>claim.nodes.cniPlugins</li> <li>claim.nodes.csiDriver</li> <li>claim.nodes.nodesHwInfo</li> <li>claim.nodes.nodeSummary</li> </ul>"},{"location":"test-output/#how-to-build-the-certsuite-tool","title":"How to build the certsuite tool","text":"<p>The certsuite tool is located in the repo\u2019s <code>cmd/tnf</code> folder. In order to compile it, just run:</p> <pre><code>make build-certsuite-tool\n</code></pre>"},{"location":"test-output/#examples","title":"Examples","text":""},{"location":"test-output/#compare-a-claim-file-against-itself-no-differences-expected","title":"Compare a claim file against itself: no differences expected","text":""},{"location":"test-output/#different-test-cases-results","title":"Different test cases results","text":"<p>Let\u2019s assume we have two claim files, claim1.json and claim2.json, obtained from two Test Suite runs in the same cluster.</p> <p>During the second run, there was a test case that failed. Let\u2019s simulate it modifying manually the second run\u2019s claim file to switch one test case\u2019s state from \u201cpassed\u201d to \u201cfailed\u201d.</p>"},{"location":"test-output/#different-cluster-configurations","title":"Different cluster configurations","text":"<p>First, let\u2019s simulate that the second run took place in a cluster with a different OCP version. As we store the OCP version in the claim file (section claim.versions), we can also modify it manually. The versions section comparison appears at the very beginning of the <code>tnf claim compare</code> output:</p> <p>Now, let\u2019s simulate that the cluster was a bit different when the second Test Suite run was performed. First, let\u2019s make a manual change in claim2.json to emulate a different CNI version in the first node.</p> <p>Finally, we\u2019ll simulate that, for some reason, the first node had one label removed when the second run was performed:</p>"},{"location":"test-run/","title":"Run","text":""},{"location":"test-run/#run-the-test-suite","title":"Run the Test Suite","text":"<p>The Test Suite can be run using the Certsuite tool directly or through a container.</p> <p>To run the Test Suite direct use:</p> <pre><code>./certsuite run -l &lt;label-filter&gt; -c &lt;tnf-config&gt; -k &lt;kubeconfig&gt; -o &lt;output-dir&gt; [&lt;flags&gt;]\n</code></pre> <p>If the kubeconfig is not provided the value of the <code>KUBECONFIG</code> environment variable will be taken by default.</p> <p>The CLI output will show the following information:</p> <ul> <li>Details of the Certsuite and claim file versions, the test case filter used and the location of the output files.</li> <li>The results for each test case grouped into test suites (the most recent log line is shown live as each test executes).</li> <li>Table with the number of test cases that have passed/failed or been skipped per test suite.</li> <li>The log lines produced by each test case that has failed.</li> </ul> <p>Once the test run has completed, the test results can be visualized by opening the <code>results.html</code> website in a web browser and loading the <code>claim.json</code> file.</p> <p>For more information on how to analyze the results see Test Output.</p>"},{"location":"test-run/#building-the-certsuite-tool-executable","title":"Building the Certsuite tool executable","text":"<p>The Certsuite binary can be built as follows:</p> <pre><code>make build-certsuite-tool\n</code></pre>"},{"location":"test-run/#test-labels","title":"Test labels","text":"<p>The test cases cases have several labels to allow for different types of groupings when selecting which to run. These are the following:</p> <ul> <li>The name of the test case</li> <li>The name of the test suite</li> <li>The category of the test case (common, telco, faredge, extended)</li> </ul> <p>These labels can be combined with some operators to create label filters that match any condition. For example:</p> <ul> <li>The label filter \u201cobservability,access-control\u201d will match the test suites observability and access-control.</li> <li>The label filter \u201coperator &amp;&amp; !operator-crd-versioning\u201d will match the operator test suite without the operator_crd_versioning test case.</li> <li>To select all the test cases the all label filter can be used.</li> </ul> <p>To view which test cases will run for a specific label or label filter use the flag <code>--list</code>.</p> <p>See the CATALOG.md to find all test labels.</p>"},{"location":"test-run/#selected-flags-description","title":"Selected flags description","text":"<p>The following is a non-exhaustive list of the most common flags that the <code>certsuite run</code> command accepts. To see the complete list use the <code>-h, --help</code> flag.</p> <ul> <li><code>-l, --label-filter</code>: Label expression to filter test cases. Can be a test suite or list or test suites, such as <code>\"observability,access-control\"</code> or a more complex expression with logical operators such as <code>\"access-control &amp;&amp; !access-control-sys-admin-capability\"</code>.</li> </ul> <p>Note</p> <p>If <code>-l</code> is not specified, the Test Suite will run in \u2018diagnostic\u2019 mode. In this mode, no test case will run: it will only get information from the cluster (PUTs, CRDs, nodes info, etc\u2026) to save it in the claim file. This can be used to make sure the configuration was properly set and the autodiscovery found the right pods/crds\u2026</p> <ul> <li> <p><code>-o, --output-dir</code>: Path of the local directory where test results (claim.json), the execution logs (certsuite.log), and the results artifacts file (results.tar.gz) will be available from after the container exits.</p> </li> <li> <p><code>-k, --kubeconfig</code>: Path to the Kubeconfig file of the target cluster.</p> </li> <li> <p><code>-c, --config-file</code>: Path to the <code>tnf_config.yml</code> file.</p> </li> <li> <p><code>--preflight-dockerconfig</code>: Path to the Dockerconfig file to be used by the Preflight test suite</p> </li> <li> <p><code>--offline-db</code>: Path to an offline DB to check the certification status of container images, operators and helm charts. Defaults to the DB included in the test container image.</p> </li> </ul> <p>Note</p> <p>See the OCT tool for more information on how to create this DB.</p>"},{"location":"test-run/#using-the-container-image","title":"Using the container image","text":"<p>The only prerequisite for running the Test Suite in container mode is having Docker or Podman installed.</p>"},{"location":"test-run/#pull-the-test-image","title":"Pull the test image","text":"<p>The test image is available at this repository and can be pulled using:</p> <pre><code>docker pull quay.io/testnetworkfunction/cnf-certification-test:&lt;image-tag&gt;\n</code></pre> <p>The image tag can be <code>latest</code> to select the latest release, <code>unstable</code> to fetch the image built with the latest commit in the repository or any existing version number such as <code>v5.2.1</code>.</p>"},{"location":"test-run/#launch-the-test-suite","title":"Launch the Test Suite","text":"<p>The Test Suite requires 3 files that must be provided to the test container:</p> <ul> <li>The Kubeconfig for the target cluster.</li> <li>The Dockerconfig of the local Docker installation (only for the Preflight test suite).</li> <li>The <code>tnf_config.yml</code>.</li> </ul> <p>To reduce the number of shared volumes with the test container in the example below those files are copied into a folder called \u201cconfig\u201d. Also, another folder to contain the output files called \u201cresults\u201d has been created. The files saved in the output directory after the test run are:</p> <ul> <li>A <code>claim.json</code> file with the test results.</li> <li>A <code>certsuite.log</code> file with the execution logs.</li> <li>A <code>.tar.gz</code> file with the above two files and an additional <code>results.html</code> file to visualize the results in a website.</li> </ul> <pre><code>docker run --rm --network host \\\n  -v &lt;path-to-local-dir&gt;/config:/usr/certsuite/config:Z \\\n  -v &lt;path-to-local-dir&gt;/results:/usr/certsuite/results:Z \\\n  quay.io/testnetworkfunction/cnf-certification-test:latest \\\n  certsuite run \\\n  --kubeconfig=/usr/certsuite/config/kubeconfig \\\n  --preflight-dockerconfig=/usr/certsuite/config/dockerconfig \\\n  --config-file=/usr/certsuite/config/tnf_config.yml \\\n  --output-dir=/usr/certsuite/results \\\n  --label-filter=all\n</code></pre>"},{"location":"test-spec-implementation/","title":"Implementation","text":""},{"location":"test-spec-implementation/#test-case-implementation","title":"Test Case Implementation","text":"<p>This section explains the implementation of some test cases so that users can have a better idea on what the is code actually doing and, in some cases, which configuration fields are relevant to them.</p>"},{"location":"test-spec-implementation/#lifecycle-deployment-scaling-and-lifecycle-statefulset-scaling","title":"lifecycle-deployment-scaling and lifecycle-statefulset-scaling","text":"<p>For each discovered deployment in the target namespaces, the test case will try to modify its replica count to check whether pod\u2019s can be correctly removed and re-deployed in the cluster.</p> <p></p> <p>As depicted in the image, the way to modify the number of replicas varies depending on whether the deployment/statefulset\u2019s replica count is managed by an HPA, a CR or it\u2019s just a standalone resource.</p> <ul> <li>If the deployment/statefulset is an standalone resource, the test case just decreases the <code>.spec.replicas</code> field by one, waits for that pod to be removed and then it restores the original number increasing the field by one again. The test case passes as soon as the new pod is up and running again.</li> <li>If there\u2019s an Horizontal Pod Autoscaler (HPA) managing the resource, the test case decreases and increases the HPA\u2019s MaxReplicas field accordingly.</li> <li>If the owner of the resource is a CR, the test makes sure that CR\u2019s CRD matches any of the suffixes in the tnf_config.yaml\u2019s targetCrdFilters field. If there\u2019s a match, the test case is skipped as it\u2019s there\u2019s another test case called lifecycle-crd-scaling that will test it. Otherwise it will flagged as failed.</li> </ul>"},{"location":"test-spec-implementation/#lifecycle-crd-scaling","title":"lifecycle-crd-scaling","text":"<p>During the program\u2019s startup, an autodiscovery phase is performed where all the CRDs and their existing CRs in the target namespaces are stored to be tested later. Only CRs whose CRD\u2019s suffix matches at least one of the targetCrdFilters and has an scale subresource will be selected as test targets.</p> <p>For every CR under test, a similar approach to the scaling of deployments and statefulsets is done.</p> <p></p>"},{"location":"test-spec/","title":"Test Specs","text":""},{"location":"test-spec/#test-specifications","title":"Test Specifications","text":""},{"location":"test-spec/#available-test-specs","title":"Available Test Specs","text":"<p>There are two categories for workload tests.</p> <ul> <li>General</li> </ul> <p>These tests are designed to test any commodity workload running on OpenShift, and include specifications such as <code>Default</code> network connectivity.</p> <ul> <li>Workload-Specific</li> </ul> <p>These tests are designed to test some unique aspects of the workload under test are behaving correctly. This could include specifications such as issuing a <code>GET</code> request to a web server, or passing traffic through an IPSEC tunnel.</p>"},{"location":"test-spec/#general-tests","title":"General tests","text":"<p>These tests belong to multiple suites that can be run in any combination as is appropriate for the workload under test.</p> <p>Info</p> <p>Test suites group tests by the topic areas.</p> Suite Test Spec Description Minimum OpenShift Version <code>access-control</code> The access-control test suite is used to test  service account, namespace and cluster/pod role binding for the pods under test. It also tests the pods/containers configuration. 4.6.0 <code>affiliated-certification</code> The affiliated-certification test suite verifies that the containers and operators discovered or listed in the configuration file are certified by Redhat 4.6.0 <code>lifecycle</code> The lifecycle test suite verifies the pods deployment, creation, shutdown and  survivability. 4.6.0 <code>networking</code> The networking test suite contains tests that check connectivity and networking config related best practices. 4.6.0 <code>operator</code> The operator test suite is designed to test basic Kubernetes Operator functionality. 4.6.0 <code>platform-alteration</code> verifies that key platform configuration is not modified by the workload under test 4.6.0 <code>observability</code> the observability test suite contains tests that check workload logging is following best practices and that CRDs have status fields 4.6.0 <p>Info</p> <p>Please refer CATALOG.md for more details.</p>"},{"location":"test-spec/#workload-specific-tests","title":"Workload-specific tests","text":"<p>TODO</p>"},{"location":"workload-developers/","title":"Workload Developers","text":""},{"location":"workload-developers/#workload-guidelines-for-developers","title":"Workload Guidelines for developers","text":"<p>Developers of Kubernetes workloads, particularly those targeting certification with Red Hat on OpenShift, can use this suite to test the interaction of their workload with OpenShift.  If interested in certification please contact Red Hat.</p> <p>Requirements</p> <ul> <li>OpenShift 4.10 installation to run the workload</li> <li>At least one extra machine to host the test suite</li> </ul>"},{"location":"workload-developers/#to-add-private-test-cases","title":"To add private test cases","text":"<p>Refer this documentation https://github.com/test-network-function/cnfextensions</p> <p>Reference</p> <p>cnf-certification-test-partner repository provides sample example to model the test setup.</p>"},{"location":"cluster-deploy/","title":"Index","text":""},{"location":"cluster-deploy/#how-to-deploy-the-cnf-cert-suite-app-inside-a-kubernetesopenshift-cluster","title":"How to deploy the CNF Cert Suite App inside a Kubernetes/Openshift cluster","text":"<p>This is a developer\u2019s guide to deploy a Pod in a kubernetes/Openshift cluster that runs the CNF Cert Suite app inside.</p> <p>This folder contains two files:</p> <ul> <li>./cnf-certsuite.yaml</li> <li>./kustomization.yaml</li> </ul>"},{"location":"cluster-deploy/#cnf-certsuiteyaml","title":"cnf-certsuite.yaml","text":"<p>This file contains all the kubernetes templates for deploying the CNF Cert Suite inside a Pod named \u201ccnf-certsuite\u201d in a namespace also named \u201ccnf-certsuite\u201d. In order to deploy the pod, just write:</p> <pre><code>oc apply -f k8s/cnf-certsuite.yaml\nnamespace/cnf-certsuite created\nclusterrole.rbac.authorization.k8s.io/cnf-certsuite-cr created\nclusterrolebinding.rbac.authorization.k8s.io/cnf-certsuite-crb created\nconfigmap/cnf-certsuite-config created\nsecret/cnf-certsuite-preflight-dockerconfig created\npod/cnf-certsuite created\n</code></pre> <p>The first thing in that yaml is the namespace, so it\u2019s the first resource that will be created in the cluster. Then, a cluster role and its cluster role binding will be created. This cluster role is needed because the CNF Cert Suite needs access to all the resources in the whole cluster.</p> <p>Then, there\u2019s a configMap with the whole config (tnf_config.yaml) that will be used by the pod to create the tnf_config.yaml file inside a volume folder. Also, there\u2019s a secret with the preflight\u2019s dockerconfig file content that will also be used by the CNF Cert Suitep pod.</p> <p>The CNF Cert Suite pod is the last resource defined in the cnf-certsuite.yaml file. It has only one container that uses the quay.io/testnetworkfunction/cnf-certification-test:latest tag of the CNF Cert Suite. The command slice of this container has a hardcoded labels to run as many test cases as possible, excluding the intrusive ones.</p>"},{"location":"cluster-deploy/#kustomizationyaml","title":"kustomization.yaml","text":"<p>This kustomization file allows the deployment of the CNF Cert Suite using this command:</p> <pre><code>oc kustomize k8s/ | oc apply -f -\n</code></pre> <p>The <code>kustomization</code> tool used by <code>oc</code> will parse the content of the ./kustomization.yaml file, which consists of a set of \u201ctransformers\u201d over the resources defined in ./cnf-certsuite.yaml.</p> <p>By default, that command will deploy the CNF Cert Suite Pod without any mutation: it will be deployed in the same namespace and with the same configuration than using the <code>oc apply -f k8s/cnf-certsuite.yaml</code>.</p> <p>But there are the three example of modifications included in ./kustomization.yaml that can be used out of the box that can be handy:</p> <ol> <li>The namespace and the prefix/suffix of each resource\u2019s name. By default, the ./cnf-certsuite.yaml uses the namespace \u201ccnf-certsuite\u201d to deploy all the reources (except the cluster role and the cluster role binding), but this can be changed uncommenting the line that starts with <code>namespace:</code>. It\u2019s highly recommended to uncomment at least one of suffixName/prefixName so unique cluster role &amp; cluster role-bindings can be created for each CNF Cert Pod. This way, you could run more than one CNF Cert Pod in the same cluster!.</li> <li>The (ginkgo) labels expression, in case you want to run different test cases. Uncomment the object that starts with \u201cpatches:\u201d. The commented example changes the command to use the \u201cpreflight\u201d label only.</li> <li>The value of the TNF_NON_INTRUSIVE_ONLY env var. Uncomment the last object that starts with \u201cpatches:\u201d. The commented example changes the TNF_NON_INTRUSIVE_ONLY to false, so all the intrusive TCs will run in case the lifecycle TCs are selected to run by the appropriate labels.</li> </ol> <p>In case both (1) and (2) wants to be used, just create a list of patches like this:</p> <pre><code>patches:\n  - target:\n      version: v1\n      kind: Pod\n      name: cnf-certsuite\n    patch: |\n      - op: replace\n        path: /spec/containers/0/args/1\n        value: |\n          ./run-cnf-suites.sh -l 'preflight' ; sleep inf\n  - target:\n      version: v1\n      kind: Pod\n      name: cnf-certsuite\n    patch: |\n      - op: replace\n        path: /spec/containers/0/env/0/value\n        value: false\n</code></pre>"}]}