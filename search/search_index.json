{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>This repository provides a set of test cases to verify the conformance of a workload with the Red Hat Best Practices for Kubernetes.</p> <p>Workload</p> <p>The app (containers/pods/operators) we want to certify according Telco partner/Red Hat\u2019s best practices.</p> <p>Red Hat Best Practices Test Suite for Kubernetes</p> <p>The tool we use to certify a workload.</p> <p>The purpose of the tests and the framework is to test the interaction of the workload with OpenShift Container Platform (OCP).  </p> <p>Info</p> <p>This test suite is provided for the workload developers to test their workload\u2019s readiness for certification. Please see the Developers\u2019 Guide for more information.</p> <p>Features</p> <ul> <li> <p>The test suite generates a report (<code>claim.json</code>) and saves the test execution log (<code>cnf-certsuite.log</code>) in a configurable output directory.</p> </li> <li> <p>The catalog of the existing test cases and test building blocks are available in CATALOG.md</p> </li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>There are 3 building blocks in the above framework.</p> <ul> <li> <p>the <code>CNF</code> represents the workload to be certified. The Test Suite identifies the resources (containers/pods/operators etc) belonging to the workload via labels or static data entries in the Config File</p> </li> <li> <p>the <code>Certification container/exec</code> is the Test Suite running on the platform or in a container. The executable verifies the workload under test configuration and its interactions with OpenShift</p> </li> <li> <p>the <code>Debug</code> pods are part of a Kubernetes daemonset responsible to run various privileged commands on Kubernetes nodes. Debug pods are useful to run platform tests and test commands (e.g. ping) in container namespaces without changing the container image content. The debug daemonset is instantiated via the privileged-daemonset repository.</p> </li> </ul>"},{"location":"configuration/","title":"Test Configuration","text":""},{"location":"configuration/#red-hat-best-practices-test-suite-for-kubernetes-configuration","title":"Red Hat Best Practices Test Suite for Kubernetes configuration","text":"<p>The Red Hat Best Practices Test Suite for Kubernetes uses a YAML configuration file to certify a specific workload. This file specifies the workload\u2019s resources to be certified, as well as any exceptions or other general configuration options.</p> <p>By default a file named tnf_config.yml will be used. Here\u2019s an example of the Config File. For a description of each config option see the section Config File options.</p>"},{"location":"configuration/#config-generator","title":"Config Generator","text":"<p>The Config File can be created using the Config Generator, which is part of the TNF tool shipped with the Test Suite. The purpose of this particular tool is to help users configuring the Test Suite providing a logical structure of the available options as well as the information required to make use of them. The result is a Config File in YAML format that will be parsed to adapt the verification process to a specific workload.</p> <p>To compile the TNF tool:</p> <pre><code>make build-tnf-tool\n</code></pre> <p>To launch the Config Generator:</p> <pre><code>./tnf generate config\n</code></pre> <p>Here\u2019s an example of how to use the tool:</p>"},{"location":"configuration/#config-file-options","title":"Config File options","text":""},{"location":"configuration/#workload-resources","title":"Workload resources","text":"<p>These options allow configuring the workload resources to be verified. Only the resources that the workload uses are required to be configured. The rest can be left empty. Usually a basic configuration includes Namespaces and Pods at least.</p> <p>Note</p> <p>Using the number of labels to determine how to get the resources under test.  If there are labels defined, we get the list of pods, statefulsets, deployments, csvs, by fetching the resources matching the labels. Otherwise, if the labels are not defined, we only test the resources that are in the namespaces under test (defined in tnf_config.yml).</p>"},{"location":"configuration/#targetnamespaces","title":"targetNameSpaces","text":"<p>The namespaces in which the workload under test will be deployed.</p> <pre><code>targetNameSpaces:\n  - name: tnf\n</code></pre>"},{"location":"configuration/#podsundertestlabels","title":"podsUnderTestLabels","text":"<p>The labels that each Pod of the workload under test must have to be verified by the Test Suite.</p> <p>Highly recommended</p> <p>The labels should be defined in Pod definition rather than added after the Pod is created, as labels added later on will be lost in case the Pod gets rescheduled. In the case of Pods defined as part of a Deployment, it\u2019s best to use the same label as the one defined in the spec.selector.matchLabels section of the Deployment YAML. The prefix field can be used to avoid naming collision with other labels.</p> <pre><code>podsUnderTestLabels:\n  - \"test-network-function.com/generic: target\"\n</code></pre>"},{"location":"configuration/#operatorsundertestlabels","title":"operatorsUnderTestLabels","text":"<p>The labels that each operator\u2019s CSV of the workload under test must have to be verified by the Test Suite.</p> <p>If a new label is used for this purpose make sure it is added to the workload operator\u2019s CSVs.</p> <pre><code>operatorsUnderTestLabels:\n  - \"test-network-function.com/operator: target\" \n</code></pre>"},{"location":"configuration/#targetcrdfilters","title":"targetCrdFilters","text":"<p>The CRD name suffix used to filter the workload\u2019s CRDs among all the CRDs present in the cluster. For each CRD it can also be specified if it\u2019s scalable or not in order to avoid some lifecycle test cases.</p> <pre><code>targetCrdFilters:\n - nameSuffix: \"group1.tnf.com\"\n   scalable: false\n - nameSuffix: \"anydomain.com\"\n   scalable: true\n</code></pre> <p>With the config show above, all CRD names in the cluster whose names have the suffix group1.tnf.com or anydomain.com ( e.g. crd1.group1.tnf.com or mycrd.mygroup.anydomain.com) will be tested.</p>"},{"location":"configuration/#manageddeployments-managedstatefulsets","title":"managedDeployments / managedStatefulSets","text":"<p>The Deployments/StatefulSets managed by a Custom Resource whose scaling is controlled using the \u201cscale\u201d subresource of the CR.</p> <p>The CRD defining that CR should be included in the CRD filters with the scalable property set to true. If so, the test case lifecycle-{deployment/statefulset}-scaling will be skipped, otherwise it will fail.</p> <pre><code>managedDeployments:\n  - name: jack\nmanagedStatefulsets:\n  - name: jack\n</code></pre>"},{"location":"configuration/#junit-xml-file-creation","title":"JUnit XML File Creation","text":"<p>The test suite has the ability to create the JUNit XML File output containing the test ID and the corresponding test result.</p> <p>To enable this, set:</p> <pre><code>export TNF_ENABLE_XML_CREATION=true\n</code></pre> <p>This will create a file named <code>cnf-certification-test/cnf-certification-tests_junit.xml</code>.</p>"},{"location":"configuration/#enable-running-container-against-openshift-local","title":"Enable running container against OpenShift Local","text":"<p>While running the test suite as a container, you can enable the container to be able to reach the local CRC instance by setting:</p> <pre><code>export TNF_ENABLE_CRC_TESTING=true\n</code></pre> <p>This utilizes the <code>--add-host</code> flag in Docker to be able to point <code>api.crc.testing</code> to the host gateway.</p>"},{"location":"configuration/#exceptions","title":"Exceptions","text":"<p>These options allow adding exceptions to skip several checks for different resources. The exceptions must be justified in order to satisfy the Red Hat Best Practices for Kubernetes.</p>"},{"location":"configuration/#acceptedkerneltaints","title":"acceptedKernelTaints","text":"<p>The list of kernel modules loaded by the workload that make the Linux kernel mark itself as tainted but that should skip verification.</p> <p>Test cases affected: platform-alteration-tainted-node-kernel.</p> <pre><code>acceptedKernelTaints:\n  - module: vboxsf\n  - module: vboxguest\n</code></pre>"},{"location":"configuration/#skiphelmchartlist","title":"skipHelmChartList","text":"<p>The list of Helm charts that the workload uses whose certification status will not be verified.</p> <p>If no exception is configured, the certification status for all Helm charts will be checked in the OpenShift Helms Charts repository.</p> <p>Test cases affected: affiliated-certification-helmchart-is-certified.</p> <pre><code>skipHelmChartList:\n  - name: coredns\n</code></pre>"},{"location":"configuration/#validprotocolnames","title":"validProtocolNames","text":"<p>The list of allowed protocol names to be used for container port names.</p> <p>The name field of a container port must be of the form protocol[-suffix] where protocol must be allowed by default or added to this list. The optional suffix can be chosen by the application. Protocol names allowed by default: grpc, grpc-web, http, http2, tcp, udp.</p> <p>Test cases affected: manageability-container-port-name-format.</p> <pre><code>validProtocolNames:\n  - \"http3\"\n  - \"sctp\"\n</code></pre>"},{"location":"configuration/#servicesignorelist","title":"servicesIgnoreList","text":"<p>The list of Services that will skip verification.</p> <p>Services included in this list will be filtered out at the autodiscovery stage and will not be subject to checks in any test case.</p> <p>Tests cases affected: networking-dual-stack-service, access-control-service-type.</p> <pre><code>servicesignorelist:\n  - \"hazelcast-platform-controller-manager-service\"\n  - \"hazelcast-platform-webhook-service\"\n  - \"new-pro-controller-manager-metrics-service\"\n</code></pre>"},{"location":"configuration/#skipscalingtestdeployments-skipscalingteststatefulsets","title":"skipScalingTestDeployments / skipScalingTestStatefulSets","text":"<p>The list of Deployments/StatefulSets that do not support scale in/out operations.</p> <p>Deployments/StatefulSets included in this list will skip any scaling operation check.</p> <p>Test cases affected: lifecycle-deployment-scaling, lifecycle-statefulset-scaling.</p> <pre><code>skipScalingTestDeployments:\n  - name: deployment1\n    namespace: tnf\nskipScalingTestStatefulSetNames:\n  - name: statefulset1\n    namespace: tnf\n</code></pre>"},{"location":"configuration/#red-hat-best-practices-test-suite-settings","title":"Red Hat Best Practices Test Suite settings","text":""},{"location":"configuration/#debugdaemonsetnamespace","title":"debugDaemonSetNamespace","text":"<p>This is an optional field with the name of the namespace where a privileged DaemonSet will be deployed. The namespace will be created in case it does not exist. In case this field is not set, the default namespace for this DaemonSet is cnf-suite.</p> <pre><code>debugDaemonSetNamespace: cnf-cert\n</code></pre> <p>This DaemonSet, called tnf-debug is deployed and used internally by the Test Suite tool to issue some shell commands that are needed in certain test cases. Some of these test cases might fail or be skipped in case it wasn\u2019t deployed correctly.</p>"},{"location":"configuration/#other-settings","title":"Other settings","text":"<p>The autodiscovery mechanism will attempt to identify the default network device and all the IP addresses of the Pods it needs for network connectivity tests, though that information can be explicitly set using annotations if needed.</p>"},{"location":"configuration/#pod-ips","title":"Pod IPs","text":"<ul> <li>The k8s.v1.cni.cncf.io/networks-status annotation is checked and all IPs from it are used. This annotation is automatically managed in OpenShift but may not be present in K8s.</li> <li>If it is not present, then only known IPs associated with the Pod are used (the Pod .status.ips field).</li> </ul>"},{"location":"configuration/#network-interfaces","title":"Network Interfaces","text":"<ul> <li>The k8s.v1.cni.cncf.io/networks-status annotation is checked and the interface from the first entry found with \u201cdefault\u201d=true is used. This annotation is automatically managed in OpenShift but may not be present in K8s.</li> </ul> <p>The label test-network-function.com/skip_connectivity_tests excludes Pods from all connectivity tests.</p> <p>The label test-network-function.com/skip_multus_connectivity_tests excludes Pods from Multus connectivity tests. Tests on the default interface are still run.</p>"},{"location":"configuration/#affinity-requirements","title":"Affinity requirements","text":"<p>For workloads that require Pods to use Pod or Node Affinity rules, the label AffinityRequired: true must be included on the Pod YAML. This will ensure that the affinity best practices are tested and prevent any test cases for anti-affinity to fail.</p>"},{"location":"developers/","title":"Developers","text":""},{"location":"developers/#steps","title":"Steps","text":"<p>To test the newly added test / existing tests locally, follow the steps</p> <ul> <li>Clone the repo</li> <li> <p>Set runtime environment variables, as per the requirement.</p> <p>For example, to deploy partner deployments in a custom namespace in the test config.</p> <pre><code>targetNameSpaces:\n  - name: mynamespace\n</code></pre> </li> <li> <p>Also, skip intrusive tests</p> </li> </ul> <pre><code>export TNF_NON_INTRUSIVE_ONLY=true\n</code></pre> <ul> <li> <p>Set K8s config of the cluster where test pods are running</p> <pre><code>export KUBECONFIG=&lt;&lt;mypath/.kube/config&gt;&gt;\n</code></pre> </li> <li> <p>Execute test suite, which would build and run the suite</p> <p>For example, to run <code>networking</code> tests</p> <pre><code>./script/development.sh networking\n</code></pre> </li> </ul>"},{"location":"developers/#dependencies-on-other-pr","title":"Dependencies on other PR","text":"<p>If you have dependencies on other Pull Requests, you can add a comment like that:</p> <pre><code>Depends-On: &lt;url of the PR&gt;\n</code></pre> <p>and the dependent PR will automatically be extracted and injected in your change during the GitHub Action CI jobs and the DCI jobs.</p>"},{"location":"exception/","title":"Exception Process","text":""},{"location":"exception/#exception-process","title":"Exception Process","text":"<p>There may exist some test cases which needs to fail always. The exception raised by the failed tests is published to Red Hat website for that partner.</p> <p>CATALOG provides the details of such exception.</p>"},{"location":"reference/","title":"Helpful Links","text":"<ul> <li>Contribution Guidelines</li> <li>CATALOG</li> <li>Best Practices Document v1.3</li> </ul>"},{"location":"runtime-env/","title":"Runtime environment variables","text":""},{"location":"runtime-env/#runtime-environment-variables","title":"Runtime environment variables","text":"<p>To run the test suite, some runtime environment variables are to be set.</p>"},{"location":"runtime-env/#ocp-412-labels","title":"OCP &gt;=4.12 Labels","text":"<p>The following labels need to be added to your default namespace in your cluster if you are running OCP &gt;=4.12:</p> <pre><code>pod-security.kubernetes.io/enforce: privileged\npod-security.kubernetes.io/enforce-version: latest\n</code></pre> <p>You can manually label the namespace with:</p> <pre><code>oc label namespace/default pod-security.kubernetes.io/enforce=privileged\noc label namespace/default pod-security.kubernetes.io/enforce-version=latest\n</code></pre>"},{"location":"runtime-env/#disable-intrusive-tests","title":"Disable intrusive tests","text":"<p>To skip intrusive tests which may disrupt cluster operations, issue the following:</p> <pre><code>export TNF_NON_INTRUSIVE_ONLY=true\n</code></pre> <p>Likewise, to enable intrusive tests, set the following:</p> <pre><code>export TNF_NON_INTRUSIVE_ONLY=false\n</code></pre> <p>Intrusive tests are enabled by default.</p>"},{"location":"runtime-env/#preflight-integration","title":"Preflight Integration","text":"<p>When running the <code>preflight</code> suite of tests, there are a few environment variables that will need to be set:</p> <p><code>PFLT_DOCKERCONFIG</code> is a required variable for running the preflight test suite. This provides credentials to the underlying preflight library for being able to pull/manipulate images and image bundles for testing.</p> <p>When running as a container, the docker config is mounted to the container via volume mount.</p> <p>When running as a standalone binary, the environment variables are consumed directly from your local machine.</p> <p>See more about this variable here.</p> <p><code>TNF_ALLOW_PREFLIGHT_INSECURE</code> (default: false) is required set to <code>true</code> if you are running against a private container registry that has self-signed certificates.</p>"},{"location":"runtime-env/#disconnected-environment","title":"Disconnected environment","text":"<p>In a disconnected environment, only specific versions of images are mirrored to the local repo. For those environments, the partner pod image <code>quay.io/testnetworkfunction/cnf-test-partner</code> and debug pod image <code>quay.io/testnetworkfunction/debug-partner</code> should be mirrored and <code>TNF_PARTNER_REPO</code> should be set to the local repo, e.g.:</p> <pre><code>export TNF_PARTNER_REPO=registry.dfwt5g.lab:5000/testnetworkfunction\n</code></pre> <p>Note that you can also specify the debug pod image to use with <code>SUPPORT_IMAGE</code> environment variable, default to <code>debug-partner:5.0.8</code>.</p>"},{"location":"test-container/","title":"Prebuilt container","text":""},{"location":"test-container/#test","title":"Test","text":"<p>The tests can be run within a prebuilt container in the OCP cluster.</p> <p>Prerequisites for the OCP cluster</p> <ul> <li>The cluster should have enough resources to drain nodes and reschedule pods. If that is not the case, then <code>lifecycle-pod-recreation</code> test should be skipped.</li> </ul>"},{"location":"test-container/#with-quay-test-container-image","title":"With quay test container image","text":""},{"location":"test-container/#pull-test-image","title":"Pull test image","text":"<p>The test image is available at this repository in quay.io and can be pulled using The image can be pulled using :</p> <pre><code>podman pull quay.io/testnetworkfunction/cnf-certification-test\n</code></pre>"},{"location":"test-container/#check-cluster-resources","title":"Check cluster resources","text":"<p>Some tests suites such as <code>platform-alteration</code> require node access to get node configuration like <code>hugepage</code>. In order to get the required information, the test suite does not <code>ssh</code> into nodes, but instead rely on oc debug tools. This tool makes it easier to fetch information from nodes and also to debug running pods.</p> <p><code>oc debug tool</code> will launch a new container ending with -debug suffix, and the container will be destroyed once the debug session is done. Ensure that the cluster should have enough resources to create debug pod, otherwise those tests would fail.</p> <p>Note</p> <p>It\u2019s recommended to clean up disk space and make sure there\u2019s enough resources to deploy another container image in every node before starting the tests.</p>"},{"location":"test-container/#run-the-tests","title":"Run the tests","text":"<pre><code>./run-tnf-container.sh\n</code></pre> <p>Required arguments</p> <ul> <li><code>-t</code> to provide the path of the local directory that contains tnf config files</li> <li><code>-o</code> to provide the path of the local directory where test results (claim.json), the execution logs (cnf-certsuite.log), and the results artifacts file (results.tar.gz) will be available from after the container exits.</li> </ul> <p>Warning</p> <p>This directory must exist in order for the claim file to be written.</p> <p>Optional arguments</p> <ul> <li><code>-l</code> to list the labels to be run. See Ginkgo Spec Labels for more information on how to filter tests with labels.</li> </ul> <p>Note</p> <p>If <code>-l</code> is not specified, the tnf will run in \u2018diagnostic\u2019 mode. In this mode, no test case will run: it will only get information from the cluster (PUTs, CRDs, nodes info, etc\u2026) to save it in the claim file. This can be used to make sure the configuration was properly set and the autodiscovery found the right pods/crds\u2026</p> <ul> <li> <p><code>-i</code> to provide a name to a custom TNF container image. Supports local images, as well as images from external registries.</p> </li> <li> <p><code>-k</code> to set a path to one or more kubeconfig files to be used by the container to authenticate with the cluster. Paths must be separated by a colon.</p> </li> </ul> <p>Note</p> <p>If <code>-k</code> is not specified, autodiscovery is performed.</p> <p>The autodiscovery first looks for paths in the <code>$KUBECONFIG</code> environment variable on the host system, and if the variable is not set or is empty, the default configuration stored in <code>$HOME/.kube/config</code> is checked.</p> <ul> <li><code>-n</code> to give the network mode of the container. Defaults set to <code>host</code>, which requires selinux to be disabled. Alternatively, <code>bridge</code> mode can be used with selinux if TNF_CONTAINER_CLIENT is set to <code>docker</code> or running the test as root.</li> </ul> <p>Note</p> <p>See the docker run \u2013network parameter reference for more information on how to configure network settings.</p> <ul> <li><code>-b</code> to set an external offline DB that will be used to verify the certification status of containers, helm charts and operators. Defaults to the DB included in the TNF container image.</li> </ul> <p>Note</p> <p>See the OCT tool for more information on how to create this DB.</p> <p>Command to run</p> <pre><code>./run-tnf-container.sh -k ~/.kube/config -t ~/tnf/config\n-o ~/tnf/output -l \"networking,access-control\"\n</code></pre> <p>See General tests for a list of available keywords.</p>"},{"location":"test-container/#run-with-docker","title":"Run with <code>docker</code>","text":"<p>By default, <code>run-container.sh</code> utilizes <code>podman</code>. However, an alternate container virtualization client using <code>TNF_CONTAINER_CLIENT</code> can be configured. This is particularly useful for operating systems that do not readily support <code>podman</code>.</p> <p>In order to configure the test harness to use <code>docker</code>, issue the following prior to <code>run-tnf-container.sh</code>:</p> <pre><code>export TNF_CONTAINER_CLIENT=docker\n</code></pre>"},{"location":"test-container/#output-targz-file-with-results-and-web-viewer-files","title":"Output tar.gz file with results and web viewer files","text":"<p>After running all the test cases, a compressed file will be created with all the results files and web artifacts to review them.</p> <p>By default, only the <code>claim.js</code>, the <code>cnf-certification-tests_junit.xml</code> file and this new tar.gz file are created after the test suite has finished, as this is probably all that normal partners/users will need.</p> <p>Two env vars allow to control the web artifacts and the the new tar.gz file generation:</p> <ul> <li>TNF_OMIT_ARTIFACTS_ZIP_FILE=true/false : Defaulted to false in the launch scripts. If set to true, the tar.gz generation will be skipped.</li> <li>TNF_INCLUDE_WEB_FILES_IN_OUTPUT_FOLDER=true/false : Defaulted to false in the launch scripts. If set to true, the web viewer/parser files will also be copied to the output (claim) folder.</li> </ul>"},{"location":"test-container/#with-local-test-container-image","title":"With local test container image","text":""},{"location":"test-container/#build-locally","title":"Build locally","text":"<pre><code>podman build -t cnf-certification-test:v5.0.8 \\\n  --build-arg TNF_VERSION=v5.0.8 \\\n</code></pre> <ul> <li><code>TNF_VERSION</code> value is set to a branch, a tag, or a hash of a commit that will be installed into the image</li> </ul>"},{"location":"test-container/#build-from-an-unofficial-source","title":"Build from an unofficial source","text":"<p>The unofficial source could be a fork of the TNF repository.</p> <p>Use the <code>TNF_SRC_URL</code> build argument to override the URL to a source repository.</p> <pre><code>podman build -t cnf-certification-test:v5.0.8 \\\n  --build-arg TNF_VERSION=v5.0.8 \\\n  --build-arg TNF_SRC_URL=https://github.com/test-network-function/cnf-certification-test .\n</code></pre>"},{"location":"test-container/#run-the-tests-2","title":"Run the tests 2","text":"<p>Specify the custom TNF image using the <code>-i</code> parameter.</p> <pre><code>./run-tnf-container.sh -i cnf-certification-test:v5.0.8\n-t ~/tnf/config -o ~/tnf/output -l \"networking,access-control\"\n</code></pre> <p>Note: see General tests for a list of available keywords.</p>"},{"location":"test-output/","title":"Test Output","text":""},{"location":"test-output/#test-output","title":"Test Output","text":""},{"location":"test-output/#claim-file","title":"Claim File","text":"<p>The test suite generates an output file, named claim file. This file is considered as the proof of the workload\u2019s test run, evaluated by Red Hat when certified status is considered.</p> <p>This file describes the following</p> <ul> <li>The system(s) under test</li> <li>The tests that are executed</li> <li>The outcome of the executed / skipped tests</li> </ul> <p>Files that need to be submitted for certification</p> <p>When submitting results back to Red Hat for certification, please include the above mentioned claim file, the JUnit file, and any available console logs.</p> <p>How to add a workload platform test result to the existing claim file?</p> <pre><code>go run cmd/tools/cmd/main.go claim-add --claimfile=claim.json\n--reportdir=/home/$USER/reports\n</code></pre> <p>Args: <code>--claimfile is an existing claim.json file</code> <code>--repordir :path to test results that you want to include.</code></p> <p>The tests result files from the given report dir will be appended under the result section of the claim file using file name as the key/value pair.  The tool will ignore the test result, if the key name is already present under result section of the claim file.</p> <pre><code> \"results\": {\n \"cnf-certification-tests_junit\": {\n \"testsuite\": {\n \"-errors\": \"0\",\n \"-failures\": \"2\",\n \"-name\": \"CNF Certification Test Suite\",\n \"-tests\": \"14\",\n ...\n</code></pre> <p>Reference</p> <p>For more details on the contents of the claim file</p> <ul> <li>Guide.</li> </ul>"},{"location":"test-output/#execution-logs","title":"Execution logs","text":"<p>The test suite also saves a copy of the execution logs at [test output directory]/cnf-certsuite.log</p>"},{"location":"test-output/#results-artifacts-zip-file","title":"Results artifacts zip file","text":"<p>After running all the test cases, a compressed file will be created with all the results files and web artifacts to review them. The file has a UTC date-time prefix and looks like this:</p> <p>20230620-110654-cnf-test-results.tar.gz</p> <p>The \u201c20230620-110654\u201d sample prefix means \u201cJune-20th 2023, 11:06:54\u201d</p> <p>This is the content of the tar.gz file:</p> <ul> <li>claim.json</li> <li>cnf-certification-tests_junit.xml (Only if enabled via <code>TNF_ENABLE_XML_CREATION</code> environment variable)</li> <li>claimjson.js</li> <li>classification.js</li> <li>results.html</li> </ul> <p>This file serves two different purposes:</p> <ol> <li>Make it easier to store and send the test results for review.</li> <li>View the results in the html web page. In addition, the web page (either results-embed.html or results.html) has a selector for workload type and allows the partner to introduce feedback for each of the failing test cases for later review from Red Hat. It\u2019s important to note that this web page needs the <code>claimjson.js</code> and <code>classification.js</code> files to be in the same folder as the html files to work properly.</li> </ol>"},{"location":"test-output/#show-results-after-running-the-test-code","title":"Show Results after running the test code","text":"<p>A standalone HTML page is available to decode the results. For more details, see: https://github.com/test-network-function/parser</p>"},{"location":"test-output/#compare-claim-files-from-two-different-test-suite-runs","title":"Compare claim files from two different Test Suite runs","text":"<p>Partners can use the <code>tnf claim compare</code> tool in order to compare two claim files. The differences are shown in a table per section. This tool can be helpful when the result of some test cases is different between two (consecutive) runs, as it shows configuration differences in both the Test Suite config and the cluster nodes that could be the root cause for some of the test cases results discrepancy.</p> <p>All the compared sections, except the test cases results are compared blindly, traversing the whole json tree and sub-trees to get a list of all the fields and their values. Three tables are shown:</p> <ul> <li>Differences: same fields with different values.</li> <li>Fields in claim 1 only: json fields in claim file 1 that don\u2019t exist in claim 2.</li> <li>Fields in claim 2 only: json fields in claim file 2 that don\u2019t exist in claim 1.</li> </ul> <p>Let\u2019s say one of the nodes of the claim.json file contains this struct:</p> <pre><code>{\n  \"field1\": \"value1\",\n  \"field2\": {\n    \"field3\": \"value2\",\n    \"field4\": {\n      \"field5\": \"value3\",\n      \"field6\": \"value4\"\n    }\n  }\n}\n</code></pre> <p>When parsing that json struct fields, it will produce a list of fields like this:</p> <pre><code>/field1=value1\n/field2/field3=value2\n/field2/field4/field5=value3\n/field2/field4/field6=finalvalue2\n</code></pre> <p>Once this list of field\u2019s path+value strings has been obtained from both claim files, it is compared in order to find the differences or the fields that only exist on each file.</p> <p>This is a fake example of a node \u201cclus0-0\u201d whose first CNI (index 0) has a different cniVersion and the ipMask flag of its first plugin (also index 0) has changed to false in the second run. Also, the plugin has another \u201cnewFakeFlag\u201d config flag in claim 2 that didn\u2019t exist in clam file 1.</p> <pre><code>...\nCNIs: Differences\nFIELD                           CLAIM 1      CLAIM 2\n/clus0-0/0/cniVersion           1.0.0        1.0.1\n/clus0-1/0/plugins/0/ipMasq     true         false\n\nCNIs: Only in CLAIM 1\n&lt;none&gt;\n\nCNIs: Only in CLAIM 2\n/clus0-1/0/plugins/0/newFakeFlag=true\n...\n</code></pre> <p>Currently, the following sections are compared, in this order:</p> <ul> <li>claim.versions</li> <li>claim.Results</li> <li>claim.configurations.Config</li> <li>claim.nodes.cniPlugins</li> <li>claim.nodes.csiDriver</li> <li>claim.nodes.nodesHwInfo</li> <li>claim.nodes.nodeSummary</li> </ul>"},{"location":"test-output/#how-to-build-the-tnf-tool","title":"How to build the tnf tool","text":"<p>The <code>tnf</code> tool is located in the repo\u2019s <code>cmd/tnf</code> folder. In order to compile it, just run:</p> <pre><code>make build-tnf-tool\n</code></pre>"},{"location":"test-output/#examples","title":"Examples","text":""},{"location":"test-output/#compare-a-claim-file-against-itself-no-differences-expected","title":"Compare a claim file against itself: no differences expected","text":""},{"location":"test-output/#different-test-cases-results","title":"Different test cases results","text":"<p>Let\u2019s assume we have two claim files, claim1.json and claim2.json, obtained from two Test Suite runs in the same cluster.</p> <p>During the second run, there was a test case that failed. Let\u2019s simulate it modifying manually the second run\u2019s claim file to switch one test case\u2019s state from \u201cpassed\u201d to \u201cfailed\u201d.</p>"},{"location":"test-output/#different-cluster-configurations","title":"Different cluster configurations","text":"<p>First, let\u2019s simulate that the second run took place in a cluster with a different OCP version. As we store the OCP version in the claim file (section claim.versions), we can also modify it manually. The versions section comparison appears at the very beginning of the <code>tnf claim compare</code> output:</p> <p>Now, let\u2019s simulate that the cluster was a bit different when the second Test Suite run was performed. First, let\u2019s make a manual change in claim2.json to emulate a different CNI version in the first node.</p> <p>Finally, we\u2019ll simulate that, for some reason, the first node had one label removed when the second run was performed:</p>"},{"location":"test-spec/","title":"Available Test Specs","text":""},{"location":"test-spec/#test-specifications","title":"Test Specifications","text":""},{"location":"test-spec/#available-test-specs","title":"Available Test Specs","text":"<p>There are two categories for workload tests.</p> <ul> <li>General</li> </ul> <p>These tests are designed to test any commodity workload running on OpenShift, and include specifications such as <code>Default</code> network connectivity.</p> <ul> <li>Workload-Specific</li> </ul> <p>These tests are designed to test some unique aspects of the workload under test are behaving correctly. This could include specifications such as issuing a <code>GET</code> request to a web server, or passing traffic through an IPSEC tunnel.</p>"},{"location":"test-spec/#general-tests","title":"General tests","text":"<p>These tests belong to multiple suites that can be run in any combination as is appropriate for the workload under test.</p> <p>Info</p> <p>Test suites group tests by the topic areas.</p> Suite Test Spec Description Minimum OpenShift Version <code>access-control</code> The access-control test suite is used to test  service account, namespace and cluster/pod role binding for the pods under test. It also tests the pods/containers configuration. 4.6.0 <code>affiliated-certification</code> The affiliated-certification test suite verifies that the containers and operators discovered or listed in the configuration file are certified by Redhat 4.6.0 <code>lifecycle</code> The lifecycle test suite verifies the pods deployment, creation, shutdown and  survivability. 4.6.0 <code>networking</code> The networking test suite contains tests that check connectivity and networking config related best practices. 4.6.0 <code>operator</code> The operator test suite is designed to test basic Kubernetes Operator functionality. 4.6.0 <code>platform-alteration</code> verifies that key platform configuration is not modified by the workload under test 4.6.0 <code>observability</code> the observability test suite contains tests that check workload logging is following best practices and that CRDs have status fields 4.6.0 <p>Info</p> <p>Please refer CATALOG.md for more details.</p>"},{"location":"test-spec/#workload-specific-tests","title":"Workload-specific tests","text":"<p>TODO</p>"},{"location":"test-standalone/","title":"Standalone test executable","text":""},{"location":"test-standalone/#standalone-test-executable","title":"Standalone test executable","text":"<p>Prerequisites</p> <p>The repo is cloned and all the commands should be run from the cloned repo.</p> <pre><code>mkdir ~/workspace\ncd ~/workspace\ngit clone git@github.com:test-network-function/cnf-certification-test.git\ncd cnf-certification-test\n</code></pre> <p>Note</p> <p>By default, <code>cnf-certification-test</code> emits results to <code>cnf-certification-test/cnf-certification-tests_junit.xml</code>.</p>"},{"location":"test-standalone/#1-install-dependencies","title":"1. Install dependencies","text":"<p>Depending on how you want to run the test suite there are different dependencies that will be needed.</p> <p>If you are planning on running the test suite as a container, the only pre-requisite is Docker or Podman.</p> <p>If you are planning on running the test suite as a standalone binary, there are pre-requisites that will need to be installed in your environment prior to runtime.</p> Dependency Minimum Version GoLang 1.22 golangci-lint 1.56.2 jq 1.6 OpenShift Client 4.12 <p>Other binary dependencies required to run tests can be installed using the following command:</p> <p>Note</p> <ul> <li>You must also make sure that <code>$GOBIN</code> (default <code>$GOPATH/bin</code>) is on your <code>$PATH</code>.</li> <li>Efforts to containerise this offering are considered a work in progress.</li> </ul>"},{"location":"test-standalone/#2-build-the-test-suite","title":"2. Build the Test Suite","text":"<p>In order to build the test executable, first make sure you have satisfied the dependencies.</p> <pre><code>make build-cnf-tests\n</code></pre> <p>Gotcha: The <code>make build*</code> commands run unit tests where appropriate. They do NOT test the workload.</p>"},{"location":"test-standalone/#3-test-a-workload","title":"3. Test a workload","text":"<p>A workload is tested by specifying which suites to run using the <code>run-cnf-suites.sh</code> helper script.</p> <p>Run any combination of the suites keywords listed at in the General tests section, e.g.</p> <pre><code>./run-cnf-suites.sh -l \"lifecycle\"\n./run-cnf-suites.sh -l \"networking,lifecycle\"\n./run-cnf-suites.sh -l \"operator,networking\"\n./run-cnf-suites.sh -l \"networking,platform-alteration\"\n./run-cnf-suites.sh -l \"networking,lifecycle,affiliated-certification,operator\"\n</code></pre> <p>Note</p> <p>As with \u201crun-tnf-container.sh\u201d, if <code>-l</code> is not specified here, the tnf will run in \u2018diagnostic\u2019 mode.</p> <p>By default the claim file will be output into the same location as the test executable. The <code>-o</code> argument for     <code>run-cnf-suites.sh</code> can be used to provide a new location that the output files will be saved to. For more detailed     control over the outputs, see the output of <code>cnf-certification-test.test --help</code>.</p> <pre><code>    cd cnf-certification-test &amp;&amp; ./cnf-certification-test.test --help\n</code></pre>"},{"location":"test-standalone/#run-a-single-test","title":"Run a single test","text":"<p>All tests have unique labels, which can be used to filter which tests are to be run. This is useful when debugging a single test.</p> <p>To select the test to be executed when running <code>run-cnf-suites.sh</code> with the following command-line:</p> <pre><code>./run-cnf-suites.sh -l operator-install-source\n</code></pre> <p>Note</p> <p>The test labels work the same as the suite labels, so you can select more than one test with the filtering mechanism shown before.</p>"},{"location":"test-standalone/#run-all-of-the-tests","title":"Run all of the tests","text":"<p>You can run all of the tests (including the intrusive tests and the extended suite) with the following commands:</p> <pre><code>./run-cnf-suites.sh -l all\n</code></pre>"},{"location":"test-standalone/#run-a-subset","title":"Run a subset","text":"<p>You can find all the labels attached to the tests by running the following command:</p> <pre><code>./run-cnf-suites.sh --list\n</code></pre> <p>You can also check the CATALOG.md to find all test labels.</p>"},{"location":"test-standalone/#labels-for-offline-environments","title":"Labels for offline environments","text":"<p>Some tests do require connectivity to Red Hat servers to validate certification status. To run the tests in an offline environment, skip the tests using the <code>l</code> option.</p> <pre><code>./run-cnf-suites.sh -l '!online'\n</code></pre> <p>Alternatively, if an offline DB for containers, helm charts and operators is available, there is no need to skip those tests if the environment variable <code>TNF_OFFLINE_DB</code> is set to the DB location. This DB can be generated using the OCT tool.</p> <p>Note: Only partner certified images are stored in the offline database. If Red Hat images are checked against the offline database, they will show up as not certified. The online database includes both Partner and Redhat images.</p>"},{"location":"test-standalone/#output-targz-file-with-results-and-web-viewer-files","title":"Output tar.gz file with results and web viewer files","text":"<p>After running all the test cases, a compressed file will be created with all the results files and web artifacts to review them.</p> <p>By default, only the <code>claim.js</code>, the <code>cnf-certification-tests_junit.xml</code> file and this new tar.gz file are created after the test suite has finished, as this is probably all that normal partners/users will need.</p> <p>Two env vars allow to control the web artifacts and the the new tar.gz file generation:</p> <ul> <li>TNF_OMIT_ARTIFACTS_ZIP_FILE=true/false : Defaulted to false in the launch scripts. If set to true, the tar.gz generation will be skipped.</li> <li>TNF_INCLUDE_WEB_FILES_IN_OUTPUT_FOLDER=true/false : Defaulted to false in the launch scripts. If set to true, the web viewer/parser files will also be copied to the output (claim) folder.</li> </ul>"},{"location":"test-standalone/#build-test-a-workload","title":"Build + Test a workload","text":"<p>Refer Developers\u2019 Guide</p>"},{"location":"workload-developers/","title":"Workload Developers","text":""},{"location":"workload-developers/#workload-guidelines-for-developers","title":"Workload Guidelines for developers","text":"<p>Developers of Kubernetes workloads, particularly those targeting certification with Red Hat on OpenShift, can use this suite to test the interaction of their workload with OpenShift.  If interested in certification please contact Red Hat.</p> <p>Requirements</p> <ul> <li>OpenShift 4.10 installation to run the workload</li> <li>At least one extra machine to host the test suite</li> </ul>"},{"location":"workload-developers/#to-add-private-test-cases","title":"To add private test cases","text":"<p>Refer this documentation https://github.com/test-network-function/cnfextensions</p> <p>Reference</p> <p>cnf-certification-test-partner repository provides sample example to model the test setup.</p>"}]}