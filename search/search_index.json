{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>This repository provides a set of Cloud-Native Network Functions (CNF) test cases and the framework to add more test cases.</p> <p>CNF</p> <p>The app (containers/pods/operators) we want to certify according Telco partner/Red Hat\u2019s best practices.</p> <p>TNF/Certification Test Suite</p> <p>The tool we use to certify a CNF.</p> <p>The purpose of the tests and the framework is to test the interaction of CNF with OpenShift Container Platform (OCP).  </p> <p>Info</p> <p>This test suite is provided for the CNF Developers to test their CNFs readiness for certification. Please see \u201cCNF Developers\u201d for more information.</p> <p>Features</p> <ul> <li> <p>The test suite generates a report (<code>claim.json</code>) and saves the test execution log (<code>tnf-execution.log</code>) in a configurable output directory.</p> </li> <li> <p>The catalog of the existing test cases and test building blocks are available in CATALOG.md</p> </li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>There are 3 building blocks in the above framework.</p> <ul> <li> <p>the <code>CNF</code> represents the CNF to be certified. The certification suite identifies the resources (containers/pods/operators etc) belonging to the CNF via labels or static data entries in the config file</p> </li> <li> <p>the <code>Certification container/exec</code> is the certification test suite running on the platform or in a container. The executable verifies the CNF under test configuration and its interactions with openshift</p> </li> <li> <p>the <code>Debug</code> pods are part of a Kubernetes daemonset responsible to run various privileged commands on kubernetes nodes. Debug pods are useful to run platform tests and test commands (e.g. ping) in container namespaces without changing the container image content. The debug daemonset is instantiated via the privileged-daemonset repository.</p> </li> </ul>"},{"location":"cnf-developers/","title":"CNF Developers","text":""},{"location":"cnf-developers/#cnf-developers-guidelines","title":"CNF Developers Guidelines","text":"<p>Developers of CNFs, particularly those targeting CNF Certification with Red Hat on OpenShift, can use this suite to test the interaction of their CNF with OpenShift.  If interested in CNF Certification please contact Red Hat.</p> <p>Requirements</p> <ul> <li>OpenShift 4.10 installation to run the CNFs</li> <li>At least one extra machine to host the test suite</li> </ul>"},{"location":"cnf-developers/#to-add-private-test-cases","title":"To add private test cases","text":"<p>Refer this documentation https://github.com/test-network-function/cnfextensions</p> <p>Reference</p> <p>cnf-certification-test-partner repository provides sample example to model the test setup.</p>"},{"location":"configuration/","title":"Test Configuration","text":""},{"location":"configuration/#test-configuration","title":"Test configuration","text":"<p>The certification test suite supports autodiscovery using labels and annotations.</p> <p>These can be configured through the following config file.</p> <ul> <li><code>tnf_config.yml</code></li> </ul> <p>Sample</p> <p>As per the requirement the following fields can be changed.</p>"},{"location":"configuration/#targetnamespaces","title":"targetNameSpaces","text":"<p>Multiple namespaces can be specified to deploy partner pods for testing through <code>targetNameSpaces</code> in the config file.</p> <pre><code>targetNameSpaces:\n- name: firstnamespace\n- name: secondnamespace\n</code></pre>"},{"location":"configuration/#targetpodlabels","title":"targetPodLabels","text":"<p>The goal of this section is to specify the labels to be used to identify the CNF resources under test.</p> <p>Highly recommended</p> <p>The labels should be defined in pod definition rather than added after pod is created, as labels added later on will be lost in case the pod gets rescheduled. In case of pods defined as part of a deployment, it\u2019s best to use the same label as the one defined in the <code>spec.selector.matchLabels</code> section of the deployment yaml. The prefix field can be used to avoid naming collision with other labels.</p> <pre><code>targetPodLabels:\n- prefix: test-network-function.com\nname: generic\nvalue: target\n</code></pre> <p>The corresponding pod label used to match pods is:</p> <pre><code>test-network-function.com/generic: target\n</code></pre> <p>Once the pods are found, all of their containers are also added to the target container list. A target deployment list will also be created with all the deployments which the test pods belong to.</p>"},{"location":"configuration/#targetcrds","title":"targetCrds","text":"<p>In order to autodiscover the CRDs to be tested, an array of search filters can be set under the \u201ctargetCrdFilters\u201d label. The autodiscovery mechanism will iterate through all the filters to look for all the CRDs that match it. Currently, filters only work by name suffix.</p> <pre><code>targetCrdFilters:\n- nameSuffix: \"group1.tnf.com\"\n- nameSuffix: \"anydomain.com\"\n</code></pre> <p>The autodiscovery mechanism will create a list of all CRD names in the cluster whose names have the suffix <code>group1.tnf.com</code> or <code>anydomain.com</code>, e.g. <code>crd1.group1.tnf.com</code> or <code>mycrd.mygroup.anydomain.com</code>.</p>"},{"location":"configuration/#testtarget","title":"testTarget","text":""},{"location":"configuration/#podsundertest-containersundertest","title":"podsUnderTest / containersUnderTest","text":"<p>The autodiscovery mechanism will attempt to identify the default network device and all the IP addresses of the pods it needs for network connectivity tests, though that information can be explicitly set using annotations if needed.</p>"},{"location":"configuration/#pod-ips","title":"Pod IPs","text":"<ul> <li>The <code>k8s.v1.cni.cncf.io/networks-status</code> annotation is checked and all IPs from it are used. This annotation is automatically managed in OpenShift but may not be present in K8s.</li> <li>If it is not present, then only known IPs associated with the pod are used (the pod <code>.status.ips</code> field).</li> </ul>"},{"location":"configuration/#network-interfaces","title":"Network Interfaces","text":"<ul> <li>The <code>k8s.v1.cni.cncf.io/networks-status</code> annotation is checked and the <code>interface</code> from the first entry found with <code>\u201cdefault\u201d=true</code> is used. This annotation is automatically managed in OpenShift but may not be present in K8s.</li> </ul> <p>The label <code>test-network-function.com/skip_connectivity_tests</code> excludes pods from all connectivity tests. The label value is trivial, only its presence. The label <code>test-network-function.com/skip_multus_connectivity_tests</code> excludes pods from Multus connectivity tests. Tests on default interface are still done. The label value is trivial, but its presence.</p>"},{"location":"configuration/#affinityrequired","title":"AffinityRequired","text":"<p>For CNF workloads that require pods to use Pod or Node Affinity rules, the label <code>AffinityRequired: true</code> must be included on the Pod YAML. This will prevent any tests for anti-affinity to fail as well as test your workloads for affinity rules that support your CNF\u2019s use-case.</p>"},{"location":"configuration/#certifiedcontainerinfo","title":"certifiedcontainerinfo","text":"<p>The <code>certifiedcontainerinfo</code> section contains information about CNFs containers that are to be checked for certification status on Red Hat catalogs.</p>"},{"location":"configuration/#operators","title":"Operators","text":"<p>The CSV of the installed Operators can be tested by the <code>operator</code> and <code>affiliated-certification</code> specs are identified with the <code>test-network-function.com/operator=target</code> label. Any value is permitted here but <code>target</code> is used here for consistency with the other specs.</p>"},{"location":"configuration/#allowedprotocolnames","title":"AllowedProtocolNames","text":"<p>This name of protocols that allowed. If we want to add another name, we just need to write the name in the yaml file.</p> <p>for example: if we want to add new protocol - \u201chttp4\u201d, we add in \u201ctnf_config.yml\u201d  below \u201cvalidProtocolNames\u201d and then this protocol (\u201chttp4\u201d) add to map allowedProtocolNames and finally \u201chttp4\u201d  will be allow protocol.</p>"},{"location":"configuration/#servicesignorelist","title":"ServicesIgnoreList","text":"<p>This is a list of service names present in the namespace under test and that should not be tested.</p>"},{"location":"configuration/#skipscalingtestdeployments-and-skipscalingteststatefulsetnames","title":"skipScalingTestDeployments and skipScalingTestStatefulSetNames","text":"<p>This section of the TNF config allows the user to skip the scaling tests that potentially cause known problems with workloads that do not like being scaled up and scaled down.</p> <p>Example:</p> <pre><code>skipScalingTestDeployments:\n- name: \"deployment1\"\nnamespace: \"tnf\"\nskipScalingTestStatefulSetNames:\n- name: \"statefulset1\"\nnamespace: \"tnf\"\n</code></pre>"},{"location":"configuration/#debugdaemonsetnamespace","title":"debugDaemonSetNamespace","text":"<p>This is an optional field with the name of the namespace where a privileged DaemonSet will be deployed. The namespace will be created in case it does not exist. In case this field is not set, the default namespace for this DaemonSet is \u201ccnf-suite\u201d.</p> <pre><code>debugDaemonSetNamespace: cnf-cert\n</code></pre> <p>This DaemonSet, called \u201ctnf-debug\u201d is deployed and used internally by the CNF Certification tool to issue some shell commands that are needed in certain test cases. Some of these test cases might fail or be skipped in case it wasn\u2019t deployed correctly.</p>"},{"location":"developers/","title":"Developers","text":""},{"location":"developers/#steps","title":"Steps","text":"<p>To test the newly added test / existing tests locally, follow the steps</p> <ul> <li>Clone the repo</li> <li> <p>Set runtime environment variables, as per the requirement.</p> <p>For example, to deploy partner deployments in a custom namespace in the test config.</p> <pre><code>targetNameSpaces:\n- name: mynamespace\n</code></pre> </li> <li> <p>Also, skip intrusive tests</p> </li> </ul> <pre><code>export TNF_NON_INTRUSIVE_ONLY=true\n</code></pre> <ul> <li> <p>Set K8s config of the cluster where test pods are running</p> <pre><code>export KUBECONFIG=&lt;&lt;mypath/.kube/config&gt;&gt;\n</code></pre> </li> <li> <p>Execute test suite, which would build and run the suite</p> <p>For example, to run <code>networking</code> tests</p> <pre><code>./script/development.sh networking\n</code></pre> </li> </ul>"},{"location":"exception/","title":"Exception Process","text":""},{"location":"exception/#exception-process","title":"Exception Process","text":"<p>There may exist some test cases which needs to fail always. The exception raised by the failed tests is published to Red Hat website for that partner.</p> <p>CATALOG provides the details of such exception.</p>"},{"location":"reference/","title":"Helpful Links","text":"<ul> <li>Contribution Guidelines</li> <li>CATALOG</li> <li>Best Practices Document v1.3</li> </ul>"},{"location":"runtime-env/","title":"Runtime environment variables","text":""},{"location":"runtime-env/#runtime-environment-variables","title":"Runtime environment variables","text":"<p>To run the test suite, some runtime environment variables are to be set.</p>"},{"location":"runtime-env/#ocp-412-labels","title":"OCP &gt;=4.12 Labels","text":"<p>The following labels need to be added to your default namespace in your cluster if you are running OCP &gt;=4.12:</p> <pre><code>pod-security.kubernetes.io/enforce: privileged\npod-security.kubernetes.io/enforce-version: latest\n</code></pre> <p>You can manually label the namespace with:</p> <pre><code>oc label namespace/default pod-security.kubernetes.io/enforce=privileged\noc label namespace/default pod-security.kubernetes.io/enforce-version=latest\n</code></pre>"},{"location":"runtime-env/#disable-intrusive-tests","title":"Disable intrusive tests","text":"<p>To skip intrusive tests which may disrupt cluster operations, issue the following:</p> <pre><code>export TNF_NON_INTRUSIVE_ONLY=true\n</code></pre> <p>Likewise, to enable intrusive tests, set the following:</p> <pre><code>export TNF_NON_INTRUSIVE_ONLY=false\n</code></pre> <p>Intrusive tests are enabled by default.</p>"},{"location":"runtime-env/#preflight-integration","title":"Preflight Integration","text":"<p>When running the <code>preflight</code> suite of tests, there are a few environment variables that will need to be set:</p> <p><code>PFLT_DOCKERCONFIG</code> is a required variable for running the preflight test suite. This provides credentials to the underlying preflight library for being able to pull/manipulate images and image bundles for testing.</p> <p>When running as a container, the docker config is mounted to the container via volume mount.</p> <p>When running as a standalone binary, the environment variables are consumed directly from your local machine.</p> <p>See more about this variable here.</p> <p><code>TNF_ALLOW_PREFLIGHT_INSECURE</code> (default: false) is required set to <code>true</code> if you are running against a private container registry that has self-signed certificates.</p>"},{"location":"runtime-env/#disconnected-environment","title":"Disconnected environment","text":"<p>In a disconnected environment, only specific versions of images are mirrored to the local repo. For those environments, the partner pod image <code>quay.io/testnetworkfunction/cnf-test-partner</code> and debug pod image <code>quay.io/testnetworkfunction/debug-partner</code> should be mirrored and <code>TNF_PARTNER_REPO</code> should be set to the local repo, e.g.:</p> <pre><code>export TNF_PARTNER_REPO=registry.dfwt5g.lab:5000/testnetworkfunction\n</code></pre> <p>Note that you can also specify the debug pod image to use with <code>SUPPORT_IMAGE</code> environment variable, default to <code>debug-partner:4.3.5</code>.</p>"},{"location":"test-container/","title":"Prebuilt container","text":""},{"location":"test-container/#test","title":"Test","text":"<p>The tests can be run within a prebuilt container in the OCP cluster.</p> <p>Prerequisites for the OCP cluster</p> <ul> <li>The cluster should have enough resources to drain nodes and reschedule pods. If that is not the case, then <code>lifecycle-pod-recreation</code> test should be skipped.</li> </ul>"},{"location":"test-container/#with-quay-test-container-image","title":"With quay test container image","text":""},{"location":"test-container/#pull-test-image","title":"Pull test image","text":"<p>The test image is available at this repository in quay.io and can be pulled using The image can be pulled using :</p> <pre><code>podman pull quay.io/testnetworkfunction/cnf-certification-test\n</code></pre>"},{"location":"test-container/#check-cluster-resources","title":"Check cluster resources","text":"<p>Some tests suites such as <code>platform-alteration</code> require node access to get node configuration like <code>hugepage</code>. In order to get the required information, the test suite does not <code>ssh</code> into nodes, but instead rely on oc debug tools. This tool makes it easier to fetch information from nodes and also to debug running pods.</p> <p><code>oc debug tool</code> will launch a new container ending with -debug suffix, and the container will be destroyed once the debug session is done. Ensure that the cluster should have enough resources to create debug pod, otherwise those tests would fail.</p> <p>Note</p> <p>It\u2019s recommended to clean up disk space and make sure there\u2019s enough resources to deploy another container image in every node before starting the tests.</p>"},{"location":"test-container/#run-the-tests","title":"Run the tests","text":"<pre><code>./run-tnf-container.sh\n</code></pre> <p>Required arguments</p> <ul> <li><code>-t</code> to provide the path of the local directory that contains tnf config files</li> <li><code>-o</code> to provide the path of the local directory where test results (claim.json), the execution logs (tnf-execution.log), and the results artifacts file (results.tar.gz) will be available from after the container exits.</li> </ul> <p>Warning</p> <p>This directory must exist in order for the claim file to be written.</p> <p>Optional arguments</p> <ul> <li><code>-l</code> to list the labels to be run. See Ginkgo Spec Labels for more information on how to filter tests with labels.</li> </ul> <p>Note</p> <p>If <code>-l</code> is not specified, the tnf will run in \u2018diagnostic\u2019 mode. In this mode, no test case will run: it will only get information from the cluster (PUTs, CRDs, nodes info, etc\u2026) to save it in the claim file. This can be used to make sure the configuration was properly set and the autodiscovery found the right pods/crds\u2026</p> <ul> <li> <p><code>-i</code> to provide a name to a custom TNF container image. Supports local images, as well as images from external registries.</p> </li> <li> <p><code>-k</code> to set a path to one or more kubeconfig files to be used by the container to authenticate with the cluster. Paths must be separated by a colon.</p> </li> </ul> <p>Note</p> <p>If <code>-k</code> is not specified, autodiscovery is performed.</p> <p>The autodiscovery first looks for paths in the <code>$KUBECONFIG</code> environment variable on the host system, and if the variable is not set or is empty, the default configuration stored in <code>$HOME/.kube/config</code> is checked.</p> <ul> <li><code>-n</code> to give the network mode of the container. Defaults set to <code>host</code>, which requires selinux to be disabled. Alternatively, <code>bridge</code> mode can be used with selinux if TNF_CONTAINER_CLIENT is set to <code>docker</code> or running the test as root.</li> </ul> <p>Note</p> <p>See the docker run \u2013network parameter reference for more information on how to configure network settings.</p> <ul> <li><code>-b</code> to set an external offline DB that will be used to verify the certification status of containers, helm charts and operators. Defaults to the DB included in the TNF container image.</li> </ul> <p>Note</p> <p>See the OCT tool for more information on how to create this DB.</p> <p>Command to run</p> <pre><code>./run-tnf-container.sh -k ~/.kube/config -t ~/tnf/config\n-o ~/tnf/output -l \"networking,access-control\"\n</code></pre> <p>See General tests for a list of available keywords.</p>"},{"location":"test-container/#run-with-docker","title":"Run with <code>docker</code>","text":"<p>By default, <code>run-container.sh</code> utilizes <code>podman</code>. However, an alternate container virtualization client using <code>TNF_CONTAINER_CLIENT</code> can be configured. This is particularly useful for operating systems that do not readily support <code>podman</code>.</p> <p>In order to configure the test harness to use <code>docker</code>, issue the following prior to <code>run-tnf-container.sh</code>:</p> <pre><code>export TNF_CONTAINER_CLIENT=docker\n</code></pre>"},{"location":"test-container/#output-targz-file-with-results-and-web-viewer-files","title":"Output tar.gz file with results and web viewer files","text":"<p>After running all the test cases, a compressed file will be created with all the results files and web artifacts to review them.</p> <p>By default, only the <code>claim.js</code>, the <code>cnf-certification-tests_junit.xml</code> file and this new tar.gz file are created after the test suite has finished, as this is probably all that normal partners/users will need.</p> <p>Two env vars allow to control the web artifacts and the the new tar.gz file generation:</p> <ul> <li>TNF_OMIT_ARTIFACTS_ZIP_FILE=true/false : Defaulted to false in the launch scripts. If set to true, the tar.gz generation will be skipped.</li> <li>TNF_INCLUDE_WEB_FILES_IN_OUTPUT_FOLDER=true/false : Defaulted to false in the launch scripts. If set to true, the web viewer/parser files will also be copied to the output (claim) folder.</li> </ul>"},{"location":"test-container/#with-local-test-container-image","title":"With local test container image","text":""},{"location":"test-container/#build-locally","title":"Build locally","text":"<pre><code>podman build -t cnf-certification-test:v4.3.5 \\\n--build-arg TNF_VERSION=v4.3.5 \\\n</code></pre> <ul> <li><code>TNF_VERSION</code> value is set to a branch, a tag, or a hash of a commit that will be installed into the image</li> </ul>"},{"location":"test-container/#build-from-an-unofficial-source","title":"Build from an unofficial source","text":"<p>The unofficial source could be a fork of the TNF repository.</p> <p>Use the <code>TNF_SRC_URL</code> build argument to override the URL to a source repository.</p> <pre><code>podman build -t cnf-certification-test:v4.3.5 \\\n--build-arg TNF_VERSION=v4.3.5 \\\n--build-arg TNF_SRC_URL=https://github.com/test-network-function/cnf-certification-test .\n</code></pre>"},{"location":"test-container/#run-the-tests-2","title":"Run the tests 2","text":"<p>Specify the custom TNF image using the <code>-i</code> parameter.</p> <pre><code>./run-tnf-container.sh -i cnf-certification-test:v4.3.5\n-t ~/tnf/config -o ~/tnf/output -l \"networking,access-control\"\n</code></pre> <p>Note: see General tests for a list of available keywords.</p>"},{"location":"test-output/","title":"Test Output","text":""},{"location":"test-output/#test-output","title":"Test Output","text":""},{"location":"test-output/#claim-file","title":"Claim File","text":"<p>The test suite generates an output file, named claim file. This file is considered as the proof of CNFs test run, evaluated by Red Hat when certified status is considered.</p> <p>This file describes the following</p> <ul> <li>The system(s) under test</li> <li>The tests that are executed</li> <li>The outcome of the executed / skipped tests</li> </ul> <p>Files that need to be submitted for certification</p> <p>When submitting results back to Red Hat for certification, please include the above mentioned claim file, the JUnit file, and any available console logs.</p> <p>How to add a CNF platform test result to the existing claim file?</p> <pre><code>go run cmd/tools/cmd/main.go claim-add --claimfile=claim.json\n--reportdir=/home/$USER/reports\n</code></pre> <p>Args: <code>--claimfile is an existing claim.json file</code> <code>--repordir :path to test results that you want to include.</code></p> <p>The tests result files from the given report dir will be appended under the result section of the claim file using file name as the key/value pair.  The tool will ignore the test result, if the key name is already present under result section of the claim file.</p> <pre><code> \"results\": {\n\"cnf-certification-tests_junit\": {\n\"testsuite\": {\n\"-errors\": \"0\",\n\"-failures\": \"2\",\n\"-name\": \"CNF Certification Test Suite\",\n\"-tests\": \"14\",\n...\n</code></pre> <p>Reference</p> <p>For more details on the contents of the claim file</p> <ul> <li>schema.  </li> <li>Guide.</li> </ul>"},{"location":"test-output/#execution-logs","title":"Execution logs","text":"<p>The test suite also saves a copy of the execution logs at [test output directory]/tnf-execution.log</p>"},{"location":"test-output/#results-artifacts-zip-file","title":"Results artifacts zip file","text":"<p>After running all the test cases, a compressed file will be created with all the results files and web artifacts to review them. The file has a UTC date-time prefix and looks like this:</p> <p>20230620-110654-cnf-test-results.tar.gz</p> <p>The \u201c20230620-110654\u201d sample prefix means \u201cJune-20th 2023, 11:06:54\u201d</p> <p>This is the content of the tar.gz file:</p> <ul> <li>claim.json</li> <li>cnf-certification-tests_junit.xml</li> <li>claimjson.js</li> <li>classification.js</li> <li>results.html</li> </ul> <p>This file serves two different purposes:</p> <ol> <li>Make it easier to store and send the test results for review.</li> <li>View the results in the html web page. In addition, the web page (either results-embed.thml or results.html) has a selector for workload type and allows the parter to introduce feedback for each of the failing test cases for later review from Red Hat. It\u2019s important to note that this web page needs the <code>claimjson.js</code> and <code>classification.js</code> files to be in the same folder as the html files to work properly.</li> </ol>"},{"location":"test-output/#show-results-after-running-the-test-code","title":"Show Results after running the test code","text":"<p>A standalone HTML page is available to decode the results. For more details, see: https://github.com/test-network-function/parser</p>"},{"location":"test-spec/","title":"Available Test Specs","text":""},{"location":"test-spec/#test-specifications","title":"Test Specifications","text":""},{"location":"test-spec/#available-test-specs","title":"Available Test Specs","text":"<p>There are two categories for CNF tests.</p> <ul> <li>General</li> </ul> <p>These tests are designed to test any commodity CNF running on OpenShift, and include specifications such as <code>Default</code> network connectivity.</p> <ul> <li>CNF-Specific</li> </ul> <p>These tests are designed to test some unique aspects of the CNF under test are behaving correctly. This could include specifications such as issuing a <code>GET</code> request to a web server, or passing traffic through an IPSEC tunnel.</p>"},{"location":"test-spec/#general-tests","title":"General tests","text":"<p>These tests belong to multiple suites that can be run in any combination as is appropriate for the CNFs under test.</p> <p>Info</p> <p>Test suites group tests by the topic areas.</p> Suite Test Spec Description Minimum OpenShift Version <code>access-control</code> The access-control test suite is used to test  service account, namespace and cluster/pod role binding for the pods under test. It also tests the pods/containers configuration. 4.6.0 <code>affiliated-certification</code> The affiliated-certification test suite verifies that the containers and operators discovered or listed in the configuration file are certified by Redhat 4.6.0 <code>lifecycle</code> The lifecycle test suite verifies the pods deployment, creation, shutdown and  survivability. 4.6.0 <code>networking</code> The networking test suite contains tests that check connectivity and networking config related best practices. 4.6.0 <code>operator</code> The operator test suite is designed to test basic Kubernetes Operator functionality. 4.6.0 <code>platform-alteration</code> verifies that key platform configuration is not modified by the CNF under test 4.6.0 <code>observability</code> the observability test suite contains tests that check CNF logging is following best practices and that CRDs have status fields 4.6.0 <p>Info</p> <p>Please refer CATALOG.md for more details.</p>"},{"location":"test-spec/#cnf-specific-tests","title":"CNF-specific tests","text":"<p>TODO</p>"},{"location":"test-standalone/","title":"Standalone test executable","text":""},{"location":"test-standalone/#standalone-test-executable","title":"Standalone test executable","text":"<p>Prerequisites</p> <p>The repo is cloned and all the commands should be run from the cloned repo.</p> <pre><code>mkdir ~/workspace\ncd ~/workspace\ngit clone git@github.com:test-network-function/cnf-certification-test.git\ncd cnf-certification-test\n</code></pre> <p>Note</p> <p>By default, <code>cnf-certification-test</code> emits results to <code>cnf-certification-test/cnf-certification-tests_junit.xml</code>.</p>"},{"location":"test-standalone/#1-install-dependencies","title":"1. Install dependencies","text":"<p>Depending on how you want to run the test suite there are different dependencies that will be needed.</p> <p>If you are planning on running the test suite as a container, the only pre-requisite is Docker or Podman.</p> <p>If you are planning on running the test suite as a standalone binary, there are pre-requisites that will need to be installed in your environment prior to runtime.</p> <p>Run the following command to install the following dependencies.</p> <pre><code>make install-tools\n</code></pre> Dependency Minimum Version GoLang 1.21 golangci-lint 1.54.2 jq 1.6 OpenShift Client 4.12 <p>Other binary dependencies required to run tests can be installed using the following command:</p> <p>Note</p> <ul> <li>You must also make sure that <code>$GOBIN</code> (default <code>$GOPATH/bin</code>) is on your <code>$PATH</code>.</li> <li>Efforts to containerise this offering are considered a work in progress.</li> </ul>"},{"location":"test-standalone/#2-build-the-test-suite","title":"2. Build the Test Suite","text":"<p>In order to build the test executable, first make sure you have satisfied the dependencies.</p> <pre><code>make build-cnf-tests\n</code></pre> <p>Gotcha: The <code>make build*</code> commands run unit tests where appropriate. They do NOT test the CNF.</p>"},{"location":"test-standalone/#3-test-a-cnf","title":"3. Test a CNF","text":"<p>A CNF is tested by specifying which suites to run using the <code>run-cnf-suites.sh</code> helper script.</p> <p>Run any combination of the suites keywords listed at in the General tests section, e.g.</p> <pre><code>./run-cnf-suites.sh -l \"lifecycle\"\n./run-cnf-suites.sh -l \"networking,lifecycle\"\n./run-cnf-suites.sh -l \"operator,networking\"\n./run-cnf-suites.sh -l \"networking,platform-alteration\"\n./run-cnf-suites.sh -l \"networking,lifecycle,affiliated-certification,operator\"\n</code></pre> <p>Note</p> <p>As with \u201crun-tnf-container.sh\u201d, if <code>-l</code> is not specified here, the tnf will run in \u2018diagnostic\u2019 mode.</p> <p>By default the claim file will be output into the same location as the test executable. The <code>-o</code> argument for     <code>run-cnf-suites.sh</code> can be used to provide a new location that the output files will be saved to. For more detailed     control over the outputs, see the output of <code>cnf-certification-test.test --help</code>.</p> <pre><code>    cd cnf-certification-test &amp;&amp; ./cnf-certification-test.test --help\n</code></pre>"},{"location":"test-standalone/#run-a-single-test","title":"Run a single test","text":"<p>All tests have unique labels, which can be used to filter which tests are to be run. This is useful when debugging a single test.</p> <p>To select the test to be executed when running <code>run-cnf-suites.sh</code> with the following command-line:</p> <pre><code>./run-cnf-suites.sh -l operator-install-source\n</code></pre> <p>Note</p> <p>The test labels work the same as the suite labels, so you can select more than one test with the filtering mechanism shown before.</p>"},{"location":"test-standalone/#run-all-of-the-tests","title":"Run all of the tests","text":"<p>You can run all of the tests (including the intrusive tests and the extended suite) with the following commands:</p> <pre><code>./run-cnf-suites.sh -l all\n</code></pre>"},{"location":"test-standalone/#run-a-subset","title":"Run a subset","text":"<p>You can find all the labels attached to the tests by running the following command:</p> <pre><code>./run-cnf-suites.sh --list\n</code></pre> <p>You can also check the CATALOG.md to find all test labels.</p>"},{"location":"test-standalone/#labels-for-offline-environments","title":"Labels for offline environments","text":"<p>Some tests do require connectivity to Red Hat servers to validate certification status. To run the tests in an offline environment, skip the tests using the <code>l</code> option.</p> <pre><code>./run-cnf-suites.sh -l '!online'\n</code></pre> <p>Alternatively, if an offline DB for containers, helm charts and operators is available, there is no need to skip those tests if the environment variable <code>TNF_OFFLINE_DB</code> is set to the DB location. This DB can be generated using the OCT tool.</p> <p>Note: Only partner certified images are stored in the offline database. If Redhat images are checked against the offline database, they will show up as not certified. The online database includes both Partner and Redhat images.</p>"},{"location":"test-standalone/#output-targz-file-with-results-and-web-viewer-files","title":"Output tar.gz file with results and web viewer files","text":"<p>After running all the test cases, a compressed file will be created with all the results files and web artifacts to review them.</p> <p>By default, only the <code>claim.js</code>, the <code>cnf-certification-tests_junit.xml</code> file and this new tar.gz file are created after the test suite has finished, as this is probably all that normal partners/users will need.</p> <p>Two env vars allow to control the web artifacts and the the new tar.gz file generation:</p> <ul> <li>TNF_OMIT_ARTIFACTS_ZIP_FILE=true/false : Defaulted to false in the launch scripts. If set to true, the tar.gz generation will be skipped.</li> <li>TNF_INCLUDE_WEB_FILES_IN_OUTPUT_FOLDER=true/false : Defaulted to false in the launch scripts. If set to true, the web viewer/parser files will also be copied to the output (claim) folder.</li> </ul>"},{"location":"test-standalone/#build-test-a-cnf","title":"Build + Test a CNF","text":"<p>Refer Developers\u2019 Guide</p>"}]}