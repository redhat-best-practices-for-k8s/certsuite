{
  "data": [
    {
      "_id": "6109561851a316e788826e73",
      "alm_examples": [
        {
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "externalArangoDBName": "arangodb",
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.2.5"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:d1996f41abc640884dd988174df35b39e4d487bdfe4a1e2c39b1c365d20612e3",
      "bundle_path_digest": "sha256:d1996f41abc640884dd988174df35b39e4d487bdfe4a1e2c39b1c365d20612e3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T14:43:36.195000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v8.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:16:06.450000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "8.2.0",
      "version_original": "8.2.0"
    },
    {
      "_id": "610956b74c1205f78b59c98b",
      "alm_examples": [
        {
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "externalArangoDBName": "arangodb",
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.2.5"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:d1996f41abc640884dd988174df35b39e4d487bdfe4a1e2c39b1c365d20612e3",
      "bundle_path_digest": "sha256:d1996f41abc640884dd988174df35b39e4d487bdfe4a1e2c39b1c365d20612e3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T14:46:15.218000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v8.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:22:39.143000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "8.2.0",
      "version_original": "8.2.0"
    },
    {
      "_id": "61095792b7b63d77728dc5f6",
      "alm_examples": [
        {
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "externalArangoDBName": "arangodb",
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.2.5"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:d1996f41abc640884dd988174df35b39e4d487bdfe4a1e2c39b1c365d20612e3",
      "bundle_path_digest": "sha256:d1996f41abc640884dd988174df35b39e4d487bdfe4a1e2c39b1c365d20612e3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T14:49:54.016000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v8.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T15:01:13.270000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "8.2.0",
      "version_original": "8.2.0"
    },
    {
      "_id": "61095de551a316e788826e7f",
      "alm_examples": [
        {
          "kind": "EntandoDeBundle",
          "metadata": {
            "name": "my-bundle",
            "namespace": "my-namespace"
          },
          "spec": {
            "details": {
              "name": "my-bundle"
            }
          }
        },
        {
          "kind": "EntandoDatabaseService",
          "metadata": {
            "name": "my-entando-database-service",
            "namespace": "my-namespace"
          },
          "spec": {
            "createDeployment": true,
            "databaseName": "my_database",
            "dbms": "postgresql"
          }
        },
        {
          "kind": "EntandoKeycloakServer",
          "metadata": {
            "name": "my-keycloak",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "replicas": 1,
            "standardImage": "redhat-sso"
          }
        },
        {
          "kind": "EntandoClusterInfrastructure",
          "metadata": {
            "name": "my-entando-cluster-infrastructure",
            "namespace": "my-namespace"
          },
          "spec": {
            "environmentVariables": [],
            "replicas": 1
          }
        },
        {
          "kind": "EntandoPlugin",
          "metadata": {
            "name": "my-entando-plugin",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "healthCheckPath": "/management/health",
            "image": "entando/entando-avatar-plugin:6.0.5",
            "ingressHostName": "my-plugin.apps-crc.testing",
            "ingressPath": "/avatarPlugin",
            "replicas": 1,
            "roles": [
              {
                "code": "admin",
                "name": "user"
              },
              {
                "code": "user",
                "name": "user"
              }
            ],
            "securityLevel": "strict"
          }
        },
        {
          "kind": "EntandoApp",
          "metadata": {
            "name": "my-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "ingressHostName": "my-app.apps-crc.testing",
            "replicas": 1,
            "standardServerImage": "eap"
          }
        },
        {
          "kind": "EntandoAppPluginLink",
          "metadata": {
            "name": "my-link",
            "namespace": "my-namespace"
          },
          "spec": {
            "entandoAppName": "my-app",
            "entandoPluginName": "my-plugin"
          }
        },
        {
          "kind": "EntandoCompositeApp",
          "metadata": {
            "name": "my-entando-composite-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "components": [
              {
                "kind": "EntandoDatabaseService",
                "metadata": {
                  "name": "inline-entando-database-service"
                },
                "spec": {
                  "createDeployment": true
                }
              },
              {
                "kind": "EntandoKeycloakServer",
                "metadata": {
                  "name": "inline-keycloak"
                },
                "spec": {
                  "standardImage": "redhat-sso"
                }
              },
              {
                "kind": "EntandoClusterInfrastructure",
                "metadata": {
                  "name": "inline-entando-cluster-infrastructure"
                },
                "spec": {}
              },
              {
                "kind": "EntandoApp",
                "metadata": {
                  "name": "inline-app"
                },
                "spec": {
                  "standardServerImage": "eap"
                }
              },
              {
                "kind": "EntandoPlugin",
                "metadata": {
                  "name": "inline-plugin"
                },
                "spec": {
                  "healthCheckPath": "/management/health",
                  "image": "entando/entando-avatar-plugin:6.0.5",
                  "ingressPath": "/avatarPlugin",
                  "roles": [
                    {
                      "code": "admin",
                      "name": "admin"
                    },
                    {
                      "code": "user",
                      "name": "user"
                    }
                  ]
                }
              },
              {
                "kind": "EntandoAppPluginLink",
                "metadata": {
                  "name": "inline-link"
                },
                "spec": {
                  "entandoAppName": "inline-app",
                  "entandoPluginName": "inline-plugin"
                }
              }
            ],
            "dbmsOverride": "postgresql",
            "ingressHostNameOverride": "entando.apps-crc.testing"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/entando/entando-operator@sha256:639e2d35738483c6c092ebe8636bbab50c0f2101bc06f2fe9a1c7f68b3ffc4a6",
      "bundle_path_digest": "sha256:639e2d35738483c6c092ebe8636bbab50c0f2101bc06f2fe9a1c7f68b3ffc4a6",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T15:16:53.387000+00:00",
      "csv_description": "## Entando\n\nThe Entando platform accelerates the development and lifecycle  management of fully modularized applications on Kubernetes. It  provides tools to help developers create and manage applications  using modular frontend and backend components.\nThe Entando Operator automates the installation, provisioning, and  configuration management of the components that make up an Entando  application. Specifically, the operator manages the following custom  resources:\n**EntandoKeycloakServers** for centralized authentication of frontend  and backend components. The operator can deploy Keycloak, or in certified  environments, Red Hat SSO servers that can then be used by subsequent  deployments as an OpenID Connect provider.\n**EntandoApps** for hosting an Entando application. EntandoApps are hosted on Wildfly or JBoss EAP containers, and can also be used to deploy custom  EntandoApp containers.\n**EntandoPlugins** for deploying microservices to customize or enhance your EntandoApp. Entando microservice plugins are deployed to your cluster, and  then linked to one or more EntandoApps.\n## Using the Operator\nThe Entando Operator can be deployed using the default settings without any  configuration. Once deployed, the operator can be customized by editing  the *configmap* and secrets.\n### ConfigMap: entando-operator-config\nThe 'entando-operator-config' ConfigMap can be added after deployment and any changes to it will be picked up by the operator on subsequent event processing. It supports the following keys:\n\n    entando.k8s.operator.gc.controller.pods: set this to \"false\" to keep controller pods after completion.\n    entando.k8s.operator.compliance.mode: set this to \"community\" if there is no requirement for Red Hat compliance.\n    entando.k8s.operator.image.pull.secrets: a comma separated list containing the names of pull secrets that will be linked to all service accounts.\n    entando.k8s.operator.disable.pvc.garbage.collection: set this to \"false\" if you want Persistent Volume Claims to be deleted with the custom resources they are associated with.\n    entando.k8s.operator.impose.default.limits: set this to \"false\" if there is no need to limit the resource consumption of pods on your cluster.\n    entando.k8s.operator.request.to.limit.ratio: a decimal number that default limits will be multiplied by to calculate default requests for resources.\n    entando.k8s.operator.force.db.password.reset: set this to \"true\" if you plan to delete Secrets from your namespace but you want to retain the Database they point to.\n    entando.k8s.operator.pull.policy.override: specify your preferred pullPolicy for images. The default is Always.\n    entando.tls.secret.name: The name of a standard TLS secret to use for HTTPS Ingresses. See the section entando-tls-secret.\n    entando.ca.secret.name: The name of a secret containing CA certificates. See the section entando-ca-cert-secret.\n    entando.assume.external.https.provider: Set this to \"true\" if your cloud provider handles HTTPS for you.\n    entando.use.auto.cert.generation: Set this to \"true\" to have Openshift use its internal CA to generate certificates for your Routes.\n    entando.default.routing.suffix: The domain name that can be suffixed to deployment names when the ingressHostName is omitted. Needs to be preconfigured on your DNS provider.\n    entando.pod.completion.timeout.seconds: The time it will take before Entando fails a run-to-completion Pod.\n    entando.pod.readiness.timeout.seconds: The time it will take before Entando fails a Service Pod.\n    entando.pod.shutdown.timeout.seconds: The time Entando will give a Pod to shutdown gracefully.\n\n\n### entando-pull-secret\nThe secret to be used to pull Entando images from the Red Hat container registry. The name of this secret is hard coded as it is required for the ClusterServiceVersion of this Operator\n### entando-tls-secret\nA standard Kubernetes TLS secret that will be used on all deployments where no custom TLS secret name is specified.\n### entando-ca-cert-secret\nThis is an opaque secret in the Entando Operator's namespace that contains the certificates of all trusted certificate authorities in your environment. This is generally used mainly for self signed certificates. As is generally the case for opaque secrets, there are no constraints on the keys in this secret. However, limit the files inside the secret to X509 certificates or certificate chains. The Entando Operator will load all of these files into a Java keystore that it then configures as the trust store for each container that uses Java.\n",
      "csv_display_name": "Entando Operator",
      "csv_metadata_description": "Processes EntandoKeycloakServer, EntandoApp and EntandoPlugin custom resources and deploys the relevant containers in the Kubernetes cluster.",
      "csv_name": "entando-k8s-operator.v6.3.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:18:23.211000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "entando-k8s-operator",
      "provided_apis": [
        {
          "group": "entando.org",
          "kind": "EntandoDatabaseService",
          "plural": "entandodatabaseservices",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDeBundle",
          "plural": "entandodebundles",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoKeycloakServer",
          "plural": "entandokeycloakservers",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoPlugin",
          "plural": "entandoplugins",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoAppPluginLink",
          "plural": "entandoapppluginlinks",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoApp",
          "plural": "entandoapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoClusterInfrastructure",
          "plural": "entandoclusterinfrastructures",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoCompositeApp",
          "plural": "entandocompositeapps",
          "version": "v1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "6.3.2",
      "version_original": "6.3.2"
    },
    {
      "_id": "61096494453d6347735bd271",
      "alm_examples": [
        {
          "kind": "EntandoDeBundle",
          "metadata": {
            "name": "my-bundle",
            "namespace": "my-namespace"
          },
          "spec": {
            "details": {
              "name": "my-bundle"
            }
          }
        },
        {
          "kind": "EntandoDatabaseService",
          "metadata": {
            "name": "my-entando-database-service",
            "namespace": "my-namespace"
          },
          "spec": {
            "createDeployment": true,
            "databaseName": "my_database",
            "dbms": "postgresql"
          }
        },
        {
          "kind": "EntandoKeycloakServer",
          "metadata": {
            "name": "my-keycloak",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "replicas": 1,
            "standardImage": "redhat-sso"
          }
        },
        {
          "kind": "EntandoClusterInfrastructure",
          "metadata": {
            "name": "my-entando-cluster-infrastructure",
            "namespace": "my-namespace"
          },
          "spec": {
            "environmentVariables": [],
            "replicas": 1
          }
        },
        {
          "kind": "EntandoPlugin",
          "metadata": {
            "name": "my-entando-plugin",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "healthCheckPath": "/management/health",
            "image": "entando/entando-avatar-plugin:6.0.5",
            "ingressHostName": "my-plugin.apps-crc.testing",
            "ingressPath": "/avatarPlugin",
            "replicas": 1,
            "roles": [
              {
                "code": "admin",
                "name": "user"
              },
              {
                "code": "user",
                "name": "user"
              }
            ],
            "securityLevel": "strict"
          }
        },
        {
          "kind": "EntandoApp",
          "metadata": {
            "name": "my-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "ingressHostName": "my-app.apps-crc.testing",
            "replicas": 1,
            "standardServerImage": "eap"
          }
        },
        {
          "kind": "EntandoAppPluginLink",
          "metadata": {
            "name": "my-link",
            "namespace": "my-namespace"
          },
          "spec": {
            "entandoAppName": "my-app",
            "entandoPluginName": "my-plugin"
          }
        },
        {
          "kind": "EntandoCompositeApp",
          "metadata": {
            "name": "my-entando-composite-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "components": [
              {
                "kind": "EntandoDatabaseService",
                "metadata": {
                  "name": "inline-entando-database-service"
                },
                "spec": {
                  "createDeployment": true
                }
              },
              {
                "kind": "EntandoKeycloakServer",
                "metadata": {
                  "name": "inline-keycloak"
                },
                "spec": {
                  "standardImage": "redhat-sso"
                }
              },
              {
                "kind": "EntandoClusterInfrastructure",
                "metadata": {
                  "name": "inline-entando-cluster-infrastructure"
                },
                "spec": {}
              },
              {
                "kind": "EntandoApp",
                "metadata": {
                  "name": "inline-app"
                },
                "spec": {
                  "standardServerImage": "eap"
                }
              },
              {
                "kind": "EntandoPlugin",
                "metadata": {
                  "name": "inline-plugin"
                },
                "spec": {
                  "healthCheckPath": "/management/health",
                  "image": "entando/entando-avatar-plugin:6.0.5",
                  "ingressPath": "/avatarPlugin",
                  "roles": [
                    {
                      "code": "admin",
                      "name": "admin"
                    },
                    {
                      "code": "user",
                      "name": "user"
                    }
                  ]
                }
              },
              {
                "kind": "EntandoAppPluginLink",
                "metadata": {
                  "name": "inline-link"
                },
                "spec": {
                  "entandoAppName": "inline-app",
                  "entandoPluginName": "inline-plugin"
                }
              }
            ],
            "dbmsOverride": "postgresql",
            "ingressHostNameOverride": "entando.apps-crc.testing"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/entando/entando-operator@sha256:639e2d35738483c6c092ebe8636bbab50c0f2101bc06f2fe9a1c7f68b3ffc4a6",
      "bundle_path_digest": "sha256:639e2d35738483c6c092ebe8636bbab50c0f2101bc06f2fe9a1c7f68b3ffc4a6",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T15:45:24.593000+00:00",
      "csv_description": "## Entando\n\nThe Entando platform accelerates the development and lifecycle  management of fully modularized applications on Kubernetes. It  provides tools to help developers create and manage applications  using modular frontend and backend components.\nThe Entando Operator automates the installation, provisioning, and  configuration management of the components that make up an Entando  application. Specifically, the operator manages the following custom  resources:\n**EntandoKeycloakServers** for centralized authentication of frontend  and backend components. The operator can deploy Keycloak, or in certified  environments, Red Hat SSO servers that can then be used by subsequent  deployments as an OpenID Connect provider.\n**EntandoApps** for hosting an Entando application. EntandoApps are hosted on Wildfly or JBoss EAP containers, and can also be used to deploy custom  EntandoApp containers.\n**EntandoPlugins** for deploying microservices to customize or enhance your EntandoApp. Entando microservice plugins are deployed to your cluster, and  then linked to one or more EntandoApps.\n## Using the Operator\nThe Entando Operator can be deployed using the default settings without any  configuration. Once deployed, the operator can be customized by editing  the *configmap* and secrets.\n### ConfigMap: entando-operator-config\nThe 'entando-operator-config' ConfigMap can be added after deployment and any changes to it will be picked up by the operator on subsequent event processing. It supports the following keys:\n\n    entando.k8s.operator.gc.controller.pods: set this to \"false\" to keep controller pods after completion.\n    entando.k8s.operator.compliance.mode: set this to \"community\" if there is no requirement for Red Hat compliance.\n    entando.k8s.operator.image.pull.secrets: a comma separated list containing the names of pull secrets that will be linked to all service accounts.\n    entando.k8s.operator.disable.pvc.garbage.collection: set this to \"false\" if you want Persistent Volume Claims to be deleted with the custom resources they are associated with.\n    entando.k8s.operator.impose.default.limits: set this to \"false\" if there is no need to limit the resource consumption of pods on your cluster.\n    entando.k8s.operator.request.to.limit.ratio: a decimal number that default limits will be multiplied by to calculate default requests for resources.\n    entando.k8s.operator.force.db.password.reset: set this to \"true\" if you plan to delete Secrets from your namespace but you want to retain the Database they point to.\n    entando.k8s.operator.pull.policy.override: specify your preferred pullPolicy for images. The default is Always.\n    entando.tls.secret.name: The name of a standard TLS secret to use for HTTPS Ingresses. See the section entando-tls-secret.\n    entando.ca.secret.name: The name of a secret containing CA certificates. See the section entando-ca-cert-secret.\n    entando.assume.external.https.provider: Set this to \"true\" if your cloud provider handles HTTPS for you.\n    entando.use.auto.cert.generation: Set this to \"true\" to have Openshift use its internal CA to generate certificates for your Routes.\n    entando.default.routing.suffix: The domain name that can be suffixed to deployment names when the ingressHostName is omitted. Needs to be preconfigured on your DNS provider.\n    entando.pod.completion.timeout.seconds: The time it will take before Entando fails a run-to-completion Pod.\n    entando.pod.readiness.timeout.seconds: The time it will take before Entando fails a Service Pod.\n    entando.pod.shutdown.timeout.seconds: The time Entando will give a Pod to shutdown gracefully.\n\n\n### entando-pull-secret\nThe secret to be used to pull Entando images from the Red Hat container registry. The name of this secret is hard coded as it is required for the ClusterServiceVersion of this Operator\n### entando-tls-secret\nA standard Kubernetes TLS secret that will be used on all deployments where no custom TLS secret name is specified.\n### entando-ca-cert-secret\nThis is an opaque secret in the Entando Operator's namespace that contains the certificates of all trusted certificate authorities in your environment. This is generally used mainly for self signed certificates. As is generally the case for opaque secrets, there are no constraints on the keys in this secret. However, limit the files inside the secret to X509 certificates or certificate chains. The Entando Operator will load all of these files into a Java keystore that it then configures as the trust store for each container that uses Java.\n",
      "csv_display_name": "Entando Operator",
      "csv_metadata_description": "Processes EntandoKeycloakServer, EntandoApp and EntandoPlugin custom resources and deploys the relevant containers in the Kubernetes cluster.",
      "csv_name": "entando-k8s-operator.v6.3.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:16:52.733000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "entando-k8s-operator",
      "provided_apis": [
        {
          "group": "entando.org",
          "kind": "EntandoAppPluginLink",
          "plural": "entandoapppluginlinks",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoApp",
          "plural": "entandoapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoClusterInfrastructure",
          "plural": "entandoclusterinfrastructures",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoCompositeApp",
          "plural": "entandocompositeapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDatabaseService",
          "plural": "entandodatabaseservices",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDeBundle",
          "plural": "entandodebundles",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoKeycloakServer",
          "plural": "entandokeycloakservers",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoPlugin",
          "plural": "entandoplugins",
          "version": "v1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "6.3.2",
      "version_original": "6.3.2"
    },
    {
      "_id": "61096a476bbb00c64eecd5e5",
      "alm_examples": [
        {
          "kind": "EntandoDeBundle",
          "metadata": {
            "name": "my-bundle",
            "namespace": "my-namespace"
          },
          "spec": {
            "details": {
              "name": "my-bundle"
            }
          }
        },
        {
          "kind": "EntandoDatabaseService",
          "metadata": {
            "name": "my-entando-database-service",
            "namespace": "my-namespace"
          },
          "spec": {
            "createDeployment": true,
            "databaseName": "my_database",
            "dbms": "postgresql"
          }
        },
        {
          "kind": "EntandoKeycloakServer",
          "metadata": {
            "name": "my-keycloak",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "replicas": 1,
            "standardImage": "redhat-sso"
          }
        },
        {
          "kind": "EntandoClusterInfrastructure",
          "metadata": {
            "name": "my-entando-cluster-infrastructure",
            "namespace": "my-namespace"
          },
          "spec": {
            "environmentVariables": [],
            "replicas": 1
          }
        },
        {
          "kind": "EntandoPlugin",
          "metadata": {
            "name": "my-entando-plugin",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "healthCheckPath": "/management/health",
            "image": "entando/entando-avatar-plugin:6.0.5",
            "ingressHostName": "my-plugin.apps-crc.testing",
            "ingressPath": "/avatarPlugin",
            "replicas": 1,
            "roles": [
              {
                "code": "admin",
                "name": "user"
              },
              {
                "code": "user",
                "name": "user"
              }
            ],
            "securityLevel": "strict"
          }
        },
        {
          "kind": "EntandoApp",
          "metadata": {
            "name": "my-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "ingressHostName": "my-app.apps-crc.testing",
            "replicas": 1,
            "standardServerImage": "eap"
          }
        },
        {
          "kind": "EntandoAppPluginLink",
          "metadata": {
            "name": "my-link",
            "namespace": "my-namespace"
          },
          "spec": {
            "entandoAppName": "my-app",
            "entandoPluginName": "my-plugin"
          }
        },
        {
          "kind": "EntandoCompositeApp",
          "metadata": {
            "name": "my-entando-composite-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "components": [
              {
                "kind": "EntandoDatabaseService",
                "metadata": {
                  "name": "inline-entando-database-service"
                },
                "spec": {
                  "createDeployment": true
                }
              },
              {
                "kind": "EntandoKeycloakServer",
                "metadata": {
                  "name": "inline-keycloak"
                },
                "spec": {
                  "standardImage": "redhat-sso"
                }
              },
              {
                "kind": "EntandoClusterInfrastructure",
                "metadata": {
                  "name": "inline-entando-cluster-infrastructure"
                },
                "spec": {}
              },
              {
                "kind": "EntandoApp",
                "metadata": {
                  "name": "inline-app"
                },
                "spec": {
                  "standardServerImage": "eap"
                }
              },
              {
                "kind": "EntandoPlugin",
                "metadata": {
                  "name": "inline-plugin"
                },
                "spec": {
                  "healthCheckPath": "/management/health",
                  "image": "entando/entando-avatar-plugin:6.0.5",
                  "ingressPath": "/avatarPlugin",
                  "roles": [
                    {
                      "code": "admin",
                      "name": "admin"
                    },
                    {
                      "code": "user",
                      "name": "user"
                    }
                  ]
                }
              },
              {
                "kind": "EntandoAppPluginLink",
                "metadata": {
                  "name": "inline-link"
                },
                "spec": {
                  "entandoAppName": "inline-app",
                  "entandoPluginName": "inline-plugin"
                }
              }
            ],
            "dbmsOverride": "postgresql",
            "ingressHostNameOverride": "entando.apps-crc.testing"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/entando/entando-operator@sha256:639e2d35738483c6c092ebe8636bbab50c0f2101bc06f2fe9a1c7f68b3ffc4a6",
      "bundle_path_digest": "sha256:639e2d35738483c6c092ebe8636bbab50c0f2101bc06f2fe9a1c7f68b3ffc4a6",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T16:09:43.943000+00:00",
      "csv_description": "## Entando\n\nThe Entando platform accelerates the development and lifecycle  management of fully modularized applications on Kubernetes. It  provides tools to help developers create and manage applications  using modular frontend and backend components.\nThe Entando Operator automates the installation, provisioning, and  configuration management of the components that make up an Entando  application. Specifically, the operator manages the following custom  resources:\n**EntandoKeycloakServers** for centralized authentication of frontend  and backend components. The operator can deploy Keycloak, or in certified  environments, Red Hat SSO servers that can then be used by subsequent  deployments as an OpenID Connect provider.\n**EntandoApps** for hosting an Entando application. EntandoApps are hosted on Wildfly or JBoss EAP containers, and can also be used to deploy custom  EntandoApp containers.\n**EntandoPlugins** for deploying microservices to customize or enhance your EntandoApp. Entando microservice plugins are deployed to your cluster, and  then linked to one or more EntandoApps.\n## Using the Operator\nThe Entando Operator can be deployed using the default settings without any  configuration. Once deployed, the operator can be customized by editing  the *configmap* and secrets.\n### ConfigMap: entando-operator-config\nThe 'entando-operator-config' ConfigMap can be added after deployment and any changes to it will be picked up by the operator on subsequent event processing. It supports the following keys:\n\n    entando.k8s.operator.gc.controller.pods: set this to \"false\" to keep controller pods after completion.\n    entando.k8s.operator.compliance.mode: set this to \"community\" if there is no requirement for Red Hat compliance.\n    entando.k8s.operator.image.pull.secrets: a comma separated list containing the names of pull secrets that will be linked to all service accounts.\n    entando.k8s.operator.disable.pvc.garbage.collection: set this to \"false\" if you want Persistent Volume Claims to be deleted with the custom resources they are associated with.\n    entando.k8s.operator.impose.default.limits: set this to \"false\" if there is no need to limit the resource consumption of pods on your cluster.\n    entando.k8s.operator.request.to.limit.ratio: a decimal number that default limits will be multiplied by to calculate default requests for resources.\n    entando.k8s.operator.force.db.password.reset: set this to \"true\" if you plan to delete Secrets from your namespace but you want to retain the Database they point to.\n    entando.k8s.operator.pull.policy.override: specify your preferred pullPolicy for images. The default is Always.\n    entando.tls.secret.name: The name of a standard TLS secret to use for HTTPS Ingresses. See the section entando-tls-secret.\n    entando.ca.secret.name: The name of a secret containing CA certificates. See the section entando-ca-cert-secret.\n    entando.assume.external.https.provider: Set this to \"true\" if your cloud provider handles HTTPS for you.\n    entando.use.auto.cert.generation: Set this to \"true\" to have Openshift use its internal CA to generate certificates for your Routes.\n    entando.default.routing.suffix: The domain name that can be suffixed to deployment names when the ingressHostName is omitted. Needs to be preconfigured on your DNS provider.\n    entando.pod.completion.timeout.seconds: The time it will take before Entando fails a run-to-completion Pod.\n    entando.pod.readiness.timeout.seconds: The time it will take before Entando fails a Service Pod.\n    entando.pod.shutdown.timeout.seconds: The time Entando will give a Pod to shutdown gracefully.\n\n\n### entando-pull-secret\nThe secret to be used to pull Entando images from the Red Hat container registry. The name of this secret is hard coded as it is required for the ClusterServiceVersion of this Operator\n### entando-tls-secret\nA standard Kubernetes TLS secret that will be used on all deployments where no custom TLS secret name is specified.\n### entando-ca-cert-secret\nThis is an opaque secret in the Entando Operator's namespace that contains the certificates of all trusted certificate authorities in your environment. This is generally used mainly for self signed certificates. As is generally the case for opaque secrets, there are no constraints on the keys in this secret. However, limit the files inside the secret to X509 certificates or certificate chains. The Entando Operator will load all of these files into a Java keystore that it then configures as the trust store for each container that uses Java.\n",
      "csv_display_name": "Entando Operator",
      "csv_metadata_description": "Processes EntandoKeycloakServer, EntandoApp and EntandoPlugin custom resources and deploys the relevant containers in the Kubernetes cluster.",
      "csv_name": "entando-k8s-operator.v6.3.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:56:37.945000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "entando-k8s-operator",
      "provided_apis": [
        {
          "group": "entando.org",
          "kind": "EntandoCompositeApp",
          "plural": "entandocompositeapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDatabaseService",
          "plural": "entandodatabaseservices",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDeBundle",
          "plural": "entandodebundles",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoKeycloakServer",
          "plural": "entandokeycloakservers",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoPlugin",
          "plural": "entandoplugins",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoAppPluginLink",
          "plural": "entandoapppluginlinks",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoApp",
          "plural": "entandoapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoClusterInfrastructure",
          "plural": "entandoclusterinfrastructures",
          "version": "v1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "6.3.2",
      "version_original": "6.3.2"
    },
    {
      "_id": "610993f56bbb00c64eecd642",
      "alm_examples": [
        {
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:64962f1aa0777ff7bec1605a7c13be82fa7596b3cc997ae62500b7714b75c084",
      "bundle_path_digest": "sha256:64962f1aa0777ff7bec1605a7c13be82fa7596b3cc997ae62500b7714b75c084",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T19:07:33.388000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.2.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:49:30.248000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "8.2.5",
      "version_original": "8.2.5"
    },
    {
      "_id": "610993fcdd1aaa9129b0cc82",
      "alm_examples": [
        {
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:64962f1aa0777ff7bec1605a7c13be82fa7596b3cc997ae62500b7714b75c084",
      "bundle_path_digest": "sha256:64962f1aa0777ff7bec1605a7c13be82fa7596b3cc997ae62500b7714b75c084",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T19:07:40.159000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.2.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:19:41.504000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "8.2.5",
      "version_original": "8.2.5"
    },
    {
      "_id": "61099407453d6347735bd2e3",
      "alm_examples": [
        {
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:64962f1aa0777ff7bec1605a7c13be82fa7596b3cc997ae62500b7714b75c084",
      "bundle_path_digest": "sha256:64962f1aa0777ff7bec1605a7c13be82fa7596b3cc997ae62500b7714b75c084",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T19:07:51.252000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.2.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:47:34.804000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "8.2.5",
      "version_original": "8.2.5"
    },
    {
      "_id": "610994a4dd1aaa9129b0cc83",
      "alm_examples": [
        {
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:64962f1aa0777ff7bec1605a7c13be82fa7596b3cc997ae62500b7714b75c084",
      "bundle_path_digest": "sha256:64962f1aa0777ff7bec1605a7c13be82fa7596b3cc997ae62500b7714b75c084",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-03T19:10:28.216000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.2.5",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:22:09.887000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "8.2.5",
      "version_original": "8.2.5"
    },
    {
      "_id": "610a4a05453d6347735bd568",
      "alm_examples": [
        {
          "kind": "OneAgent",
          "metadata": {
            "name": "oneagent",
            "namespace": "dynatrace"
          },
          "spec": {
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "tolerations": [
              {
                "effect": "NoSchedule",
                "key": "node-role.kubernetes.io/master",
                "operator": "Exists"
              }
            ],
            "useUnprivilegedMode": true
          }
        },
        {
          "kind": "OneAgentAPM",
          "metadata": {
            "name": "oneagentapm",
            "namespace": "dynatrace"
          },
          "spec": {
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator-bundle@sha256:76c97a3d3feba26f2126c4dddb3d72648e0f4bda817880b4e13988c9fc69115e",
      "bundle_path_digest": "sha256:76c97a3d3feba26f2126c4dddb3d72648e0f4bda817880b4e13988c9fc69115e",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-04T08:04:21.559000+00:00",
      "csv_description": "The Dynatrace OneAgent Operator allows users to easily deploy full-stack monitoring for [OpenShift clusters](https://www.dynatrace.com/technologies/openshift-monitoring/). The Dynatrace OneAgent automatically monitors the workload running in containers down to the code and request level.\n\n### Installing the OneAgent\nOnce you've installed the Operator, you can create OneAgent custom resources to monitor your environment.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic oneagent --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nThen please add an OneAgent object in the Project where the Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Image Override** - use a copy of the OneAgent container image from a registry other than Docker's or Red Hat's\n* **NodeSelectors** - select a subset of your cluster's nodes to run the Dynatrace OneAgent on, based on labels\n* **Tolerations** - add specific tolerations to the agent so that it can monitor all of the nodes in your cluster; we include the default toleration so that Dynatrace OneAgent also monitors the master nodes\n* **Priority Class Name** - define the priorityClassName for OneAgent pods\n* **Environment variables** - define environment variables for the OneAgent container\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Disable OneAgent Update** - disable the Operator's auto-update feature for OneAgent pods\n* **Enable Istio Auto-config** - automatically create Istio objects for egress communication to the Dynatrace environment from the OneAgent\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-oneagent-operator).\n",
      "csv_display_name": "Dynatrace OneAgent",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-monitoring.v0.10.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:17:31.015000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "oneagent-certified",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "OneAgentAPM",
          "version": "v1alpha1"
        },
        {
          "group": "dynatrace.com",
          "kind": "OneAgent",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "dynatrace-oneagent-operator-0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2-annotation"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "dynatrace-oneagent-operator"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "webhook"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "bootstrapper"
        },
        {
          "digest": "sha256:09c03345575f03325c4000d7eb98e1fc55502d7bb29c6fdb71c8b7e224f1ad7a",
          "image": "registry.connect.redhat.com/dynatrace/oneagent@sha256:09c03345575f03325c4000d7eb98e1fc55502d7bb29c6fdb71c8b7e224f1ad7a",
          "name": "dynatrace_oneagent"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.10.1",
      "version_original": "0.10.1"
    },
    {
      "_id": "610a4aab4b9725c294bb9a7a",
      "alm_examples": [
        {
          "kind": "OneAgent",
          "metadata": {
            "name": "oneagent",
            "namespace": "dynatrace"
          },
          "spec": {
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "tolerations": [
              {
                "effect": "NoSchedule",
                "key": "node-role.kubernetes.io/master",
                "operator": "Exists"
              }
            ],
            "useUnprivilegedMode": true
          }
        },
        {
          "kind": "OneAgentAPM",
          "metadata": {
            "name": "oneagentapm",
            "namespace": "dynatrace"
          },
          "spec": {
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator-bundle@sha256:76c97a3d3feba26f2126c4dddb3d72648e0f4bda817880b4e13988c9fc69115e",
      "bundle_path_digest": "sha256:76c97a3d3feba26f2126c4dddb3d72648e0f4bda817880b4e13988c9fc69115e",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-04T08:07:07.633000+00:00",
      "csv_description": "The Dynatrace OneAgent Operator allows users to easily deploy full-stack monitoring for [OpenShift clusters](https://www.dynatrace.com/technologies/openshift-monitoring/). The Dynatrace OneAgent automatically monitors the workload running in containers down to the code and request level.\n\n### Installing the OneAgent\nOnce you've installed the Operator, you can create OneAgent custom resources to monitor your environment.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic oneagent --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nThen please add an OneAgent object in the Project where the Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Image Override** - use a copy of the OneAgent container image from a registry other than Docker's or Red Hat's\n* **NodeSelectors** - select a subset of your cluster's nodes to run the Dynatrace OneAgent on, based on labels\n* **Tolerations** - add specific tolerations to the agent so that it can monitor all of the nodes in your cluster; we include the default toleration so that Dynatrace OneAgent also monitors the master nodes\n* **Priority Class Name** - define the priorityClassName for OneAgent pods\n* **Environment variables** - define environment variables for the OneAgent container\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Disable OneAgent Update** - disable the Operator's auto-update feature for OneAgent pods\n* **Enable Istio Auto-config** - automatically create Istio objects for egress communication to the Dynatrace environment from the OneAgent\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-oneagent-operator).\n",
      "csv_display_name": "Dynatrace OneAgent",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-monitoring.v0.10.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:12:39.152000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "oneagent-certified",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "OneAgentAPM",
          "plural": "oneagentapms",
          "version": "v1alpha1"
        },
        {
          "group": "dynatrace.com",
          "kind": "OneAgent",
          "plural": "oneagents",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "dynatrace-oneagent-operator-0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2-annotation"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "dynatrace-oneagent-operator"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "webhook"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "bootstrapper"
        },
        {
          "digest": "sha256:09c03345575f03325c4000d7eb98e1fc55502d7bb29c6fdb71c8b7e224f1ad7a",
          "image": "registry.connect.redhat.com/dynatrace/oneagent@sha256:09c03345575f03325c4000d7eb98e1fc55502d7bb29c6fdb71c8b7e224f1ad7a",
          "name": "dynatrace_oneagent"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.10.1",
      "version_original": "0.10.1"
    },
    {
      "_id": "610a4b3c6bbb00c64eecd944",
      "alm_examples": [
        {
          "kind": "OneAgent",
          "metadata": {
            "name": "oneagent",
            "namespace": "dynatrace"
          },
          "spec": {
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "tolerations": [
              {
                "effect": "NoSchedule",
                "key": "node-role.kubernetes.io/master",
                "operator": "Exists"
              }
            ],
            "useUnprivilegedMode": true
          }
        },
        {
          "kind": "OneAgentAPM",
          "metadata": {
            "name": "oneagentapm",
            "namespace": "dynatrace"
          },
          "spec": {
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator-bundle@sha256:76c97a3d3feba26f2126c4dddb3d72648e0f4bda817880b4e13988c9fc69115e",
      "bundle_path_digest": "sha256:76c97a3d3feba26f2126c4dddb3d72648e0f4bda817880b4e13988c9fc69115e",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-04T08:09:32.487000+00:00",
      "csv_description": "The Dynatrace OneAgent Operator allows users to easily deploy full-stack monitoring for [OpenShift clusters](https://www.dynatrace.com/technologies/openshift-monitoring/). The Dynatrace OneAgent automatically monitors the workload running in containers down to the code and request level.\n\n### Installing the OneAgent\nOnce you've installed the Operator, you can create OneAgent custom resources to monitor your environment.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic oneagent --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nThen please add an OneAgent object in the Project where the Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Image Override** - use a copy of the OneAgent container image from a registry other than Docker's or Red Hat's\n* **NodeSelectors** - select a subset of your cluster's nodes to run the Dynatrace OneAgent on, based on labels\n* **Tolerations** - add specific tolerations to the agent so that it can monitor all of the nodes in your cluster; we include the default toleration so that Dynatrace OneAgent also monitors the master nodes\n* **Priority Class Name** - define the priorityClassName for OneAgent pods\n* **Environment variables** - define environment variables for the OneAgent container\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Disable OneAgent Update** - disable the Operator's auto-update feature for OneAgent pods\n* **Enable Istio Auto-config** - automatically create Istio objects for egress communication to the Dynatrace environment from the OneAgent\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-oneagent-operator).\n",
      "csv_display_name": "Dynatrace OneAgent",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-monitoring.v0.10.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T15:06:07.523000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "oneagent-certified",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "OneAgentAPM",
          "plural": "oneagentapms",
          "version": "v1alpha1"
        },
        {
          "group": "dynatrace.com",
          "kind": "OneAgent",
          "plural": "oneagents",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "dynatrace-oneagent-operator-0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2-annotation"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "dynatrace-oneagent-operator"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "webhook"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "bootstrapper"
        },
        {
          "digest": "sha256:09c03345575f03325c4000d7eb98e1fc55502d7bb29c6fdb71c8b7e224f1ad7a",
          "image": "registry.connect.redhat.com/dynatrace/oneagent@sha256:09c03345575f03325c4000d7eb98e1fc55502d7bb29c6fdb71c8b7e224f1ad7a",
          "name": "dynatrace_oneagent"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.10.1",
      "version_original": "0.10.1"
    },
    {
      "_id": "610a4d39dd1aaa9129b0d053",
      "alm_examples": [
        {
          "kind": "OneAgent",
          "metadata": {
            "name": "oneagent",
            "namespace": "dynatrace"
          },
          "spec": {
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "tolerations": [
              {
                "effect": "NoSchedule",
                "key": "node-role.kubernetes.io/master",
                "operator": "Exists"
              }
            ],
            "useUnprivilegedMode": true
          }
        },
        {
          "kind": "OneAgentAPM",
          "metadata": {
            "name": "oneagentapm",
            "namespace": "dynatrace"
          },
          "spec": {
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator-bundle@sha256:76c97a3d3feba26f2126c4dddb3d72648e0f4bda817880b4e13988c9fc69115e",
      "bundle_path_digest": "sha256:76c97a3d3feba26f2126c4dddb3d72648e0f4bda817880b4e13988c9fc69115e",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-04T08:18:01.326000+00:00",
      "csv_description": "The Dynatrace OneAgent Operator allows users to easily deploy full-stack monitoring for [OpenShift clusters](https://www.dynatrace.com/technologies/openshift-monitoring/). The Dynatrace OneAgent automatically monitors the workload running in containers down to the code and request level.\n\n### Installing the OneAgent\nOnce you've installed the Operator, you can create OneAgent custom resources to monitor your environment.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic oneagent --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nThen please add an OneAgent object in the Project where the Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Image Override** - use a copy of the OneAgent container image from a registry other than Docker's or Red Hat's\n* **NodeSelectors** - select a subset of your cluster's nodes to run the Dynatrace OneAgent on, based on labels\n* **Tolerations** - add specific tolerations to the agent so that it can monitor all of the nodes in your cluster; we include the default toleration so that Dynatrace OneAgent also monitors the master nodes\n* **Priority Class Name** - define the priorityClassName for OneAgent pods\n* **Environment variables** - define environment variables for the OneAgent container\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Disable OneAgent Update** - disable the Operator's auto-update feature for OneAgent pods\n* **Enable Istio Auto-config** - automatically create Istio objects for egress communication to the Dynatrace environment from the OneAgent\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-oneagent-operator).\n",
      "csv_display_name": "Dynatrace OneAgent",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-monitoring.v0.10.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:46:34.343000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "oneagent-certified",
      "provided_apis": [],
      "related_images": [
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "dynatrace-oneagent-operator-0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2-annotation"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "dynatrace-oneagent-operator"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "webhook"
        },
        {
          "digest": "sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator@sha256:0c91309827f6b765068ac4f1e9b0e77c188e2aa4f127fb1463c5b0fc551826e2",
          "name": "bootstrapper"
        },
        {
          "digest": "sha256:09c03345575f03325c4000d7eb98e1fc55502d7bb29c6fdb71c8b7e224f1ad7a",
          "image": "registry.connect.redhat.com/dynatrace/oneagent@sha256:09c03345575f03325c4000d7eb98e1fc55502d7bb29c6fdb71c8b7e224f1ad7a",
          "name": "dynatrace_oneagent"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "0.10.1",
      "version_original": "0.10.1"
    },
    {
      "_id": "610a65e4453d6347735bd6a0",
      "alm_examples": [
        {
          "kind": "AtlasCluster",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "name": "test-cluster",
            "projectRef": {
              "name": "my-project"
            },
            "providerSettings": {
              "instanceSizeName": "M10",
              "providerName": "AWS",
              "regionName": "US_EAST_1"
            }
          }
        },
        {
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:60239e3cd0dde55b7e5836daa867eecfa0ec6cf2a91c03fe03b2fa1acdd59f96",
      "bundle_path_digest": "sha256:60239e3cd0dde55b7e5836daa867eecfa0ec6cf2a91c03fe03b2fa1acdd59f96",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-04T10:03:16.396000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *trial version*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes. More features like private endpoints, backup management, LDAP/X.509 authentication, etc.\n> are yet to come.\n\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasCluster` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasCluster\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasCluster` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v0.6.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:20:53.998000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "plural": "atlasdatabaseusers",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "plural": "atlasprojects",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasCluster",
          "plural": "atlasclusters",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:5f15d2d76518e1fcc87051651333167bf40df6dd060992f64e65ff8c1e1029be",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:5f15d2d76518e1fcc87051651333167bf40df6dd060992f64e65ff8c1e1029be",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.6.1",
      "version_original": "0.6.1"
    },
    {
      "_id": "610a68746bbb00c64eecd9d8",
      "alm_examples": [
        {
          "kind": "AtlasCluster",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "name": "test-cluster",
            "projectRef": {
              "name": "my-project"
            },
            "providerSettings": {
              "instanceSizeName": "M10",
              "providerName": "AWS",
              "regionName": "US_EAST_1"
            }
          }
        },
        {
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:60239e3cd0dde55b7e5836daa867eecfa0ec6cf2a91c03fe03b2fa1acdd59f96",
      "bundle_path_digest": "sha256:60239e3cd0dde55b7e5836daa867eecfa0ec6cf2a91c03fe03b2fa1acdd59f96",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-04T10:14:12.204000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *trial version*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes. More features like private endpoints, backup management, LDAP/X.509 authentication, etc.\n> are yet to come.\n\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasCluster` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasCluster\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasCluster` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v0.6.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:37:49.210000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "plural": "atlasprojects",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasCluster",
          "plural": "atlasclusters",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "plural": "atlasdatabaseusers",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:5f15d2d76518e1fcc87051651333167bf40df6dd060992f64e65ff8c1e1029be",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:5f15d2d76518e1fcc87051651333167bf40df6dd060992f64e65ff8c1e1029be",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "0.6.1",
      "version_original": "0.6.1"
    },
    {
      "_id": "610a6ebe6bbb00c64eecd9e5",
      "alm_examples": [
        {
          "kind": "AtlasCluster",
          "metadata": {
            "name": "my-atlas-cluster"
          },
          "spec": {
            "name": "test-cluster",
            "projectRef": {
              "name": "my-project"
            },
            "providerSettings": {
              "instanceSizeName": "M10",
              "providerName": "AWS",
              "regionName": "US_EAST_1"
            }
          }
        },
        {
          "kind": "AtlasDatabaseUser",
          "metadata": {
            "name": "my-database-user"
          },
          "spec": {
            "databaseName": "admin",
            "passwordSecretRef": {
              "name": "my-database-user-password"
            },
            "projectRef": {
              "name": "my-project"
            },
            "roles": [
              {
                "databaseName": "admin",
                "roleName": "readWriteAnyDatabase"
              }
            ],
            "username": "david"
          }
        },
        {
          "kind": "AtlasProject",
          "metadata": {
            "name": "my-project"
          },
          "spec": {
            "name": "Test Atlas Operator Project",
            "projectIpAccessList": [
              {
                "comment": "IP address for Application Server A",
                "ipAddress": "192.0.2.15"
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator-bundle@sha256:60239e3cd0dde55b7e5836daa867eecfa0ec6cf2a91c03fe03b2fa1acdd59f96",
      "bundle_path_digest": "sha256:60239e3cd0dde55b7e5836daa867eecfa0ec6cf2a91c03fe03b2fa1acdd59f96",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-04T10:41:02.966000+00:00",
      "csv_description": "The MongoDB Atlas Operator provides a native integration between the Kubernetes orchestration platform and MongoDB Atlas \u2014\nthe only multi-cloud document database service that gives you the versatility you need to build sophisticated and resilient applications that can adapt to changing customer demands and market trends.\n\n> Current Status: *trial version*. The Operator gives users the ability to provision\n> Atlas projects, clusters and database users using Kubernetes Specifications and bind connection information\n> into applications deployed to Kubernetes. More features like private endpoints, backup management, LDAP/X.509 authentication, etc.\n> are yet to come.\n\n\n## Quick Start guide\n### Step 1. Deploy Kubernetes operator by clicking Install button.\n\n### Step 2. Create Atlas Cluster\n\n**1.** Create an Atlas API Key Secret\nIn order to work with the Atlas Operator you need to provide [authentication information](https://docs.atlas.mongodb.com/configure-api-access)\n to allow the Atlas Operator to communicate with Atlas API. Once you have generated a Public and Private key in Atlas, you can create a Kuberentes Secret with:\n```\nkubectl create secret generic mongodb-atlas-operator-api-key \\\n         --from-literal=\"orgId=<the_atlas_organization_id>\" \\\n         --from-literal=\"publicApiKey=<the_atlas_api_public_key>\" \\\n         --from-literal=\"privateApiKey=<the_atlas_api_private_key>\" \\\n         -n openshift-operators\n```\n(Note, that you should use the namespace where the Operator was installed - it's `openshift-operators` by default)\n\n**2.** Create an `AtlasProject` Custom Resource\n\nThe `AtlasProject` CustomResource represents Atlas Projects in our Kubernetes cluster. You need to specify\n`projectIpAccessList` with the IP addresses or CIDR blocks of any hosts that will connect to the Atlas Cluster.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasProject\nmetadata:\n  name: my-project\nspec:\n  name: Test Atlas Operator Project\n  projectIpAccessList:\n    - ipAddress: \"192.0.2.15\"\n      comment: \"IP address for Application Server A\"\n    - ipAddress: \"203.0.113.0/24\"\n      comment: \"CIDR block for Application Server B - D\"\n```\n**3.** Create an `AtlasCluster` Custom Resource.\nThe example below is a minimal configuration to create an M10 Atlas cluster in the AWS US East region. For a full list of properties, check\n`atlasclusters.atlas.mongodb.com` [CRD specification](config/crd/bases/atlas.mongodb.com_atlasclusters.yaml)):\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasCluster\nmetadata:\n  name: my-atlas-cluster\nspec:\n  name: \"Test-cluster\"\n  projectRef:\n    name: my-project\n  providerSettings:\n    instanceSizeName: M10\n    providerName: AWS\n    regionName: US_EAST_1\n```\n\n**4.** Create a database user password Kubernetes Secret\nThe Secret must be created in the same namespace as the `AtlasCluster` and `AtlasProject` were created.\n```\nkubectl create secret generic the-user-password --from-literal=\"password=P@@sword%\"\n```\n\n**5.** Create an `AtlasDatabaseUser` Custom Resource\n\nIn order to connect to an Atlas Cluster the database user needs to be created. `AtlasDatabaseUser` resource should reference\nthe password Kubernetes Secret created in the previous step.\n```\napiVersion: atlas.mongodb.com/v1\nkind: AtlasDatabaseUser\nmetadata:\n  name: my-database-user\nspec:\n  roles:\n    - roleName: \"readWriteAnyDatabase\"\n      databaseName: \"admin\"\n  projectRef:\n    name: my-project\n  username: theuser\n  passwordSecretRef:\n    name: the-user-password\n```\n**6.** Wait for the `AtlasDatabaseUser` Custom Resource to be ready\n\nWait until the AtlasDatabaseUser resource gets to \"ready\" status (it will wait until the cluster is created that may take around 10 minutes):\n```\nkubectl get atlasdatabaseusers my-database-user -o=jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\nTrue\n```\n### Step 3. Connect your application to the Atlas Cluster\n\nThe Atlas Operator will create a Kubernetes Secret with the information necessary to connect to the Atlas Cluster created\nin the previous step. An application in the same Kubernetes Cluster can mount and use the Secret:\n\n```\n...\ncontainers:\n      - name: test-app\n        env:\n         - name: \"CONNECTION_STRING\"\n           valueFrom:\n             secretKeyRef:\n               name: test-atlas-operator-project-test-cluster-theuser\n               key: connectionStringStandardSrv\n\n```\n",
      "csv_display_name": "MongoDB Atlas Operator",
      "csv_metadata_description": "The MongoDB Atlas Kubernetes Operator enables easy management of Clusters in MongoDB Atlas",
      "csv_name": "mongodb-atlas-kubernetes.v0.6.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T15:10:45.109000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "mongodb-atlas-kubernetes",
      "provided_apis": [
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasCluster",
          "plural": "atlasclusters",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasDatabaseUser",
          "plural": "atlasdatabaseusers",
          "version": "v1"
        },
        {
          "group": "atlas.mongodb.com",
          "kind": "AtlasProject",
          "plural": "atlasprojects",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:5f15d2d76518e1fcc87051651333167bf40df6dd060992f64e65ff8c1e1029be",
          "image": "registry.connect.redhat.com/mongodb/mongodb-atlas-kubernetes-operator@sha256:5f15d2d76518e1fcc87051651333167bf40df6dd060992f64e65ff8c1e1029be",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.6.1",
      "version_original": "0.6.1"
    },
    {
      "_id": "610bc8bf6bbb00c64eecdab8",
      "alm_examples": [
        {
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": "1",
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": " ",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be \n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,vci,pgaudit,pg_prewarm'\nsession_preload_libraries='vci,pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be \n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n\n# vci parameters in default fep installation\nvci.enable = on\nvci.maintenance_work_mem = 256MB\nvci.max_local_ros = 64MB\nvci.force_max_parallelism = off\n\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\n\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        },
        {
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'DISALLOW_TO_FAILOVER'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nconnect_timeout = 10000\nsr_check_period = 0\nsr_check_user = ' postgres '\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = off\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = master\n\n",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent"
          }
        },
        {
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "toFEPcluster": "new-fep-2"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-bundle@sha256:c7c2737e5b4946c78c1f1d5433aef205c383681299b315784d3e5b7b46d2de34",
      "bundle_path_digest": "sha256:c7c2737e5b4946c78c1f1d5433aef205c383681299b315784d3e5b7b46d2de34",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-05T11:17:19.280000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 12 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution\nof PostgreSQL to support enterprise-level workloads and provide improved deployment and management,\navailability, performance, data governance and security.\n\nAvailable as a multi-architecture container built for both amd64 and s390x.\n\nUse of the product is subject to Fujitsu evaluation license located at:\nhttps://www.fast.fujitsu.com/fujitsu-enterprise-postgres-trial-version-software-evaluation-license-agreement\nand the license period is 90 days after the download\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 12 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 12",
      "csv_name": "fujitsu-enterprise-operator.v2.2.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:57:53.941000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "fep-ansible-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:f168bdac1c42b864ebb18f843debbd4da9033cd465025d01dcb0490b9fb573d9",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:f168bdac1c42b864ebb18f843debbd4da9033cd465025d01dcb0490b9fb573d9",
          "name": "fujitsu-enterprise-postgres-12-operator-f168bdac1c42b864ebb18f843debbd4da9033cd465025d01dcb0490b9fb573d9-annotation"
        },
        {
          "digest": "sha256:618b61707b38fe8a9636766261a5ea70b45f4c2549dde769c5b20a8ebca8f59d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:618b61707b38fe8a9636766261a5ea70b45f4c2549dde769c5b20a8ebca8f59d",
          "name": "fep-ansible-operator"
        },
        {
          "digest": "sha256:a9edcb9a3464c19bd6e7d772a084f05452b893b9ae3dc1dbf30da35dcb681ee3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:a9edcb9a3464c19bd6e7d772a084f05452b893b9ae3dc1dbf30da35dcb681ee3",
          "name": "fep"
        },
        {
          "digest": "sha256:953833154e75d79b15f96790368ad38fb20f05a48bf1b75be1968632fd5bdb40",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:953833154e75d79b15f96790368ad38fb20f05a48bf1b75be1968632fd5bdb40",
          "name": "backup"
        },
        {
          "digest": "sha256:0bdff064fcebea38f4d796ddbf0de211e4ba6bb297e42dc514d5ef44980ee313",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:0bdff064fcebea38f4d796ddbf0de211e4ba6bb297e42dc514d5ef44980ee313",
          "name": "restore"
        },
        {
          "digest": "sha256:2ecd8feb1a9fec0ed015908a5c53a8d4df405703068d0de21effdb3f3d33a8df",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:2ecd8feb1a9fec0ed015908a5c53a8d4df405703068d0de21effdb3f3d33a8df",
          "name": "pgpool2"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "2.2.1",
      "version_original": "2.2.1"
    },
    {
      "_id": "610c311a1faba499d0f16894",
      "alm_examples": [
        {
          "kind": "Neuvector",
          "metadata": {
            "name": "neuvector"
          },
          "spec": {
            "admissionwebhook": {
              "type": "ClusterIP"
            },
            "bottlerocket": {
              "enabled": false,
              "runtimePath": "/run/dockershim.sock"
            },
            "containerd": {
              "enabled": false,
              "path": "/var/run/containerd/containerd.sock"
            },
            "controller": {
              "apisvc": {
                "annotations": {},
                "route": {
                  "enabled": false,
                  "host": "",
                  "termination": "passthrough"
                },
                "type": ""
              },
              "azureFileShare": {
                "enabled": false,
                "secretName": "",
                "shareName": ""
              },
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "configmap": {
                "data": "",
                "enabled": false
              },
              "disruptionbudget": 0,
              "enabled": true,
              "federation": {
                "managedsvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                },
                "mastersvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                }
              },
              "image": "registry.connect.redhat.com/neuvector/controller",
              "ingress": {
                "annotations": {
                  "ingress.kubernetes.io/protocol": "https"
                },
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "pvc": {
                "accessModes": [
                  "ReadWriteMany"
                ],
                "capacity": "",
                "enabled": false,
                "storageClass": ""
              },
              "replicas": 3,
              "resources": {},
              "strategy": {
                "rollingUpdate": {
                  "maxSurge": 1,
                  "maxUnavailable": 0
                },
                "type": "RollingUpdate"
              }
            },
            "crdwebhook": {
              "enabled": true,
              "type": "ClusterIP"
            },
            "crio": {
              "enabled": true,
              "path": "/var/run/crio/crio.sock"
            },
            "cve": {
              "scanner": {
                "dockerPath": "",
                "enabled": true,
                "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
                "priorityClassName": "",
                "replicas": 3,
                "resources": {},
                "strategy": {
                  "rollingUpdate": {
                    "maxSurge": 1,
                    "maxUnavailable": 0
                  },
                  "type": "RollingUpdate"
                }
              },
              "updater": {
                "enabled": true,
                "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
                "priorityClassName": "",
                "schedule": "0 0 * * *"
              }
            },
            "docker": {
              "enabled": false,
              "path": "/var/run/docker.sock"
            },
            "enforcer": {
              "enabled": true,
              "image": "registry.connect.redhat.com/neuvector/enforcer",
              "priorityClassName": "",
              "resources": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master"
                }
              ]
            },
            "k3s": {
              "enabled": false,
              "runtimePath": "/run/k3s/containerd/containerd.sock"
            },
            "manager": {
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "enabled": true,
              "env": {
                "ssl": true
              },
              "image": "registry.connect.redhat.com/neuvector/manager",
              "ingress": {
                "annotations": {},
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "resources": {},
              "route": {
                "enabled": true,
                "host": "",
                "termination": "passthrough"
              },
              "svc": {
                "annotations": {},
                "loadBalancerIP": "",
                "type": "NodePort"
              }
            },
            "openshift": true,
            "psp": false,
            "resources": {},
            "serviceAccount": "default"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/neuvector/neuvector-operator-bundle@sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "bundle_path_digest": "sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-05T18:42:34.177000+00:00",
      "csv_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.\n\nThe NeuVector Operator runs  in the openshift container platform to deploy and manage the NeuVector Security cluster components. The NeuVector operator contains all necessary information to deploy NeuVector using helm charts. You simply need to install the NeuVector operator from the OpenShift embeded operator hub and create NeuVector instance. You can modify the NeuVector installation configuration by modifying yaml while creating the NeuVector instance such as imagePullSecrets, tag version, etc. Please refer to [github link](https://github.com/neuvector/neuvector-helm/tree/master/charts/core) for the values that can be modifed during installation. To upgrade to a newer version of NeuVector, just reapply the NeuVector instance with desired tag , which in turn pulls the specified NeuVector image tags and upgrades as per upgrade plan configured on the helm chart.  NeuVector Operator versions are tied to NeuVector product versions. Version 1.2.8 of the NeuVector Certified Operator deploys version 4.3.1 of NeuVector.\n\n**Complete below steps to Grant Service Account Access to the Privileged SCC before installation.**\n\nCreate the NeuVector namespace\n\n         oc new-project  neuvector\nLogin as system:admin account\n\n         oc login -u system:admin\n\nGrant Service Account Access to the Privileged SCC\n\n         oc -n neuvector adm policy add-scc-to-user privileged -z default\n\nThe following info will be added in the Privileged SCC users:\n\n         - system:serviceaccount:neuvector:default\n\nIn OpenShift 4.6+ use the following to check:\n\n         oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide\n         NAME                              ROLE                                          AGE     USERS   GROUPS   SERVICEACCOUNTS\n         system:openshift:scc:privileged   ClusterRole/system:openshift:scc:privileged   9m22s                    neuvector/default\n\n\n**Add NeuVector license from NeuVector WebUI->setting**\n\n\n#Deploying the NeuVector Operator#\n\n\nPlease refer to the instructions [here](https://github.com/neuvector/neuvector-operator/blob/master/README.md)\n\n\n",
      "csv_display_name": "NeuVector Operator",
      "csv_metadata_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.",
      "csv_name": "neuvector-operator.v1.2.8",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:13:33.859000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "neuvector-certified-operator",
      "provided_apis": [
        {
          "group": "apm.neuvector.com",
          "kind": "Neuvector",
          "plural": "neuvectors",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator-9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72-annotation"
        },
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator"
        },
        {
          "digest": "sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "image": "registry.connect.redhat.com/neuvector/controller@sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "name": "controller"
        },
        {
          "digest": "sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "image": "registry.connect.redhat.com/neuvector/enforcer@sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "name": "enforcer"
        },
        {
          "digest": "sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "image": "registry.connect.redhat.com/neuvector/manager@sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "name": "manager"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "updater"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner-a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06-annotation"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "ubi8-091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.2.8",
      "version_original": "1.2.8"
    },
    {
      "_id": "610c322b1faba499d0f16895",
      "alm_examples": [
        {
          "kind": "Neuvector",
          "metadata": {
            "name": "neuvector"
          },
          "spec": {
            "admissionwebhook": {
              "type": "ClusterIP"
            },
            "bottlerocket": {
              "enabled": false,
              "runtimePath": "/run/dockershim.sock"
            },
            "containerd": {
              "enabled": false,
              "path": "/var/run/containerd/containerd.sock"
            },
            "controller": {
              "apisvc": {
                "annotations": {},
                "route": {
                  "enabled": false,
                  "host": "",
                  "termination": "passthrough"
                },
                "type": ""
              },
              "azureFileShare": {
                "enabled": false,
                "secretName": "",
                "shareName": ""
              },
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "configmap": {
                "data": "",
                "enabled": false
              },
              "disruptionbudget": 0,
              "enabled": true,
              "federation": {
                "managedsvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                },
                "mastersvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                }
              },
              "image": "registry.connect.redhat.com/neuvector/controller",
              "ingress": {
                "annotations": {
                  "ingress.kubernetes.io/protocol": "https"
                },
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "pvc": {
                "accessModes": [
                  "ReadWriteMany"
                ],
                "capacity": "",
                "enabled": false,
                "storageClass": ""
              },
              "replicas": 3,
              "resources": {},
              "strategy": {
                "rollingUpdate": {
                  "maxSurge": 1,
                  "maxUnavailable": 0
                },
                "type": "RollingUpdate"
              }
            },
            "crdwebhook": {
              "enabled": true,
              "type": "ClusterIP"
            },
            "crio": {
              "enabled": true,
              "path": "/var/run/crio/crio.sock"
            },
            "cve": {
              "scanner": {
                "dockerPath": "",
                "enabled": true,
                "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
                "priorityClassName": "",
                "replicas": 3,
                "resources": {},
                "strategy": {
                  "rollingUpdate": {
                    "maxSurge": 1,
                    "maxUnavailable": 0
                  },
                  "type": "RollingUpdate"
                }
              },
              "updater": {
                "enabled": true,
                "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
                "priorityClassName": "",
                "schedule": "0 0 * * *"
              }
            },
            "docker": {
              "enabled": false,
              "path": "/var/run/docker.sock"
            },
            "enforcer": {
              "enabled": true,
              "image": "registry.connect.redhat.com/neuvector/enforcer",
              "priorityClassName": "",
              "resources": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master"
                }
              ]
            },
            "k3s": {
              "enabled": false,
              "runtimePath": "/run/k3s/containerd/containerd.sock"
            },
            "manager": {
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "enabled": true,
              "env": {
                "ssl": true
              },
              "image": "registry.connect.redhat.com/neuvector/manager",
              "ingress": {
                "annotations": {},
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "resources": {},
              "route": {
                "enabled": true,
                "host": "",
                "termination": "passthrough"
              },
              "svc": {
                "annotations": {},
                "loadBalancerIP": "",
                "type": "NodePort"
              }
            },
            "openshift": true,
            "psp": false,
            "resources": {},
            "serviceAccount": "default"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/neuvector/neuvector-operator-bundle@sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "bundle_path_digest": "sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-05T18:47:07.689000+00:00",
      "csv_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.\n\nThe NeuVector Operator runs  in the openshift container platform to deploy and manage the NeuVector Security cluster components. The NeuVector operator contains all necessary information to deploy NeuVector using helm charts. You simply need to install the NeuVector operator from the OpenShift embeded operator hub and create NeuVector instance. You can modify the NeuVector installation configuration by modifying yaml while creating the NeuVector instance such as imagePullSecrets, tag version, etc. Please refer to [github link](https://github.com/neuvector/neuvector-helm/tree/master/charts/core) for the values that can be modifed during installation. To upgrade to a newer version of NeuVector, just reapply the NeuVector instance with desired tag , which in turn pulls the specified NeuVector image tags and upgrades as per upgrade plan configured on the helm chart.  NeuVector Operator versions are tied to NeuVector product versions. Version 1.2.8 of the NeuVector Certified Operator deploys version 4.3.1 of NeuVector.\n\n**Complete below steps to Grant Service Account Access to the Privileged SCC before installation.**\n\nCreate the NeuVector namespace\n\n         oc new-project  neuvector\nLogin as system:admin account\n\n         oc login -u system:admin\n\nGrant Service Account Access to the Privileged SCC\n\n         oc -n neuvector adm policy add-scc-to-user privileged -z default\n\nThe following info will be added in the Privileged SCC users:\n\n         - system:serviceaccount:neuvector:default\n\nIn OpenShift 4.6+ use the following to check:\n\n         oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide\n         NAME                              ROLE                                          AGE     USERS   GROUPS   SERVICEACCOUNTS\n         system:openshift:scc:privileged   ClusterRole/system:openshift:scc:privileged   9m22s                    neuvector/default\n\n\n**Add NeuVector license from NeuVector WebUI->setting**\n\n\n#Deploying the NeuVector Operator#\n\n\nPlease refer to the instructions [here](https://github.com/neuvector/neuvector-operator/blob/master/README.md)\n\n\n",
      "csv_display_name": "NeuVector Operator",
      "csv_metadata_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.",
      "csv_name": "neuvector-operator.v1.2.8",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:16:31.723000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "neuvector-certified-operator",
      "provided_apis": [
        {
          "group": "apm.neuvector.com",
          "kind": "Neuvector",
          "plural": "neuvectors",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator-9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72-annotation"
        },
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator"
        },
        {
          "digest": "sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "image": "registry.connect.redhat.com/neuvector/controller@sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "name": "controller"
        },
        {
          "digest": "sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "image": "registry.connect.redhat.com/neuvector/enforcer@sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "name": "enforcer"
        },
        {
          "digest": "sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "image": "registry.connect.redhat.com/neuvector/manager@sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "name": "manager"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "updater"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner-a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06-annotation"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "ubi8-091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.2.8",
      "version_original": "1.2.8"
    },
    {
      "_id": "610c330b0f5e559095d7e1ed",
      "alm_examples": [
        {
          "kind": "Neuvector",
          "metadata": {
            "name": "neuvector"
          },
          "spec": {
            "admissionwebhook": {
              "type": "ClusterIP"
            },
            "bottlerocket": {
              "enabled": false,
              "runtimePath": "/run/dockershim.sock"
            },
            "containerd": {
              "enabled": false,
              "path": "/var/run/containerd/containerd.sock"
            },
            "controller": {
              "apisvc": {
                "annotations": {},
                "route": {
                  "enabled": false,
                  "host": "",
                  "termination": "passthrough"
                },
                "type": ""
              },
              "azureFileShare": {
                "enabled": false,
                "secretName": "",
                "shareName": ""
              },
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "configmap": {
                "data": "",
                "enabled": false
              },
              "disruptionbudget": 0,
              "enabled": true,
              "federation": {
                "managedsvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                },
                "mastersvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                }
              },
              "image": "registry.connect.redhat.com/neuvector/controller",
              "ingress": {
                "annotations": {
                  "ingress.kubernetes.io/protocol": "https"
                },
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "pvc": {
                "accessModes": [
                  "ReadWriteMany"
                ],
                "capacity": "",
                "enabled": false,
                "storageClass": ""
              },
              "replicas": 3,
              "resources": {},
              "strategy": {
                "rollingUpdate": {
                  "maxSurge": 1,
                  "maxUnavailable": 0
                },
                "type": "RollingUpdate"
              }
            },
            "crdwebhook": {
              "enabled": true,
              "type": "ClusterIP"
            },
            "crio": {
              "enabled": true,
              "path": "/var/run/crio/crio.sock"
            },
            "cve": {
              "scanner": {
                "dockerPath": "",
                "enabled": true,
                "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
                "priorityClassName": "",
                "replicas": 3,
                "resources": {},
                "strategy": {
                  "rollingUpdate": {
                    "maxSurge": 1,
                    "maxUnavailable": 0
                  },
                  "type": "RollingUpdate"
                }
              },
              "updater": {
                "enabled": true,
                "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
                "priorityClassName": "",
                "schedule": "0 0 * * *"
              }
            },
            "docker": {
              "enabled": false,
              "path": "/var/run/docker.sock"
            },
            "enforcer": {
              "enabled": true,
              "image": "registry.connect.redhat.com/neuvector/enforcer",
              "priorityClassName": "",
              "resources": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master"
                }
              ]
            },
            "k3s": {
              "enabled": false,
              "runtimePath": "/run/k3s/containerd/containerd.sock"
            },
            "manager": {
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "enabled": true,
              "env": {
                "ssl": true
              },
              "image": "registry.connect.redhat.com/neuvector/manager",
              "ingress": {
                "annotations": {},
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "resources": {},
              "route": {
                "enabled": true,
                "host": "",
                "termination": "passthrough"
              },
              "svc": {
                "annotations": {},
                "loadBalancerIP": "",
                "type": "NodePort"
              }
            },
            "openshift": true,
            "psp": false,
            "resources": {},
            "serviceAccount": "default"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/neuvector/neuvector-operator-bundle@sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "bundle_path_digest": "sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-05T18:50:51.421000+00:00",
      "csv_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.\n\nThe NeuVector Operator runs  in the openshift container platform to deploy and manage the NeuVector Security cluster components. The NeuVector operator contains all necessary information to deploy NeuVector using helm charts. You simply need to install the NeuVector operator from the OpenShift embeded operator hub and create NeuVector instance. You can modify the NeuVector installation configuration by modifying yaml while creating the NeuVector instance such as imagePullSecrets, tag version, etc. Please refer to [github link](https://github.com/neuvector/neuvector-helm/tree/master/charts/core) for the values that can be modifed during installation. To upgrade to a newer version of NeuVector, just reapply the NeuVector instance with desired tag , which in turn pulls the specified NeuVector image tags and upgrades as per upgrade plan configured on the helm chart.  NeuVector Operator versions are tied to NeuVector product versions. Version 1.2.8 of the NeuVector Certified Operator deploys version 4.3.1 of NeuVector.\n\n**Complete below steps to Grant Service Account Access to the Privileged SCC before installation.**\n\nCreate the NeuVector namespace\n\n         oc new-project  neuvector\nLogin as system:admin account\n\n         oc login -u system:admin\n\nGrant Service Account Access to the Privileged SCC\n\n         oc -n neuvector adm policy add-scc-to-user privileged -z default\n\nThe following info will be added in the Privileged SCC users:\n\n         - system:serviceaccount:neuvector:default\n\nIn OpenShift 4.6+ use the following to check:\n\n         oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide\n         NAME                              ROLE                                          AGE     USERS   GROUPS   SERVICEACCOUNTS\n         system:openshift:scc:privileged   ClusterRole/system:openshift:scc:privileged   9m22s                    neuvector/default\n\n\n**Add NeuVector license from NeuVector WebUI->setting**\n\n\n#Deploying the NeuVector Operator#\n\n\nPlease refer to the instructions [here](https://github.com/neuvector/neuvector-operator/blob/master/README.md)\n\n\n",
      "csv_display_name": "NeuVector Operator",
      "csv_metadata_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.",
      "csv_name": "neuvector-operator.v1.2.8",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:49:33.347000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "neuvector-certified-operator",
      "provided_apis": [
        {
          "group": "apm.neuvector.com",
          "kind": "Neuvector",
          "plural": "neuvectors",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator-9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72-annotation"
        },
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator"
        },
        {
          "digest": "sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "image": "registry.connect.redhat.com/neuvector/controller@sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "name": "controller"
        },
        {
          "digest": "sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "image": "registry.connect.redhat.com/neuvector/enforcer@sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "name": "enforcer"
        },
        {
          "digest": "sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "image": "registry.connect.redhat.com/neuvector/manager@sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "name": "manager"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "updater"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner-a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06-annotation"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "ubi8-091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.2.8",
      "version_original": "1.2.8"
    },
    {
      "_id": "610c33761faba499d0f16896",
      "alm_examples": [
        {
          "kind": "Neuvector",
          "metadata": {
            "name": "neuvector"
          },
          "spec": {
            "admissionwebhook": {
              "type": "ClusterIP"
            },
            "bottlerocket": {
              "enabled": false,
              "runtimePath": "/run/dockershim.sock"
            },
            "containerd": {
              "enabled": false,
              "path": "/var/run/containerd/containerd.sock"
            },
            "controller": {
              "apisvc": {
                "annotations": {},
                "route": {
                  "enabled": false,
                  "host": "",
                  "termination": "passthrough"
                },
                "type": ""
              },
              "azureFileShare": {
                "enabled": false,
                "secretName": "",
                "shareName": ""
              },
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "configmap": {
                "data": "",
                "enabled": false
              },
              "disruptionbudget": 0,
              "enabled": true,
              "federation": {
                "managedsvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                },
                "mastersvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                }
              },
              "image": "registry.connect.redhat.com/neuvector/controller",
              "ingress": {
                "annotations": {
                  "ingress.kubernetes.io/protocol": "https"
                },
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "pvc": {
                "accessModes": [
                  "ReadWriteMany"
                ],
                "capacity": "",
                "enabled": false,
                "storageClass": ""
              },
              "replicas": 3,
              "resources": {},
              "strategy": {
                "rollingUpdate": {
                  "maxSurge": 1,
                  "maxUnavailable": 0
                },
                "type": "RollingUpdate"
              }
            },
            "crdwebhook": {
              "enabled": true,
              "type": "ClusterIP"
            },
            "crio": {
              "enabled": true,
              "path": "/var/run/crio/crio.sock"
            },
            "cve": {
              "scanner": {
                "dockerPath": "",
                "enabled": true,
                "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
                "priorityClassName": "",
                "replicas": 3,
                "resources": {},
                "strategy": {
                  "rollingUpdate": {
                    "maxSurge": 1,
                    "maxUnavailable": 0
                  },
                  "type": "RollingUpdate"
                }
              },
              "updater": {
                "enabled": true,
                "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
                "priorityClassName": "",
                "schedule": "0 0 * * *"
              }
            },
            "docker": {
              "enabled": false,
              "path": "/var/run/docker.sock"
            },
            "enforcer": {
              "enabled": true,
              "image": "registry.connect.redhat.com/neuvector/enforcer",
              "priorityClassName": "",
              "resources": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master"
                }
              ]
            },
            "k3s": {
              "enabled": false,
              "runtimePath": "/run/k3s/containerd/containerd.sock"
            },
            "manager": {
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "enabled": true,
              "env": {
                "ssl": true
              },
              "image": "registry.connect.redhat.com/neuvector/manager",
              "ingress": {
                "annotations": {},
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "resources": {},
              "route": {
                "enabled": true,
                "host": "",
                "termination": "passthrough"
              },
              "svc": {
                "annotations": {},
                "loadBalancerIP": "",
                "type": "NodePort"
              }
            },
            "openshift": true,
            "psp": false,
            "resources": {},
            "serviceAccount": "default"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/neuvector/neuvector-operator-bundle@sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "bundle_path_digest": "sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-05T18:52:38.422000+00:00",
      "csv_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.\n\nThe NeuVector Operator runs  in the openshift container platform to deploy and manage the NeuVector Security cluster components. The NeuVector operator contains all necessary information to deploy NeuVector using helm charts. You simply need to install the NeuVector operator from the OpenShift embeded operator hub and create NeuVector instance. You can modify the NeuVector installation configuration by modifying yaml while creating the NeuVector instance such as imagePullSecrets, tag version, etc. Please refer to [github link](https://github.com/neuvector/neuvector-helm/tree/master/charts/core) for the values that can be modifed during installation. To upgrade to a newer version of NeuVector, just reapply the NeuVector instance with desired tag , which in turn pulls the specified NeuVector image tags and upgrades as per upgrade plan configured on the helm chart.  NeuVector Operator versions are tied to NeuVector product versions. Version 1.2.8 of the NeuVector Certified Operator deploys version 4.3.1 of NeuVector.\n\n**Complete below steps to Grant Service Account Access to the Privileged SCC before installation.**\n\nCreate the NeuVector namespace\n\n         oc new-project  neuvector\nLogin as system:admin account\n\n         oc login -u system:admin\n\nGrant Service Account Access to the Privileged SCC\n\n         oc -n neuvector adm policy add-scc-to-user privileged -z default\n\nThe following info will be added in the Privileged SCC users:\n\n         - system:serviceaccount:neuvector:default\n\nIn OpenShift 4.6+ use the following to check:\n\n         oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide\n         NAME                              ROLE                                          AGE     USERS   GROUPS   SERVICEACCOUNTS\n         system:openshift:scc:privileged   ClusterRole/system:openshift:scc:privileged   9m22s                    neuvector/default\n\n\n**Add NeuVector license from NeuVector WebUI->setting**\n\n\n#Deploying the NeuVector Operator#\n\n\nPlease refer to the instructions [here](https://github.com/neuvector/neuvector-operator/blob/master/README.md)\n\n\n",
      "csv_display_name": "NeuVector Operator",
      "csv_metadata_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.",
      "csv_name": "neuvector-operator.v1.2.8",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:51:56.318000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "neuvector-certified-operator",
      "provided_apis": [
        {
          "group": "apm.neuvector.com",
          "kind": "Neuvector",
          "plural": "neuvectors",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator-9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72-annotation"
        },
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator"
        },
        {
          "digest": "sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "image": "registry.connect.redhat.com/neuvector/controller@sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "name": "controller"
        },
        {
          "digest": "sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "image": "registry.connect.redhat.com/neuvector/enforcer@sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "name": "enforcer"
        },
        {
          "digest": "sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "image": "registry.connect.redhat.com/neuvector/manager@sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "name": "manager"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "updater"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner-a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06-annotation"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "ubi8-091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.2.8",
      "version_original": "1.2.8"
    },
    {
      "_id": "61112872fbeeedc1ec2e69d9",
      "alm_examples": [
        {
          "kind": "CloudBeesCI",
          "metadata": {
            "name": "cloudbeesci-sample"
          },
          "spec": {
            "Agents": {
              "Enabled": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-core-agent:2.289.3.2"
              },
              "SeparateNamespace": {
                "Create": false,
                "Enabled": false
              }
            },
            "Hibernation": {
              "Enabled": false,
              "Image": {
                "dockerImage": "cloudbees/managed-master-hibernation-monitor:247.c5dfce00a179"
              },
              "NodeSelector": {},
              "Tolerations": []
            },
            "Master": {
              "Enabled": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-core-mm:2.289.3.2"
              }
            },
            "NetworkPolicy": {
              "Enabled": false,
              "JMXSelectors": [],
              "ingressControllerSelector": []
            },
            "OperationsCenter": {
              "AgentListenerPort": 50000,
              "Annotations": {},
              "CSRF": {
                "ProxyCompatibility": false
              },
              "CasC": {
                "ConfigMapName": "oc-casc-bundle",
                "Enabled": false
              },
              "ContainerPort": 8080,
              "Enabled": true,
              "ExtraConfigMaps": [],
              "ExtraContainers": [],
              "ExtraGroovyConfiguration": {},
              "ExtraVolumeMounts": [],
              "ExtraVolumes": [],
              "HealthProbeLivenessFailureThreshold": 12,
              "HealthProbes": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-cloud-core-oc:2.289.3.2"
              },
              "Ingress": {
                "Annotations": {
                  "kubernetes.io/tls-acme": "false"
                },
                "Class": "nginx",
                "tls": {
                  "Enable": false
                }
              },
              "LoadBalancerSourceRanges": [
                "0.0.0.0/0"
              ],
              "Name": "cjoc",
              "NodeSelector": {},
              "Platform": "standard",
              "Protocol": "http",
              "Resources": {
                "Limits": {
                  "Cpu": 1,
                  "Memory": "2G"
                },
                "Requests": {
                  "Cpu": 1,
                  "Memory": "2G"
                }
              },
              "Route": {
                "tls": {
                  "Enable": false,
                  "InsecureEdgeTerminationPolicy": "Redirect",
                  "Termination": "edge"
                }
              },
              "ServiceAgentListenerPort": 50000,
              "ServiceAnnotations": {},
              "ServicePort": 80,
              "ServiceType": "ClusterIP",
              "Tolerations": []
            },
            "Persistence": {
              "AccessMode": "ReadWriteOnce",
              "Annotations": {},
              "Size": "20Gi"
            },
            "PodSecurityPolicy": {
              "Annotations": {},
              "Enabled": false
            },
            "Subdomain": false,
            "ingress-nginx": {
              "Enabled": false,
              "controller": {
                "admissionWebhooks": {
                  "patch": {
                    "nodeSelector": {
                      "kubernetes.io/os": "linux"
                    }
                  }
                },
                "ingressClass": "nginx",
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                },
                "service": {
                  "externalTrafficPolicy": "Local"
                }
              },
              "defaultBackend": {
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                }
              }
            },
            "nginx-ingress": {
              "Enabled": false,
              "controller": {
                "ingressClass": "nginx",
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                },
                "service": {
                  "externalTrafficPolicy": "Local"
                }
              },
              "defaultBackend": {
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                }
              }
            },
            "rbac": {
              "agentsServiceAccountName": "jenkins-agents",
              "hibernationMonitorServiceAccountName": "managed-master-hibernation-monitor",
              "install": true,
              "masterServiceAccountName": "jenkins",
              "serviceAccountName": "cjoc"
            },
            "sidecarinjector": {
              "Enabled": false
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cloudbees/cloudbees-core-modern@sha256:2cf058c819b40512094f84f4d374457455370ec4994d34cee6e39b9d87841f05",
      "bundle_path_digest": "sha256:2cf058c819b40512094f84f4d374457455370ec4994d34cee6e39b9d87841f05",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-09T13:06:58.115000+00:00",
      "csv_description": "CloudBees CI",
      "csv_display_name": "CloudBees CI",
      "csv_metadata_description": "",
      "csv_name": "cloudbees-ci.v3.34.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:14:43.865000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "cloudbees-ci",
      "provided_apis": [
        {
          "group": "charts.cloudbees.com",
          "kind": "CloudBeesCI",
          "plural": "cloudbeescis",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:bd0bd72e9e7807b0fa307de8a93a56967221fe2dd181bac02383f0f9be9120e4",
          "image": "registry.connect.redhat.com/cloudbees/cloudbees-ci-operator@sha256:bd0bd72e9e7807b0fa307de8a93a56967221fe2dd181bac02383f0f9be9120e4",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "3.34.1",
      "version_original": "3.34.1"
    },
    {
      "_id": "611128a6fbeeedc1ec2e69da",
      "alm_examples": [
        {
          "kind": "CloudBeesCI",
          "metadata": {
            "name": "cloudbeesci-sample"
          },
          "spec": {
            "Agents": {
              "Enabled": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-core-agent:2.289.3.2"
              },
              "SeparateNamespace": {
                "Create": false,
                "Enabled": false
              }
            },
            "Hibernation": {
              "Enabled": false,
              "Image": {
                "dockerImage": "cloudbees/managed-master-hibernation-monitor:247.c5dfce00a179"
              },
              "NodeSelector": {},
              "Tolerations": []
            },
            "Master": {
              "Enabled": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-core-mm:2.289.3.2"
              }
            },
            "NetworkPolicy": {
              "Enabled": false,
              "JMXSelectors": [],
              "ingressControllerSelector": []
            },
            "OperationsCenter": {
              "AgentListenerPort": 50000,
              "Annotations": {},
              "CSRF": {
                "ProxyCompatibility": false
              },
              "CasC": {
                "ConfigMapName": "oc-casc-bundle",
                "Enabled": false
              },
              "ContainerPort": 8080,
              "Enabled": true,
              "ExtraConfigMaps": [],
              "ExtraContainers": [],
              "ExtraGroovyConfiguration": {},
              "ExtraVolumeMounts": [],
              "ExtraVolumes": [],
              "HealthProbeLivenessFailureThreshold": 12,
              "HealthProbes": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-cloud-core-oc:2.289.3.2"
              },
              "Ingress": {
                "Annotations": {
                  "kubernetes.io/tls-acme": "false"
                },
                "Class": "nginx",
                "tls": {
                  "Enable": false
                }
              },
              "LoadBalancerSourceRanges": [
                "0.0.0.0/0"
              ],
              "Name": "cjoc",
              "NodeSelector": {},
              "Platform": "standard",
              "Protocol": "http",
              "Resources": {
                "Limits": {
                  "Cpu": 1,
                  "Memory": "2G"
                },
                "Requests": {
                  "Cpu": 1,
                  "Memory": "2G"
                }
              },
              "Route": {
                "tls": {
                  "Enable": false,
                  "InsecureEdgeTerminationPolicy": "Redirect",
                  "Termination": "edge"
                }
              },
              "ServiceAgentListenerPort": 50000,
              "ServiceAnnotations": {},
              "ServicePort": 80,
              "ServiceType": "ClusterIP",
              "Tolerations": []
            },
            "Persistence": {
              "AccessMode": "ReadWriteOnce",
              "Annotations": {},
              "Size": "20Gi"
            },
            "PodSecurityPolicy": {
              "Annotations": {},
              "Enabled": false
            },
            "Subdomain": false,
            "ingress-nginx": {
              "Enabled": false,
              "controller": {
                "admissionWebhooks": {
                  "patch": {
                    "nodeSelector": {
                      "kubernetes.io/os": "linux"
                    }
                  }
                },
                "ingressClass": "nginx",
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                },
                "service": {
                  "externalTrafficPolicy": "Local"
                }
              },
              "defaultBackend": {
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                }
              }
            },
            "nginx-ingress": {
              "Enabled": false,
              "controller": {
                "ingressClass": "nginx",
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                },
                "service": {
                  "externalTrafficPolicy": "Local"
                }
              },
              "defaultBackend": {
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                }
              }
            },
            "rbac": {
              "agentsServiceAccountName": "jenkins-agents",
              "hibernationMonitorServiceAccountName": "managed-master-hibernation-monitor",
              "install": true,
              "masterServiceAccountName": "jenkins",
              "serviceAccountName": "cjoc"
            },
            "sidecarinjector": {
              "Enabled": false
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cloudbees/cloudbees-core-modern@sha256:2cf058c819b40512094f84f4d374457455370ec4994d34cee6e39b9d87841f05",
      "bundle_path_digest": "sha256:2cf058c819b40512094f84f4d374457455370ec4994d34cee6e39b9d87841f05",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-09T13:07:50.436000+00:00",
      "csv_description": "CloudBees CI",
      "csv_display_name": "CloudBees CI",
      "csv_metadata_description": "",
      "csv_name": "cloudbees-ci.v3.34.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:58:20.246000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "cloudbees-ci",
      "provided_apis": [
        {
          "group": "charts.cloudbees.com",
          "kind": "CloudBeesCI",
          "plural": "cloudbeescis",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:bd0bd72e9e7807b0fa307de8a93a56967221fe2dd181bac02383f0f9be9120e4",
          "image": "registry.connect.redhat.com/cloudbees/cloudbees-ci-operator@sha256:bd0bd72e9e7807b0fa307de8a93a56967221fe2dd181bac02383f0f9be9120e4",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "3.34.1",
      "version_original": "3.34.1"
    },
    {
      "_id": "6111bb9b0f5e559095d7e6a7",
      "alm_examples": [
        {
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {
              "priorityClassName": "system-node-critical",
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "nvidia.com/gpu",
                  "operator": "Exists"
                }
              ]
            },
            "dcgm": {
              "enabled": true,
              "hostPort": 5555,
              "image": "dcgm",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c"
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              },
              "env": [
                {
                  "name": "DCGM_EXPORTER_LISTEN",
                  "value": ":9400"
                },
                {
                  "name": "DCGM_EXPORTER_KUBERNETES",
                  "value": "true"
                },
                {
                  "name": "DCGM_EXPORTER_COLLECTORS",
                  "value": "/etc/dcgm-exporter/dcp-metrics-included.csv"
                }
              ],
              "image": "dcgm-exporter",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff"
            },
            "devicePlugin": {
              "args": [],
              "env": [
                {
                  "name": "PASS_DEVICE_SPECS",
                  "value": "true"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                },
                {
                  "name": "DEVICE_LIST_STRATEGY",
                  "value": "envvar"
                },
                {
                  "name": "DEVICE_ID_STRATEGY",
                  "value": "uuid"
                },
                {
                  "name": "NVIDIA_VISIBLE_DEVICES",
                  "value": "all"
                },
                {
                  "name": "NVIDIA_DRIVER_CAPABILITIES",
                  "value": "all"
                }
              ],
              "image": "k8s-device-plugin",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05"
            },
            "driver": {
              "enabled": true,
              "image": "driver",
              "imagePullSecrets": [],
              "licensingConfig": {
                "configMapName": ""
              },
              "manager": {
                "env": [
                  {
                    "name": "DRAIN_USE_FORCE",
                    "value": "false"
                  },
                  {
                    "name": "DRAIN_POD_SELECTOR_LABEL",
                    "value": ""
                  },
                  {
                    "name": "DRAIN_TIMEOUT_SECONDS",
                    "value": "0s"
                  },
                  {
                    "name": "DRAIN_DELETE_EMPTYDIR_DATA",
                    "value": "false"
                  }
                ],
                "image": "k8s-driver-manager",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia/cloud-native",
                "version": "sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b"
              },
              "nodeSelector": {
                "nvidia.com/gpu.deploy.driver": "true"
              },
              "podSecurityContext": {},
              "rdma": {
                "enabled": false
              },
              "repoConfig": {
                "configMapName": "",
                "destinationDir": ""
              },
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {
              "env": [
                {
                  "name": "GFD_SLEEP_INTERVAL",
                  "value": "60s"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                }
              ],
              "image": "gpu-feature-discovery",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f"
            },
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true,
              "env": [
                {
                  "name": "WITH_REBOOT",
                  "value": "false"
                }
              ],
              "image": "k8s-mig-manager",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec"
            },
            "nodeStatusExporter": {
              "enabled": true,
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585"
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {
                "image": "cuda",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia",
                "version": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce"
              }
            },
            "toolkit": {
              "enabled": true,
              "image": "container-toolkit",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "version": "sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703"
            },
            "validator": {
              "env": [
                {
                  "name": "WITH_WORKLOAD",
                  "value": "true"
                }
              ],
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:949e1d767985e99cde04a070667ba0b81e8422e71555c0c233c528fd746667d3",
      "bundle_path_digest": "sha256:949e1d767985e99cde04a070667ba0b81e8422e71555c0c233c528fd746667d3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.8",
      "creation_date": "2021-08-09T23:34:51.472000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/index.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:54:38.054000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:3a0ff79c4edc24de708f82e5f9dc2946897aa4a50e8827203221ee2281008974",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:3a0ff79c4edc24de708f82e5f9dc2946897aa4a50e8827203221ee2281008974",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
          "image": "nvcr.io/nvidia/driver@sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
          "name": "driver-image"
        },
        {
          "digest": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "image": "nvcr.io/nvidia/cuda@sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b",
          "name": "k8s-driver-manager-image"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.8.0",
      "version_original": "1.8.0"
    },
    {
      "_id": "6111bc38fbeeedc1ec2e6bf7",
      "alm_examples": [
        {
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {
              "priorityClassName": "system-node-critical",
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "nvidia.com/gpu",
                  "operator": "Exists"
                }
              ]
            },
            "dcgm": {
              "enabled": true,
              "hostPort": 5555,
              "image": "dcgm",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c"
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              },
              "env": [
                {
                  "name": "DCGM_EXPORTER_LISTEN",
                  "value": ":9400"
                },
                {
                  "name": "DCGM_EXPORTER_KUBERNETES",
                  "value": "true"
                },
                {
                  "name": "DCGM_EXPORTER_COLLECTORS",
                  "value": "/etc/dcgm-exporter/dcp-metrics-included.csv"
                }
              ],
              "image": "dcgm-exporter",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff"
            },
            "devicePlugin": {
              "args": [],
              "env": [
                {
                  "name": "PASS_DEVICE_SPECS",
                  "value": "true"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                },
                {
                  "name": "DEVICE_LIST_STRATEGY",
                  "value": "envvar"
                },
                {
                  "name": "DEVICE_ID_STRATEGY",
                  "value": "uuid"
                },
                {
                  "name": "NVIDIA_VISIBLE_DEVICES",
                  "value": "all"
                },
                {
                  "name": "NVIDIA_DRIVER_CAPABILITIES",
                  "value": "all"
                }
              ],
              "image": "k8s-device-plugin",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05"
            },
            "driver": {
              "enabled": true,
              "image": "driver",
              "imagePullSecrets": [],
              "licensingConfig": {
                "configMapName": ""
              },
              "manager": {
                "env": [
                  {
                    "name": "DRAIN_USE_FORCE",
                    "value": "false"
                  },
                  {
                    "name": "DRAIN_POD_SELECTOR_LABEL",
                    "value": ""
                  },
                  {
                    "name": "DRAIN_TIMEOUT_SECONDS",
                    "value": "0s"
                  },
                  {
                    "name": "DRAIN_DELETE_EMPTYDIR_DATA",
                    "value": "false"
                  }
                ],
                "image": "k8s-driver-manager",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia/cloud-native",
                "version": "sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b"
              },
              "nodeSelector": {
                "nvidia.com/gpu.deploy.driver": "true"
              },
              "podSecurityContext": {},
              "rdma": {
                "enabled": false
              },
              "repoConfig": {
                "configMapName": "",
                "destinationDir": ""
              },
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {
              "env": [
                {
                  "name": "GFD_SLEEP_INTERVAL",
                  "value": "60s"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                }
              ],
              "image": "gpu-feature-discovery",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f"
            },
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true,
              "env": [
                {
                  "name": "WITH_REBOOT",
                  "value": "false"
                }
              ],
              "image": "k8s-mig-manager",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec"
            },
            "nodeStatusExporter": {
              "enabled": true,
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585"
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {
                "image": "cuda",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia",
                "version": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce"
              }
            },
            "toolkit": {
              "enabled": true,
              "image": "container-toolkit",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "version": "sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703"
            },
            "validator": {
              "env": [
                {
                  "name": "WITH_WORKLOAD",
                  "value": "true"
                }
              ],
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:949e1d767985e99cde04a070667ba0b81e8422e71555c0c233c528fd746667d3",
      "bundle_path_digest": "sha256:949e1d767985e99cde04a070667ba0b81e8422e71555c0c233c528fd746667d3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.8",
      "creation_date": "2021-08-09T23:37:28.536000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/index.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:22:51.721000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:3a0ff79c4edc24de708f82e5f9dc2946897aa4a50e8827203221ee2281008974",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:3a0ff79c4edc24de708f82e5f9dc2946897aa4a50e8827203221ee2281008974",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
          "image": "nvcr.io/nvidia/driver@sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
          "name": "driver-image"
        },
        {
          "digest": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "image": "nvcr.io/nvidia/cuda@sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b",
          "name": "k8s-driver-manager-image"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.8.0",
      "version_original": "1.8.0"
    },
    {
      "_id": "6111bcbffbeeedc1ec2e6bf8",
      "alm_examples": [
        {
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {
              "priorityClassName": "system-node-critical",
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "nvidia.com/gpu",
                  "operator": "Exists"
                }
              ]
            },
            "dcgm": {
              "enabled": true,
              "hostPort": 5555,
              "image": "dcgm",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c"
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              },
              "env": [
                {
                  "name": "DCGM_EXPORTER_LISTEN",
                  "value": ":9400"
                },
                {
                  "name": "DCGM_EXPORTER_KUBERNETES",
                  "value": "true"
                },
                {
                  "name": "DCGM_EXPORTER_COLLECTORS",
                  "value": "/etc/dcgm-exporter/dcp-metrics-included.csv"
                }
              ],
              "image": "dcgm-exporter",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff"
            },
            "devicePlugin": {
              "args": [],
              "env": [
                {
                  "name": "PASS_DEVICE_SPECS",
                  "value": "true"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                },
                {
                  "name": "DEVICE_LIST_STRATEGY",
                  "value": "envvar"
                },
                {
                  "name": "DEVICE_ID_STRATEGY",
                  "value": "uuid"
                },
                {
                  "name": "NVIDIA_VISIBLE_DEVICES",
                  "value": "all"
                },
                {
                  "name": "NVIDIA_DRIVER_CAPABILITIES",
                  "value": "all"
                }
              ],
              "image": "k8s-device-plugin",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05"
            },
            "driver": {
              "enabled": true,
              "image": "driver",
              "imagePullSecrets": [],
              "licensingConfig": {
                "configMapName": ""
              },
              "manager": {
                "env": [
                  {
                    "name": "DRAIN_USE_FORCE",
                    "value": "false"
                  },
                  {
                    "name": "DRAIN_POD_SELECTOR_LABEL",
                    "value": ""
                  },
                  {
                    "name": "DRAIN_TIMEOUT_SECONDS",
                    "value": "0s"
                  },
                  {
                    "name": "DRAIN_DELETE_EMPTYDIR_DATA",
                    "value": "false"
                  }
                ],
                "image": "k8s-driver-manager",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia/cloud-native",
                "version": "sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b"
              },
              "nodeSelector": {
                "nvidia.com/gpu.deploy.driver": "true"
              },
              "podSecurityContext": {},
              "rdma": {
                "enabled": false
              },
              "repoConfig": {
                "configMapName": "",
                "destinationDir": ""
              },
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {
              "env": [
                {
                  "name": "GFD_SLEEP_INTERVAL",
                  "value": "60s"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                }
              ],
              "image": "gpu-feature-discovery",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f"
            },
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true,
              "env": [
                {
                  "name": "WITH_REBOOT",
                  "value": "false"
                }
              ],
              "image": "k8s-mig-manager",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec"
            },
            "nodeStatusExporter": {
              "enabled": true,
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585"
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {
                "image": "cuda",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia",
                "version": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce"
              }
            },
            "toolkit": {
              "enabled": true,
              "image": "container-toolkit",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "version": "sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703"
            },
            "validator": {
              "env": [
                {
                  "name": "WITH_WORKLOAD",
                  "value": "true"
                }
              ],
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:949e1d767985e99cde04a070667ba0b81e8422e71555c0c233c528fd746667d3",
      "bundle_path_digest": "sha256:949e1d767985e99cde04a070667ba0b81e8422e71555c0c233c528fd746667d3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.8",
      "creation_date": "2021-08-09T23:39:43.820000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/index.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-05T11:18:27.987000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:3a0ff79c4edc24de708f82e5f9dc2946897aa4a50e8827203221ee2281008974",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:3a0ff79c4edc24de708f82e5f9dc2946897aa4a50e8827203221ee2281008974",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
          "image": "nvcr.io/nvidia/driver@sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
          "name": "driver-image"
        },
        {
          "digest": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "image": "nvcr.io/nvidia/cuda@sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b",
          "name": "k8s-driver-manager-image"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.8.0",
      "version_original": "1.8.0"
    },
    {
      "_id": "6112ab71836d19274c41a73e",
      "alm_examples": [
        {
          "kind": "AquaCsp",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "db-password",
                "name": "aqua-database-password"
              },
              "dbDiskSize": 10
            },
            "database": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "gateway": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "infra": {
              "namespace": "aqua",
              "requirements": true,
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "route": true,
            "server": {
              "replicas": 1,
              "service": "LoadBalancer"
            }
          }
        },
        {
          "kind": "AquaDatabase",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "diskSize": 10,
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        },
        {
          "kind": "AquaEnforcer",
          "metadata": {
            "name": "aqua"
          },
          "spec": {
            "gateway": {
              "host": "aqua-gateway",
              "port": 8443
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "token": "<<your-token>>"
          }
        },
        {
          "kind": "AquaKubeEnforcer",
          "metadata": {
            "name": "aqua"
          },
          "spec": {
            "config": {
              "cluster_name": "aqua-secure",
              "gateway_address": "aqua-gateway.aqua:8443",
              "imagePullSecret": "aqua-registry"
            },
            "deploy": {
              "service": "ClusterIP"
            },
            "infra": {
              "serviceAccount": "aqua-kube-enforcer-sa",
              "version": "6.0"
            },
            "token": "<<KUBE_ENFORCER_GROUP_TOKEN>>"
          }
        },
        {
          "kind": "AquaGateway",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "<<EXTERNAL DB PASSWORD SECRET KEY>>",
                "name": "<<EXTERNAL DB PASSWORD SECRET NAME>>"
              },
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "externalDb": {
              "host": "<<EXTERNAL DB IP OR DNS>>",
              "password": "<<EXTERNAL DB PASSWORD (if secret does not exist)>>",
              "port": "<<EXTERNAL DB PORT>>",
              "username": "<<EXTERNAL DB USERNAME>>"
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        },
        {
          "kind": "AquaScanner",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "deploy": {
              "replicas": 1
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "login": {
              "host": "http://aqua-server:8080",
              "password": "<<YOUR AQUA USER PASSWORD>>",
              "username": "<<YOUR AQUA USER NAME>>"
            }
          }
        },
        {
          "kind": "AquaServer",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "<<EXTERNAL DB PASSWORD SECRET KEY>>",
                "name": "<<EXTERNAL DB PASSWORD SECRET NAME>>"
              },
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "LoadBalancer"
            },
            "externalDb": {
              "host": "<<EXTERNAL DB IP OR DNS>>",
              "password": "<<EXTERNAL DB PASSWORD (if secret does not exist)>>",
              "port": "<<EXTERNAL DB PORT>>",
              "username": "<<EXTERNAL DB USERNAME>>"
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/aquasec/aquasec-operator-bundle@sha256:26b6039d957e1dd8de17b1f46bde410c797159c581a217d5e396b39ccfd304cf",
      "bundle_path_digest": "sha256:26b6039d957e1dd8de17b1f46bde410c797159c581a217d5e396b39ccfd304cf",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "6.0",
      "creation_date": "2021-08-10T16:38:09.879000+00:00",
      "csv_description": "The Aqua Security Operator runs within an OpenShift cluster and provides a means to deploy and manage the Aqua Security cluster and components :\n* Server (aka \u201cconsole\u201d)\n* Database (for production environments we recommend to use an external database and not the Aqua default database)\n* Gateway\n* Enforcer (aka \u201cagent\u201d)\n* KubeEnforcer\n* Scanner\n* CSP (package that contains the Server, Database, and Gateway)\n\nUse the Aqua-Operator to\n* Deploy Aqua Security components on OpenShift\n* Scale up Aqua Security components with extra replicas\n* Assign metadata tags to Aqua Security components\n* Automatically scale the number of Aqua scanners based on the number of images in the scan queue\n\nThe Aqua Operator provides a few [Custom Resources](https://github.com/aquasecurity/aqua-operator/tree/6.0.0/deploy/crds) for managing the Aqua CSP platform.\n\n## Prerequisites\n\nThere are only 3 prerequisites:\n1. Make sure you have a license and to obtain one, please contact Aqua Security at [Contact Us](mailto:cloudsales@aquasec.com).\n2. Create a new project for aqua\n\n    ```oc new-project aqua```\n\n3. Create the secret for Aqua Database password. If you are using an External Database, make sure you use those credentials.\nNOTE: This step is optional and you can specify the Database password when creating the CRs.\n\n    ```oc create secret generic aqua-database-password --from-literal=db-password=<password> -n aqua```\n\nPlease note that for the certified operator, the Docker registry secret is NOT needed. For the Red Hat certified operator, the Aqua application images are stored in the Red Hat Connect registry and will be automatically pulled by the Aqua operator.\n\n## Choosing the right channel\n\nChannels are a flexible publishing mechanism that offers you three different Aqua versions to deploy, at any point. These channels map to their respective Aqua versions, describing the maturity based on where they are in their lifecycle:\n* 4.6: This maps to the older version of Aqua 4.6.0\n* 5.0: This maps to the Aqua version 5.0.0\n* 5.3: This maps to the Aqua version 5.3.0\n* 6.0: This maps to the Aqua version 6.0.0\n* 6.2: This is the latest and greatest version of Aqua (default)\n\n## Deploying the Aqua Operator\n\nAqua Operator follows an easy push button deployment that installs the operator in its own aqua namespace.\nPlease refer to the instructions [here](https://github.com/aquasecurity/aqua-operator/blob/6.0.0/docs/DeployOpenShiftOperator.md).\n\n## Configuring the Aqua Operator\n\nOperators leverage CRDs as a configuration mechanism for the application. Aqua offers a number of CRDs that can be tweaked and configured as per your needs. Please refer to this [link](https://github.com/aquasecurity/aqua-operator/blob/6.0.0/docs/DeployOpenShiftOperator.md#aquacsp-crds) for Custom Resource examples for popular scenarios.\n\n## Support\n\nFor support please contact support@aquasec.com.",
      "csv_display_name": "Aqua Security Operator",
      "csv_metadata_description": "The Aqua Security Operator runs within a Openshift cluster and provides a means to deploy and manage Aqua Security cluster and components.",
      "csv_name": "aqua-operator.v6.0.8",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-04T14:56:56.956000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "aqua-operator-certified",
      "provided_apis": [
        {
          "group": "operator.aquasec.com",
          "kind": "AquaServer",
          "plural": "aquaservers",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaCsp",
          "plural": "aquacsps",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaDatabase",
          "plural": "aquadatabases",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaEnforcer",
          "plural": "aquaenforcers",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaGateway",
          "plural": "aquagateways",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaKubeEnforcer",
          "plural": "aquakubeenforcers",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaScanner",
          "plural": "aquascanners",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skip_range": ">=1.0.2 <6.0.8",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "6.0.8",
      "version_original": "6.0.8"
    },
    {
      "_id": "6112acce6e1e42ca4d6de2a6",
      "alm_examples": [
        {
          "kind": "AquaCsp",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "db-password",
                "name": "aqua-database-password"
              },
              "dbDiskSize": 10
            },
            "database": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "gateway": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "infra": {
              "namespace": "aqua",
              "requirements": true,
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "route": true,
            "server": {
              "replicas": 1,
              "service": "LoadBalancer"
            }
          }
        },
        {
          "kind": "AquaDatabase",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "diskSize": 10,
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        },
        {
          "kind": "AquaEnforcer",
          "metadata": {
            "name": "aqua"
          },
          "spec": {
            "gateway": {
              "host": "aqua-gateway",
              "port": 8443
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "token": "<<your-token>>"
          }
        },
        {
          "kind": "AquaKubeEnforcer",
          "metadata": {
            "name": "aqua"
          },
          "spec": {
            "config": {
              "cluster_name": "aqua-secure",
              "gateway_address": "aqua-gateway.aqua:8443",
              "imagePullSecret": "aqua-registry"
            },
            "deploy": {
              "service": "ClusterIP"
            },
            "infra": {
              "serviceAccount": "aqua-kube-enforcer-sa",
              "version": "6.0"
            },
            "token": "<<KUBE_ENFORCER_GROUP_TOKEN>>"
          }
        },
        {
          "kind": "AquaGateway",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "<<EXTERNAL DB PASSWORD SECRET KEY>>",
                "name": "<<EXTERNAL DB PASSWORD SECRET NAME>>"
              },
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "externalDb": {
              "host": "<<EXTERNAL DB IP OR DNS>>",
              "password": "<<EXTERNAL DB PASSWORD (if secret does not exist)>>",
              "port": "<<EXTERNAL DB PORT>>",
              "username": "<<EXTERNAL DB USERNAME>>"
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        },
        {
          "kind": "AquaScanner",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "deploy": {
              "replicas": 1
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "login": {
              "host": "http://aqua-server:8080",
              "password": "<<YOUR AQUA USER PASSWORD>>",
              "username": "<<YOUR AQUA USER NAME>>"
            }
          }
        },
        {
          "kind": "AquaServer",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "<<EXTERNAL DB PASSWORD SECRET KEY>>",
                "name": "<<EXTERNAL DB PASSWORD SECRET NAME>>"
              },
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "LoadBalancer"
            },
            "externalDb": {
              "host": "<<EXTERNAL DB IP OR DNS>>",
              "password": "<<EXTERNAL DB PASSWORD (if secret does not exist)>>",
              "port": "<<EXTERNAL DB PORT>>",
              "username": "<<EXTERNAL DB USERNAME>>"
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/aquasec/aquasec-operator-bundle@sha256:26b6039d957e1dd8de17b1f46bde410c797159c581a217d5e396b39ccfd304cf",
      "bundle_path_digest": "sha256:26b6039d957e1dd8de17b1f46bde410c797159c581a217d5e396b39ccfd304cf",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "6.0",
      "creation_date": "2021-08-10T16:43:58.465000+00:00",
      "csv_description": "The Aqua Security Operator runs within an OpenShift cluster and provides a means to deploy and manage the Aqua Security cluster and components :\n* Server (aka \u201cconsole\u201d)\n* Database (for production environments we recommend to use an external database and not the Aqua default database)\n* Gateway\n* Enforcer (aka \u201cagent\u201d)\n* KubeEnforcer\n* Scanner\n* CSP (package that contains the Server, Database, and Gateway)\n\nUse the Aqua-Operator to\n* Deploy Aqua Security components on OpenShift\n* Scale up Aqua Security components with extra replicas\n* Assign metadata tags to Aqua Security components\n* Automatically scale the number of Aqua scanners based on the number of images in the scan queue\n\nThe Aqua Operator provides a few [Custom Resources](https://github.com/aquasecurity/aqua-operator/tree/6.0.0/deploy/crds) for managing the Aqua CSP platform.\n\n## Prerequisites\n\nThere are only 3 prerequisites:\n1. Make sure you have a license and to obtain one, please contact Aqua Security at [Contact Us](mailto:cloudsales@aquasec.com).\n2. Create a new project for aqua\n\n    ```oc new-project aqua```\n\n3. Create the secret for Aqua Database password. If you are using an External Database, make sure you use those credentials.\nNOTE: This step is optional and you can specify the Database password when creating the CRs.\n\n    ```oc create secret generic aqua-database-password --from-literal=db-password=<password> -n aqua```\n\nPlease note that for the certified operator, the Docker registry secret is NOT needed. For the Red Hat certified operator, the Aqua application images are stored in the Red Hat Connect registry and will be automatically pulled by the Aqua operator.\n\n## Choosing the right channel\n\nChannels are a flexible publishing mechanism that offers you three different Aqua versions to deploy, at any point. These channels map to their respective Aqua versions, describing the maturity based on where they are in their lifecycle:\n* 4.6: This maps to the older version of Aqua 4.6.0\n* 5.0: This maps to the Aqua version 5.0.0\n* 5.3: This maps to the Aqua version 5.3.0\n* 6.0: This maps to the Aqua version 6.0.0\n* 6.2: This is the latest and greatest version of Aqua (default)\n\n## Deploying the Aqua Operator\n\nAqua Operator follows an easy push button deployment that installs the operator in its own aqua namespace.\nPlease refer to the instructions [here](https://github.com/aquasecurity/aqua-operator/blob/6.0.0/docs/DeployOpenShiftOperator.md).\n\n## Configuring the Aqua Operator\n\nOperators leverage CRDs as a configuration mechanism for the application. Aqua offers a number of CRDs that can be tweaked and configured as per your needs. Please refer to this [link](https://github.com/aquasecurity/aqua-operator/blob/6.0.0/docs/DeployOpenShiftOperator.md#aquacsp-crds) for Custom Resource examples for popular scenarios.\n\n## Support\n\nFor support please contact support@aquasec.com.",
      "csv_display_name": "Aqua Security Operator",
      "csv_metadata_description": "The Aqua Security Operator runs within a Openshift cluster and provides a means to deploy and manage Aqua Security cluster and components.",
      "csv_name": "aqua-operator.v6.0.8",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-04T14:37:38.058000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "aqua-operator-certified",
      "provided_apis": [],
      "related_images": [],
      "skip_range": ">=1.0.2 <6.0.8",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "6.0.8",
      "version_original": "6.0.8"
    },
    {
      "_id": "6112ada0828b767a4f5f9352",
      "alm_examples": [
        {
          "kind": "AquaCsp",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "db-password",
                "name": "aqua-database-password"
              },
              "dbDiskSize": 10
            },
            "database": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "gateway": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "infra": {
              "namespace": "aqua",
              "requirements": true,
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "route": true,
            "server": {
              "replicas": 1,
              "service": "LoadBalancer"
            }
          }
        },
        {
          "kind": "AquaDatabase",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "diskSize": 10,
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        },
        {
          "kind": "AquaEnforcer",
          "metadata": {
            "name": "aqua"
          },
          "spec": {
            "gateway": {
              "host": "aqua-gateway",
              "port": 8443
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "token": "<<your-token>>"
          }
        },
        {
          "kind": "AquaKubeEnforcer",
          "metadata": {
            "name": "aqua"
          },
          "spec": {
            "config": {
              "cluster_name": "aqua-secure",
              "gateway_address": "aqua-gateway.aqua:8443",
              "imagePullSecret": "aqua-registry"
            },
            "deploy": {
              "service": "ClusterIP"
            },
            "infra": {
              "serviceAccount": "aqua-kube-enforcer-sa",
              "version": "6.0"
            },
            "token": "<<KUBE_ENFORCER_GROUP_TOKEN>>"
          }
        },
        {
          "kind": "AquaGateway",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "<<EXTERNAL DB PASSWORD SECRET KEY>>",
                "name": "<<EXTERNAL DB PASSWORD SECRET NAME>>"
              },
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "ClusterIP"
            },
            "externalDb": {
              "host": "<<EXTERNAL DB IP OR DNS>>",
              "password": "<<EXTERNAL DB PASSWORD (if secret does not exist)>>",
              "port": "<<EXTERNAL DB PORT>>",
              "username": "<<EXTERNAL DB USERNAME>>"
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        },
        {
          "kind": "AquaScanner",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "deploy": {
              "replicas": 1
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            },
            "login": {
              "host": "http://aqua-server:8080",
              "password": "<<YOUR AQUA USER PASSWORD>>",
              "username": "<<YOUR AQUA USER NAME>>"
            }
          }
        },
        {
          "kind": "AquaServer",
          "metadata": {
            "name": "aqua",
            "namespace": "aqua"
          },
          "spec": {
            "common": {
              "databaseSecret": {
                "key": "<<EXTERNAL DB PASSWORD SECRET KEY>>",
                "name": "<<EXTERNAL DB PASSWORD SECRET NAME>>"
              },
              "splitDB": false
            },
            "deploy": {
              "replicas": 1,
              "service": "LoadBalancer"
            },
            "externalDb": {
              "host": "<<EXTERNAL DB IP OR DNS>>",
              "password": "<<EXTERNAL DB PASSWORD (if secret does not exist)>>",
              "port": "<<EXTERNAL DB PORT>>",
              "username": "<<EXTERNAL DB USERNAME>>"
            },
            "infra": {
              "serviceAccount": "aqua-sa",
              "version": "6.0"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/aquasec/aquasec-operator-bundle@sha256:26b6039d957e1dd8de17b1f46bde410c797159c581a217d5e396b39ccfd304cf",
      "bundle_path_digest": "sha256:26b6039d957e1dd8de17b1f46bde410c797159c581a217d5e396b39ccfd304cf",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "6.0",
      "creation_date": "2021-08-10T16:47:28.558000+00:00",
      "csv_description": "The Aqua Security Operator runs within an OpenShift cluster and provides a means to deploy and manage the Aqua Security cluster and components :\n* Server (aka \u201cconsole\u201d)\n* Database (for production environments we recommend to use an external database and not the Aqua default database)\n* Gateway\n* Enforcer (aka \u201cagent\u201d)\n* KubeEnforcer\n* Scanner\n* CSP (package that contains the Server, Database, and Gateway)\n\nUse the Aqua-Operator to\n* Deploy Aqua Security components on OpenShift\n* Scale up Aqua Security components with extra replicas\n* Assign metadata tags to Aqua Security components\n* Automatically scale the number of Aqua scanners based on the number of images in the scan queue\n\nThe Aqua Operator provides a few [Custom Resources](https://github.com/aquasecurity/aqua-operator/tree/6.0.0/deploy/crds) for managing the Aqua CSP platform.\n\n## Prerequisites\n\nThere are only 3 prerequisites:\n1. Make sure you have a license and to obtain one, please contact Aqua Security at [Contact Us](mailto:cloudsales@aquasec.com).\n2. Create a new project for aqua\n\n    ```oc new-project aqua```\n\n3. Create the secret for Aqua Database password. If you are using an External Database, make sure you use those credentials.\nNOTE: This step is optional and you can specify the Database password when creating the CRs.\n\n    ```oc create secret generic aqua-database-password --from-literal=db-password=<password> -n aqua```\n\nPlease note that for the certified operator, the Docker registry secret is NOT needed. For the Red Hat certified operator, the Aqua application images are stored in the Red Hat Connect registry and will be automatically pulled by the Aqua operator.\n\n## Choosing the right channel\n\nChannels are a flexible publishing mechanism that offers you three different Aqua versions to deploy, at any point. These channels map to their respective Aqua versions, describing the maturity based on where they are in their lifecycle:\n* 4.6: This maps to the older version of Aqua 4.6.0\n* 5.0: This maps to the Aqua version 5.0.0\n* 5.3: This maps to the Aqua version 5.3.0\n* 6.0: This maps to the Aqua version 6.0.0\n* 6.2: This is the latest and greatest version of Aqua (default)\n\n## Deploying the Aqua Operator\n\nAqua Operator follows an easy push button deployment that installs the operator in its own aqua namespace.\nPlease refer to the instructions [here](https://github.com/aquasecurity/aqua-operator/blob/6.0.0/docs/DeployOpenShiftOperator.md).\n\n## Configuring the Aqua Operator\n\nOperators leverage CRDs as a configuration mechanism for the application. Aqua offers a number of CRDs that can be tweaked and configured as per your needs. Please refer to this [link](https://github.com/aquasecurity/aqua-operator/blob/6.0.0/docs/DeployOpenShiftOperator.md#aquacsp-crds) for Custom Resource examples for popular scenarios.\n\n## Support\n\nFor support please contact support@aquasec.com.",
      "csv_display_name": "Aqua Security Operator",
      "csv_metadata_description": "The Aqua Security Operator runs within a Openshift cluster and provides a means to deploy and manage Aqua Security cluster and components.",
      "csv_name": "aqua-operator.v6.0.8",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-05T11:14:55.822000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "aqua-operator-certified",
      "provided_apis": [
        {
          "group": "operator.aquasec.com",
          "kind": "AquaDatabase",
          "plural": "aquadatabases",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaEnforcer",
          "plural": "aquaenforcers",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaGateway",
          "plural": "aquagateways",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaKubeEnforcer",
          "plural": "aquakubeenforcers",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaScanner",
          "plural": "aquascanners",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaServer",
          "plural": "aquaservers",
          "version": "v1alpha1"
        },
        {
          "group": "operator.aquasec.com",
          "kind": "AquaCsp",
          "plural": "aquacsps",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skip_range": ">=1.0.2 <6.0.8",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "6.0.8",
      "version_original": "6.0.8"
    },
    {
      "_id": "611405c5828b767a4f5f98df",
      "alm_examples": [
        {
          "kind": "Backup",
          "metadata": {
            "name": "backup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            }
          }
        },
        {
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample"
          },
          "spec": {
            "instances": 3,
            "primaryUpdateStrategy": "unsupervised",
            "storage": {
              "size": "1Gi"
            }
          }
        },
        {
          "kind": "ScheduledBackup",
          "metadata": {
            "name": "scheduledbackup-sample"
          },
          "spec": {
            "cluster": {
              "name": "cluster-sample"
            },
            "schedule": "0 0 0 * * *"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/enterprisedb/cloud-native-postgresql@sha256:e56533516e64f88632df8ac576516c15ecc235614d317e9e7d62e558abeca3f8",
      "bundle_path_digest": "sha256:e56533516e64f88632df8ac576516c15ecc235614d317e9e7d62e558abeca3f8",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-11T17:15:49.470000+00:00",
      "csv_description": "Cloud Native PostgreSQL is an operator designed by EnterpriseDB\nto manage PostgreSQL workloads on any supported Kubernetes\ncluster running in private, public, or hybrid cloud environments,\nincluding OpenShift.\nCloud Native PostgreSQL adheres to DevOps principles and concepts\nsuch as declarative configuration and immutable infrastructure.\n\nIt defines a new Kubernetes resource called \"Cluster\" representing a PostgreSQL\ncluster made up of a single primary and an optional number of replicas that co-exist\nin a chosen Kubernetes namespace for High Availability and offloading of\nread-only queries.\n\nApplications that reside in the same Kubernetes cluster can access the\nPostgreSQL database using a service which is solely managed by the operator,\nwithout having to worry about changes of the primary role following a failover\nor a switchover. Applications that reside outside the Kubernetes cluster need\nto configure an Ingress object to expose the service via TCP.\n\nCloud Native PostgreSQL works with PostgreSQL (13, 12, 11 and 10) and\nEDB Postgres Advanced (13, 12, 11 and 10), and it is available under\nthe EnterpriseDB Limited Use License.\nYou can evaluate Cloud Native PostgreSQL for free.\nYou need a valid license key to use Cloud Native PostgreSQL in production.\n\n# Main features\n\n* Direct integration with Kubernetes API server for High Availability, without requiring an external tool\n* Self-Healing capability, through:\n    * failover of the primary instance by promoting the most aligned replica\n    * automated recreation of a replica\n* Planned switchover of the primary instance by promoting a selected replica\n* Scale up/down capabilities\n* Definition of an arbitrary number of instances (minimum 1 - one primary server)\n* Definition of the *read-write* service, to connect your applications to the only primary server of the cluster\n* Definition of the *read-only* service, to connect your applications to any of the instances for reading workloads\n* Support for Local Persistent Volumes with PVC templates\n* Reuse of Persistent Volumes storage in Pods\n* Rolling updates for PostgreSQL minor versions and operator upgrades\n* TLS connections and client certificate authentication\n* Support for custom TLS certificates (including integration with cert-manager)\n* Continuous backup to an S3 compatible object store\n* Full recovery and Point-In-Time recovery from an S3 compatible object store backup\n* Support for Synchronous Replicas\n* Support for node affinity via `nodeSelector`\n* Native customizable exporter of user defined metrics for Prometheus through the `metrics` port (9187)\n* Standard output logging of PostgreSQL error messages in JSON format\n* Support for the `restricted` security context constraint (SCC) in Red Hat OpenShift\n* `cnp` plugin for `kubectl`\n",
      "csv_display_name": "Cloud Native PostgreSQL",
      "csv_metadata_description": "Operator to manage Postgres high availability clusters with a primary/standby architecture.",
      "csv_name": "cloud-native-postgresql.v1.7.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:42:04.029000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "cloud-native-postgresql",
      "provided_apis": [],
      "related_images": [
        {
          "digest": "sha256:e13f29706203df835bee73553183116032498b6562802466965878b43f068ae6",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:e13f29706203df835bee73553183116032498b6562802466965878b43f068ae6",
          "name": "cloud-native-postgresql-e13f29706203df835bee73553183116032498b6562802466965878b43f068ae6-annotation"
        },
        {
          "digest": "sha256:e13f29706203df835bee73553183116032498b6562802466965878b43f068ae6",
          "image": "quay.io/enterprisedb/cloud-native-postgresql@sha256:e13f29706203df835bee73553183116032498b6562802466965878b43f068ae6",
          "name": "manager"
        }
      ],
      "skip_range": ">=0.6.0 <1.7.1",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.7.1",
      "version_original": "1.7.1"
    },
    {
      "_id": "611436056e1e42ca4d6ded6c",
      "alm_examples": [
        {
          "kind": "AnzoGraph",
          "metadata": {
            "name": "azg01"
          },
          "spec": {
            "db": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_data": "anzograph-data-grpc",
                      "app_mgmt": "anzograph-mgmt-grpc"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_data": "anzograph-data-grpc",
                        "app_mgmt": "anzograph-mgmt-grpc"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
                          "name": "db",
                          "resources": {
                            "limits": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            },
                            "requests": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            },
            "deployFrontend": false,
            "frontend": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_mgmt": "anzograph-frontend"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_mgmt": "anzograph-frontend"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
                          "name": "frontend",
                          "resources": {
                            "limits": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator-bundle@sha256:9418a3505873ac387f2b5455ec7e696739a2bb6f03232f8c0f59489c605b96f2",
      "bundle_path_digest": "sha256:9418a3505873ac387f2b5455ec7e696739a2bb6f03232f8c0f59489c605b96f2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-11T20:41:41.515000+00:00",
      "csv_description": "The AnzoGraph Operator provides the way to install and configure an AnzoGraph\ncluster on Red Hat K8S environment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzograph/blob/v2.0.1/README_openshift_marketplace.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzograph/userdoc/ )\n\n### Support\n\nWe offer Support to our customers with the AnzoGraph db Enterprise Edition License [here]( https://customercenter.cambridgesemantics.com/ ). For AnzoGraph db Free Edition questions, get help from our Anzograph User Community at Stack Overflow. When submitting a question, include the tag 'anzograph'.",
      "csv_display_name": "AnzoGraph Operator",
      "csv_metadata_description": "kubernetes operator for AnzoGraph DB",
      "csv_name": "anzograph-operator.v2.0.102",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:55:19.528000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "anzograph-operator",
      "provided_apis": [],
      "related_images": [
        {
          "digest": "sha256:a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732",
          "name": "anzograph-operator-a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732-annotation"
        },
        {
          "digest": "sha256:a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732",
          "name": "manager"
        },
        {
          "digest": "sha256:e3bdddf811b5ba69f388d023aea1ba538edd00e9f415c33ec555321d39129a36",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph@sha256:e3bdddf811b5ba69f388d023aea1ba538edd00e9f415c33ec555321d39129a36",
          "name": "anzograph_allinone"
        },
        {
          "digest": "sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
          "name": "anzograph_db"
        },
        {
          "digest": "sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
          "name": "anzograph_frontend"
        },
        {
          "digest": "sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
          "name": "anzograph-frontend-c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3-annotation"
        },
        {
          "digest": "sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
          "name": "anzograph-db-8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "2.0.1",
      "version_original": "2.0.1"
    },
    {
      "_id": "6114e5df836d19274c41b054",
      "alm_examples": [
        {
          "kind": "AnchoreEngine",
          "metadata": {
            "name": "anchoreengine-sample"
          },
          "spec": {
            "anchoreGlobal": {
              "defaultAdminEmail": "example@email.com"
            },
            "postgresql": {
              "extraEnv": [
                {
                  "name": "POSTGRESQL_USER",
                  "value": "anchoreengine"
                },
                {
                  "name": "POSTGRESQL_PASSWORD",
                  "value": "anchore-postgres,123"
                },
                {
                  "name": "POSTGRESQL_DATABASE",
                  "value": "anchore"
                },
                {
                  "name": "PGUSER",
                  "value": "postgres"
                }
              ],
              "postgresDatabase": "anchore",
              "postgresPassword": "anchore-postgres,123",
              "postgresUser": "anchoreengine"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/anchore/engine-operator-bundle@sha256:aac8eefb589ceafced25b345b5f4d5ae6dbb7c2952ffb1985227ebee9ace1714",
      "bundle_path_digest": "sha256:aac8eefb589ceafced25b345b5f4d5ae6dbb7c2952ffb1985227ebee9ace1714",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-12T09:11:59.453000+00:00",
      "csv_description": "Anchore Engine is an open source software system that provides a centralized service for analyzing container images, \n scanning for security vulnerabilities, and enforcing deployment policies. Anchore Engine is provided as a Docker \n container image that can be run standalone or with an orchestration platform such as Kubernetes. Anchore Engine allows \n users to perform detailed analysis on their container images, run queries, produce reports, and define policies that \n can be used in CI/CD pipelines. Users can extend Anchore Engine with plugins that add new queries, image analysis, and \n policies. Anchore Engine can be accessed directly through a RESTful API or via the Anchore CLI.\n\n The Anchore Engine Operator is based on the official stable [Helm Chart](https://github.com/anchore/anchore-charts/tree/master/stable/anchore-engine).",
      "csv_display_name": "Anchore Engine Operator",
      "csv_metadata_description": "Anchore Engine - container image scanning service for policy-based security, best-practice and compliance enforcement.",
      "csv_name": "anchore-engine.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:29:20.676000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "anchore-engine",
      "provided_apis": [
        {
          "group": "charts.anchore.io",
          "kind": "AnchoreEngine",
          "plural": "anchoreengines",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:57bd6e4d776b0bc26c32201ecbbe256d91ad40348c52878e52a1d33a15d35dfb",
          "image": "registry.connect.redhat.com/anchore/engine-operator@sha256:57bd6e4d776b0bc26c32201ecbbe256d91ad40348c52878e52a1d33a15d35dfb",
          "name": "engine-operator-57bd6e4d776b0bc26c32201ecbbe256d91ad40348c52878e52a1d33a15d35dfb-annotation"
        },
        {
          "digest": "sha256:57bd6e4d776b0bc26c32201ecbbe256d91ad40348c52878e52a1d33a15d35dfb",
          "image": "registry.connect.redhat.com/anchore/engine-operator@sha256:57bd6e4d776b0bc26c32201ecbbe256d91ad40348c52878e52a1d33a15d35dfb",
          "name": "manager"
        },
        {
          "digest": "sha256:bde9eedf639d70ad1ff5f907ee530adfa8e771f8e5909fd75ec4088250a956b3",
          "image": "registry.connect.redhat.com/anchore/engine0@sha256:bde9eedf639d70ad1ff5f907ee530adfa8e771f8e5909fd75ec4088250a956b3",
          "name": "anchore_engine"
        },
        {
          "digest": "sha256:ed53ca7b191432f7cf9da0fd8629d7de14ade609ca5f38aba443716f83616f2e",
          "image": "registry.redhat.io/rhel8/postgresql-96@sha256:ed53ca7b191432f7cf9da0fd8629d7de14ade609ca5f38aba443716f83616f2e",
          "name": "anchore_postgresql"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "611eca7cbd674341b5c5f0ce",
      "alm_examples": [
        {
          "kind": "Elasticsearch",
          "metadata": {
            "name": "elasticsearch-sample"
          },
          "spec": {
            "nodeSets": [
              {
                "config": {
                  "node.attr.attr_name": "attr_value",
                  "node.roles": [
                    "master",
                    "data"
                  ],
                  "node.store.allow_mmap": false
                },
                "count": 3,
                "name": "default",
                "podTemplate": {
                  "metadata": {
                    "labels": {
                      "foo": "bar"
                    }
                  },
                  "spec": {
                    "containers": [
                      {
                        "name": "elasticsearch",
                        "resources": {
                          "limits": {
                            "cpu": 2,
                            "memory": "4Gi"
                          },
                          "requests": {
                            "cpu": 1,
                            "memory": "4Gi"
                          }
                        }
                      }
                    ]
                  }
                }
              }
            ],
            "version": "7.12.1"
          }
        },
        {
          "kind": "Kibana",
          "metadata": {
            "name": "kibana-sample"
          },
          "spec": {
            "count": 1,
            "elasticsearchRef": {
              "name": "elasticsearch-sample"
            },
            "podTemplate": {
              "metadata": {
                "labels": {
                  "foo": "bar"
                }
              },
              "spec": {
                "containers": [
                  {
                    "name": "kibana",
                    "resources": {
                      "limits": {
                        "cpu": 2,
                        "memory": "2Gi"
                      },
                      "requests": {
                        "cpu": 0.5,
                        "memory": "1Gi"
                      }
                    }
                  }
                ]
              }
            },
            "version": "7.12.1"
          }
        },
        {
          "kind": "ApmServer",
          "metadata": {
            "name": "apmserver-sample"
          },
          "spec": {
            "count": 1,
            "elasticsearchRef": {
              "name": "elasticsearch-sample"
            },
            "version": "7.12.1"
          }
        },
        {
          "kind": "EnterpriseSearch",
          "metadata": {
            "name": "ent-sample"
          },
          "spec": {
            "config": {
              "ent_search.external_url": "https://localhost:3002"
            },
            "count": 1,
            "elasticsearchRef": {
              "name": "elasticsearch-sample"
            },
            "version": "7.12.1"
          }
        },
        {
          "kind": "Beat",
          "metadata": {
            "name": "heartbeat-sample"
          },
          "spec": {
            "config": {
              "heartbeat.monitors": [
                {
                  "hosts": [
                    "elasticsearch-sample-es-http.default.svc:9200"
                  ],
                  "schedule": "@every 5s",
                  "type": "tcp"
                }
              ]
            },
            "deployment": {
              "podTemplate": {
                "spec": {
                  "securityContext": {
                    "runAsUser": 0
                  }
                }
              },
              "replicas": 1
            },
            "elasticsearchRef": {
              "name": "elasticsearch-sample"
            },
            "type": "heartbeat",
            "version": "7.12.1"
          }
        },
        {
          "kind": "Agent",
          "metadata": {
            "name": "agent-sample"
          },
          "spec": {
            "config": {
              "inputs": [
                {
                  "data_stream": {
                    "namespace": "default"
                  },
                  "meta": {
                    "package": {
                      "name": "system",
                      "version": "0.9.1"
                    }
                  },
                  "name": "system-1",
                  "revision": 1,
                  "streams": [
                    {
                      "cpu.metrics": [
                        "percentages",
                        "normalized_percentages"
                      ],
                      "data_stream": {
                        "dataset": "system.cpu",
                        "type": "metrics"
                      },
                      "id": "system/metrics-system.cpu",
                      "metricsets": [
                        "cpu"
                      ],
                      "period": "10s"
                    }
                  ],
                  "type": "system/metrics",
                  "use_output": "default"
                }
              ]
            },
            "daemonSet": {},
            "elasticsearchRefs": [
              {
                "name": "elasticsearch-sample"
              }
            ],
            "version": "7.12.1"
          }
        },
        {
          "kind": "ElasticMapsServer",
          "metadata": {
            "name": "ems-sample"
          },
          "spec": {
            "count": 1,
            "elasticsearchRef": {
              "name": "elasticsearch-sample"
            },
            "version": "7.12.1"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/elastic/eck@sha256:30333a951cdac81f368f047aad9d8b6b7261bd1cc8a8f7434c69d2ff103ecb1d",
      "bundle_path_digest": "sha256:30333a951cdac81f368f047aad9d8b6b7261bd1cc8a8f7434c69d2ff103ecb1d",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-19T21:17:48.389000+00:00",
      "csv_description": "Elastic Cloud on Kubernetes (ECK) is the official operator by Elastic for automating the deployment, provisioning, management, and orchestration of Elasticsearch, Kibana, APM Server, Beats, Enterprise Search, Beats, Elastic Agent, and Elastic Maps Server on Kubernetes.\n\nCurrent features:\n\n*  Elasticsearch, Kibana, APM Server, Enterprise Search, Beats, Elastic Agent and Elastic Maps Server deployments\n*  TLS Certificates management\n*  Safe Elasticsearch cluster configuration and topology changes\n*  Persistent volumes usage\n*  Custom node configuration and attributes\n*  Secure settings keystore updates\n\nSupported versions:\n\n* Kubernetes: 1.12+ or OpenShift 3.11+\n* Elasticsearch: 6.8+, 7.1+\n\nSee the [Quickstart](https://www.elastic.co/guide/en/cloud-on-k8s/1.6/k8s-quickstart.html) to get started with ECK.",
      "csv_display_name": "Elasticsearch (ECK) Operator",
      "csv_metadata_description": "Run Elasticsearch, Kibana, APM Server, Enterprise Search, Beats, Elastic Agent and Elastic Maps Server on Kubernetes and OpenShift",
      "csv_name": "elasticsearch-eck-operator-certified.v1.6.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:42:35.941000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "elasticsearch-eck-operator-certified",
      "provided_apis": [
        {
          "group": "apm.k8s.elastic.co",
          "kind": "ApmServer",
          "plural": "apmservers",
          "version": "v1alpha1"
        },
        {
          "group": "maps.k8s.elastic.co",
          "kind": "ElasticMapsServer",
          "plural": "elasticmapsservers",
          "version": "v1alpha1"
        },
        {
          "group": "elasticsearch.k8s.elastic.co",
          "kind": "Elasticsearch",
          "plural": "elasticsearches",
          "version": "v1"
        },
        {
          "group": "elasticsearch.k8s.elastic.co",
          "kind": "Elasticsearch",
          "plural": "elasticsearches",
          "version": "v1beta1"
        },
        {
          "group": "enterprisesearch.k8s.elastic.co",
          "kind": "EnterpriseSearch",
          "plural": "enterprisesearches",
          "version": "v1"
        },
        {
          "group": "enterprisesearch.k8s.elastic.co",
          "kind": "EnterpriseSearch",
          "plural": "enterprisesearches",
          "version": "v1beta1"
        },
        {
          "group": "agent.k8s.elastic.co",
          "kind": "Agent",
          "plural": "agents",
          "version": "v1alpha1"
        },
        {
          "group": "elasticsearch.k8s.elastic.co",
          "kind": "Elasticsearch",
          "plural": "elasticsearches",
          "version": "v1alpha1"
        },
        {
          "group": "kibana.k8s.elastic.co",
          "kind": "Kibana",
          "plural": "kibanas",
          "version": "v1beta1"
        },
        {
          "group": "apm.k8s.elastic.co",
          "kind": "ApmServer",
          "plural": "apmservers",
          "version": "v1"
        },
        {
          "group": "apm.k8s.elastic.co",
          "kind": "ApmServer",
          "plural": "apmservers",
          "version": "v1beta1"
        },
        {
          "group": "beat.k8s.elastic.co",
          "kind": "Beat",
          "plural": "beats",
          "version": "v1beta1"
        },
        {
          "group": "kibana.k8s.elastic.co",
          "kind": "Kibana",
          "plural": "kibanas",
          "version": "v1"
        },
        {
          "group": "kibana.k8s.elastic.co",
          "kind": "Kibana",
          "plural": "kibanas",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:91c6d22b19fddec0b49fa0336795019063d9c566704476d853b8a238dbbc8165",
          "image": "registry.connect.redhat.com/elastic/eck-operator@sha256:91c6d22b19fddec0b49fa0336795019063d9c566704476d853b8a238dbbc8165",
          "name": "eck-operator-91c6d22b19fddec0b49fa0336795019063d9c566704476d853b8a238dbbc8165-annotation"
        },
        {
          "digest": "sha256:91c6d22b19fddec0b49fa0336795019063d9c566704476d853b8a238dbbc8165",
          "image": "registry.connect.redhat.com/elastic/eck-operator@sha256:91c6d22b19fddec0b49fa0336795019063d9c566704476d853b8a238dbbc8165",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.6.0",
      "version_original": "1.6.0"
    },
    {
      "_id": "611fbd13bd674341b5c5f0ef",
      "alm_examples": [
        {
          "kind": "Anzo",
          "metadata": {
            "name": "agent01"
          },
          "spec": {
            "nodeConfig": {
              "spec": {
                "replicas": 1,
                "selector": {
                  "matchLabels": {
                    "app": "anzo"
                  }
                },
                "serviceName": "anzo-agent01",
                "template": {
                  "metadata": {
                    "labels": {
                      "app": "anzo"
                    }
                  },
                  "spec": {
                    "containers": [
                      {
                        "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:d1c2d54ca205d669c5ea1bb6aae105afe2385cc5695cecd78423ffa1d98929f8",
                        "name": "anzo",
                        "resources": {
                          "limits": {
                            "cpu": "4000m",
                            "memory": "12Gi"
                          },
                          "requests": {
                            "cpu": "4000m",
                            "memory": "12Gi"
                          }
                        }
                      }
                    ],
                    "serviceAccountName": "anzo-operator"
                  }
                }
              }
            },
            "role": "AnzoAgent"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzo-operator-bundle@sha256:28703523a1c160570fc23fb1236d2a02108b97e4423d42b0d3cde98c94d351cb",
      "bundle_path_digest": "sha256:28703523a1c160570fc23fb1236d2a02108b97e4423d42b0d3cde98c94d351cb",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:32:51.478000+00:00",
      "csv_description": "The Anzo Operator provides the way to install and configure an anzo agent setup on Red Hat K8S environment.\nCurrently, this is supported only through when deployed as an Anzo Agent as part of an Anzo Unstructured deployment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzo/blob/v2.0.0/README.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzo/userdoc/cloud-deployments.htm )\n\n### Support\n\nWe offer Support to our customers through [ Cambridge Semantics Customer Center ]( https://customercenter.cambridgesemantics.com/ ).",
      "csv_display_name": "Anzo Operator",
      "csv_metadata_description": "kubernetes operator for Anzo",
      "csv_name": "anzo-operator.v2.0.101",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:55:02.008000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "anzo-operator",
      "provided_apis": [
        {
          "group": "anzo.cambridgesemantics.com",
          "kind": "Anzo",
          "plural": "anzos",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:563e964f5771b9bfd860ea993b6e0936a54504908f5b2d1dff6f99ade74bb7a0",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:563e964f5771b9bfd860ea993b6e0936a54504908f5b2d1dff6f99ade74bb7a0",
          "name": "anzo-operator-563e964f5771b9bfd860ea993b6e0936a54504908f5b2d1dff6f99ade74bb7a0-annotation"
        },
        {
          "digest": "sha256:563e964f5771b9bfd860ea993b6e0936a54504908f5b2d1dff6f99ade74bb7a0",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:563e964f5771b9bfd860ea993b6e0936a54504908f5b2d1dff6f99ade74bb7a0",
          "name": "manager"
        },
        {
          "digest": "sha256:d1c2d54ca205d669c5ea1bb6aae105afe2385cc5695cecd78423ffa1d98929f8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:d1c2d54ca205d669c5ea1bb6aae105afe2385cc5695cecd78423ffa1d98929f8",
          "name": "anzo"
        },
        {
          "digest": "sha256:d1c2d54ca205d669c5ea1bb6aae105afe2385cc5695cecd78423ffa1d98929f8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:d1c2d54ca205d669c5ea1bb6aae105afe2385cc5695cecd78423ffa1d98929f8",
          "name": "anzo-d1c2d54ca205d669c5ea1bb6aae105afe2385cc5695cecd78423ffa1d98929f8-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "611fbd1c9f14588c41ebe8e8",
      "alm_examples": [
        {
          "kind": "NutanixCsiStorage",
          "metadata": {
            "name": "nutanixcsistorage"
          },
          "spec": {
            "imagePullPolicy": "IfNotPresent",
            "node": {
              "image": "registry.connect.redhat.com/nutanix/ntnx-csi-os@sha256:2289045e0f79733c4f1871f7c686e35b49a4d484aa82446784e3713dfa3d23c7"
            },
            "provisioner": {
              "image": "registry.connect.redhat.com/nutanix/ntnx-csi-os@sha256:2289045e0f79733c4f1871f7c686e35b49a4d484aa82446784e3713dfa3d23c7"
            },
            "scc": "yes",
            "sidecars": {
              "livenessprobe": {
                "image": "quay.io/k8scsi/livenessprobe@sha256:04a9c4a49de1bd83d21e962122da2ac768f356119fb384660aa33d93183996c3"
              },
              "provisioner": {
                "image": "quay.io/k8scsi/csi-provisioner@sha256:78e3393f5fd5ff6c1e5dada2478cfa456fb7164929e573cf9a87bf6532730679"
              },
              "registrar": {
                "image": "quay.io/k8scsi/csi-node-driver-registrar@sha256:9622c6a6dac7499a055a382930f4de82905a3c5735c0753f7094115c9c871309"
              },
              "resizer": {
                "image": "quay.io/k8scsi/csi-resizer@sha256:6c6a0332693a7c456378f6abd2bb40611826c1e1a733cadbdae2daab3125b71c"
              },
              "snapshotController": {
                "image": "quay.io/k8scsi/snapshot-controller@sha256:45720fd73a0de75ad512351a725c197a6c144912b1cb09500c09752a3741388c"
              },
              "snapshotter": {
                "image": "quay.io/k8scsi/csi-snapshotter@sha256:35ead85dd09aa8cc612fdb598d4e0e2f048bef816f1b74df5eeab67cd21b10aa"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nutanix/nutanix-csi-operator-bundle@sha256:95cb57529b51a318cc3e8a39f68d1424e4a6e5ec0550b0010ccf331b6cfedf54",
      "bundle_path_digest": "sha256:95cb57529b51a318cc3e8a39f68d1424e4a6e5ec0550b0010ccf331b6cfedf54",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-20T14:33:00.787000+00:00",
      "csv_description": "# Overview\n\nThe Nutanix CSI Operator for Kubernetes packages, deploys, manages, upgrades the Nutanix CSI Driver on Kubernetes and OpenShift for dynamic provisioning of persistent volumes on Nutanix Enterprise Cloud platform.\n\nThe Nutanix Container Storage Interface (CSI) Driver for Kubernetes leverages Nutanix Volumes and Nutanix Files to provide scalable and persistent storage for stateful applications.\n\nWith Nutanix CSI Provider you can:\n\n  *  **Provide persistent storage to your containers**\n     * Leverage PVC ressources to consume dynamicaly Nutanix storage\n\n     * With Files storage classes, applications on multiple pods can access the same storage, and also have the benefit of multi-pod read and write access.\n\n## Configuring k8s secret and storage class\nIn order to use this driver, create the relevant storage classes and secrets, by followinig the below section:\n 1. Create a Nutanix system secret\n```\n apiVersion: v1\n kind: Secret\n metadata:\n   name: ntnx-secret\n   namespace: ntnx-system\n data:\n   # base64 encoded prism-ip:prism-port:admin:password. \n   # E.g.: echo -n \"10.0.0.14:9440:admin:mypassword\" | base64\n   key: MS4yLjMuNDo5NDQwOm51dGFuaXg6aXRpc21hZ2lj\n```\n 2. Create storage classes\n```\n kind: StorageClass\n apiVersion: storage.k8s.io/v1\n metadata:\n     name: nutanix-volume\n provisioner: csi.nutanix.com\n parameters:\n     csi.storage.k8s.io/provisioner-secret-name: ntnx-secret\n     csi.storage.k8s.io/provisioner-secret-namespace: ntnx-system\n     csi.storage.k8s.io/node-publish-secret-name: ntnx-secret\n     csi.storage.k8s.io/node-publish-secret-namespace: ntnx-system\n     csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret\n     csi.storage.k8s.io/controller-expand-secret-namespace: ntnx-system\n     csi.storage.k8s.io/fstype: ext4\n     dataServiceEndPoint: 10.0.0.15:3260\n     storageContainer: default-container\n     storageType: NutanixVolumes\n     #whitelistIPMode: ENABLED\n     #chapAuth: ENABLED\n allowVolumeExpansion: true\n reclaimPolicy: Delete\n```\n",
      "csv_display_name": "Nutanix CSI Operator",
      "csv_metadata_description": "An Operator for deploying and managing the Nutanix CSI Driver",
      "csv_name": "nutanixcsioperator.v0.1.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-05T10:59:58.632000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "nutanixcsioperator",
      "provided_apis": [
        {
          "group": "crd.nutanix.com",
          "kind": "NutanixCsiStorage",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:6d0286b8a8f6f3cd9d6cd8319400acf27b70fbb52df5808ec6fe2d9849be7d8c",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:6d0286b8a8f6f3cd9d6cd8319400acf27b70fbb52df5808ec6fe2d9849be7d8c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:1a51992e5f0f3bea36996cb5e60cb9e62f51f8cff4a5dcc3e977e0b301546dd7",
          "image": "registry.connect.redhat.com/nutanix/nutanix-csi-operator@sha256:1a51992e5f0f3bea36996cb5e60cb9e62f51f8cff4a5dcc3e977e0b301546dd7",
          "name": "manager"
        },
        {
          "digest": "sha256:35ead85dd09aa8cc612fdb598d4e0e2f048bef816f1b74df5eeab67cd21b10aa",
          "image": "quay.io/k8scsi/csi-snapshotter@sha256:35ead85dd09aa8cc612fdb598d4e0e2f048bef816f1b74df5eeab67cd21b10aa",
          "name": "csi-snapshotter-35ead85dd09aa8cc612fdb598d4e0e2f048bef816f1b74df5eeab67cd21b10aa-annotation"
        },
        {
          "digest": "sha256:45720fd73a0de75ad512351a725c197a6c144912b1cb09500c09752a3741388c",
          "image": "quay.io/k8scsi/snapshot-controller@sha256:45720fd73a0de75ad512351a725c197a6c144912b1cb09500c09752a3741388c",
          "name": "snapshot-controller-45720fd73a0de75ad512351a725c197a6c144912b1cb09500c09752a3741388c-annotation"
        },
        {
          "digest": "sha256:6c6a0332693a7c456378f6abd2bb40611826c1e1a733cadbdae2daab3125b71c",
          "image": "quay.io/k8scsi/csi-resizer@sha256:6c6a0332693a7c456378f6abd2bb40611826c1e1a733cadbdae2daab3125b71c",
          "name": "csi-resizer-6c6a0332693a7c456378f6abd2bb40611826c1e1a733cadbdae2daab3125b71c-annotation"
        },
        {
          "digest": "sha256:9622c6a6dac7499a055a382930f4de82905a3c5735c0753f7094115c9c871309",
          "image": "quay.io/k8scsi/csi-node-driver-registrar@sha256:9622c6a6dac7499a055a382930f4de82905a3c5735c0753f7094115c9c871309",
          "name": "csi-node-driver-registrar-9622c6a6dac7499a055a382930f4de82905a3c5735c0753f7094115c9c871309-annotation"
        },
        {
          "digest": "sha256:78e3393f5fd5ff6c1e5dada2478cfa456fb7164929e573cf9a87bf6532730679",
          "image": "quay.io/k8scsi/csi-provisioner@sha256:78e3393f5fd5ff6c1e5dada2478cfa456fb7164929e573cf9a87bf6532730679",
          "name": "csi-provisioner-78e3393f5fd5ff6c1e5dada2478cfa456fb7164929e573cf9a87bf6532730679-annotation"
        },
        {
          "digest": "sha256:04a9c4a49de1bd83d21e962122da2ac768f356119fb384660aa33d93183996c3",
          "image": "quay.io/k8scsi/livenessprobe@sha256:04a9c4a49de1bd83d21e962122da2ac768f356119fb384660aa33d93183996c3",
          "name": "livenessprobe-04a9c4a49de1bd83d21e962122da2ac768f356119fb384660aa33d93183996c3-annotation"
        },
        {
          "digest": "sha256:2289045e0f79733c4f1871f7c686e35b49a4d484aa82446784e3713dfa3d23c7",
          "image": "registry.connect.redhat.com/nutanix/ntnx-csi-os@sha256:2289045e0f79733c4f1871f7c686e35b49a4d484aa82446784e3713dfa3d23c7",
          "name": "ntnx-csi-os-2289045e0f79733c4f1871f7c686e35b49a4d484aa82446784e3713dfa3d23c7-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.1.3",
      "version_original": "0.1.3"
    },
    {
      "_id": "611fbd2edece23122b7a7834",
      "alm_examples": [
        {
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:3d0a9de740110b924b1ad5e83bb1e36b308f1b53f9e76a50cffcbeda9d34ea78",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoCompaction": {
                "databaseFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                },
                "parallelCompaction": false,
                "timeWindow": {
                  "abortCompactionOutsideWindow": true,
                  "end": "06:00",
                  "start": "02:00"
                },
                "tombstonePurgeInterval": "72h",
                "viewFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                }
              },
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:fd6d9c0ef033009e76d60dc36f55ce7f3aaa942a7be9c2b66c335eabc8f5b11e",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:18015c72d17a33a21ea221d48fddf493848fc1ca5702007f289369c5815fb3df"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:78656ed0df41696e67429609e3262ec31f5ca9f5ca946300e6a4a6503c1bbd18",
      "bundle_path_digest": "sha256:78656ed0df41696e67429609e3262ec31f5ca9f5ca946300e6a4a6503c1bbd18",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:33:18.033000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:56:04.771000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "version": "v1"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:87f601a4fd68ba8a5fed9a610dc03661addc9db9b4e332bfc4907d754d91c496",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:87f601a4fd68ba8a5fed9a610dc03661addc9db9b4e332bfc4907d754d91c496",
          "name": "operator-87f601a4fd68ba8a5fed9a610dc03661addc9db9b4e332bfc4907d754d91c496-annotation"
        },
        {
          "digest": "sha256:87f601a4fd68ba8a5fed9a610dc03661addc9db9b4e332bfc4907d754d91c496",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:87f601a4fd68ba8a5fed9a610dc03661addc9db9b4e332bfc4907d754d91c496",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:fd6d9c0ef033009e76d60dc36f55ce7f3aaa942a7be9c2b66c335eabc8f5b11e",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:fd6d9c0ef033009e76d60dc36f55ce7f3aaa942a7be9c2b66c335eabc8f5b11e",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:3d0a9de740110b924b1ad5e83bb1e36b308f1b53f9e76a50cffcbeda9d34ea78",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:3d0a9de740110b924b1ad5e83bb1e36b308f1b53f9e76a50cffcbeda9d34ea78",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:18015c72d17a33a21ea221d48fddf493848fc1ca5702007f289369c5815fb3df",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:18015c72d17a33a21ea221d48fddf493848fc1ca5702007f289369c5815fb3df",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:18015c72d17a33a21ea221d48fddf493848fc1ca5702007f289369c5815fb3df",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:18015c72d17a33a21ea221d48fddf493848fc1ca5702007f289369c5815fb3df",
          "name": "exporter-18015c72d17a33a21ea221d48fddf493848fc1ca5702007f289369c5815fb3df-annotation"
        },
        {
          "digest": "sha256:3d0a9de740110b924b1ad5e83bb1e36b308f1b53f9e76a50cffcbeda9d34ea78",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:3d0a9de740110b924b1ad5e83bb1e36b308f1b53f9e76a50cffcbeda9d34ea78",
          "name": "operator-backup-3d0a9de740110b924b1ad5e83bb1e36b308f1b53f9e76a50cffcbeda9d34ea78-annotation"
        },
        {
          "digest": "sha256:fd6d9c0ef033009e76d60dc36f55ce7f3aaa942a7be9c2b66c335eabc8f5b11e",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:fd6d9c0ef033009e76d60dc36f55ce7f3aaa942a7be9c2b66c335eabc8f5b11e",
          "name": "server-fd6d9c0ef033009e76d60dc36f55ce7f3aaa942a7be9c2b66c335eabc8f5b11e-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.1.0",
      "version_original": "2.1.0"
    },
    {
      "_id": "611fbd319f14588c41ebe8eb",
      "alm_examples": [
        {
          "kind": "HPECSIDriver",
          "metadata": {
            "name": "csi-driver"
          },
          "spec": {
            "cspClientTimeout": 60,
            "disable": {
              "alletra6000": false,
              "alletra9000": false,
              "nimble": false,
              "primera": false
            },
            "disableNodeConformance": false,
            "disableNodeGetVolumeStats": false,
            "imagePullPolicy": "IfNotPresent",
            "iscsi": {
              "chapPassword": "",
              "chapUser": ""
            },
            "kubeletRootDir": "/var/lib/kubelet/",
            "logLevel": "info",
            "registry": "quay.io"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hpestorage/csi-driver-operator-bundle@sha256:24d8008a3b72f61d720a20f0f12eecc5b13fdc0916edcb7cb0ae99498c012c9b",
      "bundle_path_digest": "sha256:24d8008a3b72f61d720a20f0f12eecc5b13fdc0916edcb7cb0ae99498c012c9b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:33:21.047000+00:00",
      "csv_description": "The HPE CSI Operator for Kubernetes packages, deploys, manages, upgrades the HPE CSI Driver on Kubernetes and OpenShift for dynamic provisioning of persistent volumes on HPE storage systems.\nThe HPE CSI Driver for Kubernetes leverages HPE storage platforms to provide scalable and persistent storage for stateful applications. Please refer to our CSI driver [documentation](https://scod.hpedev.io/csi_driver/index.html) for supported HPE platforms and full list of supported CSI features.\n## Installation\nRefer to the HPE CSI Operator for Kubernetes [documentation](https://scod.hpedev.io/csi_driver/deployment.html#operator).\n",
      "csv_display_name": "HPE CSI Operator for Kubernetes",
      "csv_metadata_description": "A Container Storage Interface (CSI) driver for HPE storage platforms. The CSI driver allows you to use HPE storage with your preferred container orchestrator.",
      "csv_name": "hpe-csi-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:57:34.327000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "hpe-csi-operator",
      "provided_apis": [
        {
          "group": "storage.hpe.com",
          "kind": "HPECSIDriver",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:e397fc7820775a2667e28a8f5ce59860ffc802968f1752e1672319c6e2e8e771",
          "image": "registry.connect.redhat.com/hpestorage/csi-driver-operator@sha256:e397fc7820775a2667e28a8f5ce59860ffc802968f1752e1672319c6e2e8e771",
          "name": "csi-driver-operator-e397fc7820775a2667e28a8f5ce59860ffc802968f1752e1672319c6e2e8e771-annotation"
        },
        {
          "digest": "sha256:e397fc7820775a2667e28a8f5ce59860ffc802968f1752e1672319c6e2e8e771",
          "image": "registry.connect.redhat.com/hpestorage/csi-driver-operator@sha256:e397fc7820775a2667e28a8f5ce59860ffc802968f1752e1672319c6e2e8e771",
          "name": "hpe-csi-operator"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "611fbd46bd674341b5c5f0fb",
      "alm_examples": [
        {
          "kind": "LinstorController",
          "metadata": {
            "name": "linstor"
          },
          "spec": {
            "controllerImage": "",
            "dbConnectionURL": "etcd://linstor-etcd:2379",
            "drbdRepoCred": "",
            "priorityClassName": ""
          }
        },
        {
          "kind": "LinstorCSIDriver",
          "metadata": {
            "name": "linstor"
          },
          "spec": {
            "controllerEndpoint": "http://linstor:3370",
            "controllerReplicas": 1,
            "csiControllerServiceAccountName": "csi-controller",
            "csiNodeServiceAccountName": "csi-node",
            "imagePullSecret": "",
            "linstorPluginImage": ""
          }
        },
        {
          "kind": "LinstorSatelliteSet",
          "metadata": {
            "name": "linstor-satellites"
          },
          "spec": {
            "automaticStorageType": "None",
            "controllerEndpoint": "http://linstor:3370",
            "drbdRepoCred": "",
            "kernelModuleInjectionMode": "ShippedModules",
            "priorityClassName": "",
            "satelliteImage": ""
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/linbit/linstor-operator-bundle@sha256:20c0b821db27ef8f551212d14ba508f88a116f92a9f9a214e79709744b96678b",
      "bundle_path_digest": "sha256:20c0b821db27ef8f551212d14ba508f88a116f92a9f9a214e79709744b96678b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:33:42.185000+00:00",
      "csv_description": "LINSTOR is a configuration management system for storage on Linux systems.\nIt manages LVM logical volumes and/or ZFS ZVOLs on a cluster of nodes.\nIt leverages DRBD for replication between different nodes and to provide block\nstorage devices to users and applications. It manages snapshots, encryption and\ncaching of HDD backed data in SSDs via bcache.\n\nLINBIT provides a certified LINSTOR operator to ease deployment of LINSTOR\non Openshift by installing DRBD, managing Satellite and Controller pods,\nand integrating with Openshift to provision persistent storage for your workloads.\n\nFor detailed instructions and more configuration options, see our [user guide].\n\n[user guide]: https://www.linbit.com/drbd-user-guide/linstor-guide-1_0-en/#ch-openshift\n\n## Install\n\nUnlike deployment via the helm chart, the certified Openshift\noperator does not deploy the needed etcd cluster. You must deploy this\nyourself ahead of time. We do this via the etcd operator available in the\nOperatorHub.\n\nIMPORTANT: It is advised that the etcd deployment uses persistent\nstorage of some type. Either use an existing storage provisioner with\na default `StorageClass` or simply use `hostPath` volumes.\n\n### Installing the operator\n\nHit \"Install\", select the stable update channel and a namespace for the\noperator. Use of a new namespace is recommended.\n\nHit \"Install\" again. At this point you should have just one pod, the\noperator pod, running. Next we needs to configure the remaining provided APIs.\n\n#### A note on operator namespaces\nThe LINSTOR operator can only watch for events and manage\ncustom resources that are within the same namespace it is deployed\nwithin (OwnNamsespace). This means the LINSTOR Controller, LINSTOR\nSatellites, and LINSTOR CSI Driver pods all need to be deployed in the\nsame namsepace as the LINSTOR Operator pod.\n\n### Deploying the LINSTOR Controller\n\nNavigate to the left-hand control pane of the Openshift Web\nConsole. Expand the \"Operators\" section, selecting \"Installed Operators\".\nFind the entry for the \"Linstor Operator\", then select the \"LinstorController\"\nfrom the \"Provided APIs\" column on the right.\n\nFrom here you should see a page that says \"No Operands Found\" and will\nfeature a large button on the right which says \"Create\nLinstorController\". Click the \"Create LinstorController\" button.\n\nHere you will be presented with options to configure the LINSTOR\nController. Either via the web-form view or the YAML View. Regardless\nof which view you select, make sure that the `dbConnectionURL` matches\nthe endpoint provided from your etcd deployment. Otherwise, the\ndefaults are usually fine for most purposes.\n\nLastly hit \"Create\", you should now see a linstor-controller pod\nrunning.\n\n### Deploying the LINSTOR Satellites\n\nNext we need to deploy the Satellites Set. Just as before navigate\nto the left-hand control pane of the Openshift Web Console. Expand the\n\"Operators\" section, but this time select \"Installed Operators\". Find\nthe entry for the \"Linstor Operator\", then select the\n\"LinstorSatelliteSet\" from the \"Provided APIs\" column on the right.\n\nFrom here you should see a page that says \"No Operands Found\" and will\nfeature a large button on the right which says \"Create\nLinstorSatelliteSet\". Click the \"Create LinstorSatelliteSet\" button.\n\nHere you will be presented with the options to configure the LINSTOR\nSatellites. The defaults should be enough to get you started.\nMake sure the `controllerEndpoint` matches what is available in the\nopenshift endpoints. The default is usually correct here.\n\nYou can edit the `storagePools` section to configure LINSTOR storage pools,\nincluding preparing the backing devices. See our [storage guide].\n\n[storage guide]: https://www.linbit.com/drbd-user-guide/linstor-guide-1_0-en/#s-kubernetes-storage\n\nBelow is an example manifest:\n\n```yaml\napiVersion: linstor.linbit.com/v1\nkind: LinstorSatelliteSet\nmetadata:\n  name: linstor-satellites\nspec:\n  satelliteImage: ''\n  automaticStorageType: None\n  storagePools:\n    lvmThinPools:\n    - name: openshift-pool\n      volumeGroup: \"\"\n      thinVolume: openshift\n      devicePaths:\n      - /dev/vdb\n  drbdRepoCred: ''\n  kernelModuleInjectionMode: ShippedModules\n  controllerEndpoint: 'http://linstor:3370'\n  priorityClassName: ''\n```\n\nLastly hit \"Create\", you should now see a linstor-node pod\nrunning on every worker node.\n\n### Deploying the LINSTOR CSI driver\n\nLast bit left is the CSI pods to bridge the layer between the CSI and\nLINSTOR. Just as before navigate to the left-hand control pane of the\nOpenshift Web Console. Expand the \"Operators\" section, but this time\nselect \"Installed Operators\". Find the entry for the \"Linstor Operator\",\nthen select the \"LinstorCSIDriver\" from the \"Provided APIs\" column on the\nright.\n\nFrom here you should see a page that says \"No Operands Found\" and will\nfeature a large button on the right which says \"Create\nLinstorCSIDriver\". Click the \"Create LinstorCSIDriver\" button.\n\nAgain, you will be presented with the options. Make sure that the\n`controllerEndpoint` is correct. Otherwise the defaults are fine for\nmost use cases.\n\nLastly hit \"Create\". You will now see a single \"linstor-csi-controller\" pod,\nas well as a \"linstor-csi-node\" pod on all worker nodes.\n\n## Interacting with LINSTOR in Openshift.\n\nThe Controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.\nFor instance:\n\n```\noc exec deployment/linstor-cs-controller -- linstor storage-pool list\n```\n\nThis should only be necessary for investigating problems and accessing advanced functionality.\nRegular operation such as creating volumes should be achieved via the [Openshift/Kubernetes integration].\n\n[Openshift/Kubernetes integration]: https://www.linbit.com/drbd-user-guide/linstor-guide-1_0-en/#s-kubernetes-basic-configuration-and-deployment",
      "csv_display_name": "Linstor Operator",
      "csv_metadata_description": "LINSTOR Kubernetes Operator",
      "csv_name": "linstor-operator.v1.5.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-05T10:59:10.068000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "linstor-operator",
      "provided_apis": [
        {
          "group": "linstor.linbit.com",
          "kind": "LinstorCSIDriver",
          "version": "v1"
        },
        {
          "group": "linstor.linbit.com",
          "kind": "LinstorSatelliteSet",
          "version": "v1"
        },
        {
          "group": "linstor.linbit.com",
          "kind": "LinstorController",
          "version": "v1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.5.1",
      "version_original": "1.5.1"
    },
    {
      "_id": "611fbd4f9f14588c41ebe8f5",
      "alm_examples": [
        {
          "kind": "ZabbixAgent",
          "metadata": {
            "labels": {
              "app": "agent",
              "vendor": "zabbix"
            },
            "name": "zabbix-agent"
          },
          "spec": {
            "active_allow": true,
            "activeservers": "",
            "allow_key": "",
            "allow_privileged": true,
            "buffer#": 100,
            "buffer_send": 5,
            "debug_level": 3,
            "deny_key": "system.run[*]",
            "host_interface": "",
            "host_interface_item": "",
            "hostname": "",
            "hostname_item": "system.hostname",
            "load_module": "",
            "log_remote_commands": true,
            "max_lines_per_second": 20,
            "metadata": "",
            "metadata_item": "",
            "passive_allow": true,
            "passive_servers": "",
            "refresh_active_checks": 120,
            "server_host": "zabbix-server",
            "start_agents": 3,
            "timeout": 3,
            "tls_accept": "unencrypted",
            "tls_ca_file_name": "",
            "tls_cert_file_name": "",
            "tls_cipher_all": "",
            "tls_cipher_cert_13": "",
            "tls_cipher_psk": "",
            "tls_cipherall_13": "",
            "tls_connect": "unencrypted",
            "tls_crl_file_name": "",
            "tls_key_file_name": "",
            "tls_psk_file_name": "",
            "tls_psk_identity": "",
            "tls_server_cert_issuer": "",
            "tls_server_cert_subject": "",
            "tlsciphercert": "",
            "tlscipherpsk13": "",
            "unsafe_user_parameters": false
          }
        },
        {
          "kind": "ZabbixAppliance",
          "metadata": {
            "labels": {
              "app": "appliance",
              "vendor": "zabbix"
            },
            "name": "zabbix-appliance"
          },
          "spec": {
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "server": {
              "cache#": "8M",
              "cache_update_frequency": 60,
              "db_tls_cipher": "",
              "db_tls_cipher13": "",
              "db_tls_cipher_13": "",
              "db_tls_connect": "",
              "debug_level": 3,
              "export_file#": "",
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "history_storage_date_index": true,
              "history_storage_types": "",
              "housekeeping_frequency": 1,
              "load_module": "",
              "log_slow_queries": 0,
              "max_housekeeper_delete": 5000,
              "proxy_config_frequency": 3600,
              "proxy_data_frequency": 1,
              "start_alerters": 3,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_escalators": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_lld_processors": 2,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_proxy_pollers": 0,
              "start_timers": 1,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "stats_allowed_ip": "",
              "timeout": 4,
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tlscipherall": "",
              "tlscipherall13": "",
              "tlsciphercert": "",
              "tlsciphercert13": "",
              "tlscipherpsk": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "trend_cache#": "4M",
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "value_cache#": "8M",
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "web": {
              "db_double_ieee754": true,
              "deny_gui_access": false,
              "enable_web_access_log": true,
              "gui_access_ip_range": "",
              "gui_warning_msg": "Zabbix is under maintenance.",
              "history_storage_types": "",
              "max_execution_time": 300,
              "max_input_time": 300,
              "memory_limit": "128M",
              "post_max#": "16M",
              "server_name": "Kubernetes installation",
              "session_name": "zbx_sessionid",
              "sso_settings": "",
              "timezone": "Europe/Riga",
              "upload_max_filesize": "2M"
            },
            "web_enable_route": true,
            "zabbix_mysql_volumeclaim": "mysql-volume-claim",
            "zabbix_mysqlsecret": "zabbix-mysql-secrets"
          }
        },
        {
          "kind": "ZabbixFull",
          "metadata": {
            "labels": {
              "app": "server",
              "vendor": "zabbix"
            },
            "name": "zabbix-full"
          },
          "spec": {
            "history_storage_url": "",
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "java_gateway#": 1,
            "server": {
              "cache#": "8M",
              "cache_update_frequency": 60,
              "db_tls_cipher": "",
              "db_tls_cipher_13": "",
              "db_tls_connect": "",
              "debug_level": 3,
              "export_file#": "",
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "history_storage_date_index": true,
              "history_storage_types": "",
              "housekeeping_frequency": 1,
              "load_module": "",
              "log_slow_queries": 0,
              "max_housekeeper_delete": 5000,
              "proxy_config_frequency": 3600,
              "proxy_data_frequency": 1,
              "start_alerters": 3,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_escalators": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_lld_processors": 2,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_proxy_pollers": 0,
              "start_timers": 1,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "stats_allowed_ip": "",
              "timeout": 4,
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_cipher_all": "",
              "tls_cipher_cert_13": "",
              "tls_cipher_psk": "",
              "tls_cipherall_13": "",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tlsciphercert": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "trend_cache#": "4M",
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "value_cache#": "8M",
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "web": {
              "db_cipher_list": "",
              "db_double_ieee754": true,
              "db_encryption": false,
              "db_verify_host": false,
              "deny_gui_access": false,
              "enable_web_access_log": true,
              "gui_access_ip_range": "",
              "gui_warning_msg": "Zabbix is under maintenance.",
              "history_storage_types": "",
              "max_execution_time": 300,
              "max_input_time": 300,
              "memory_limit": "128M",
              "post_max#": "16M",
              "server_name": "Kubernetes installation",
              "session_name": "zbx_sessionid",
              "sso_settings": "",
              "timezone": "Europe/Riga",
              "upload_max_filesize": "2M"
            },
            "web#": 2,
            "web_enable_route": true,
            "zabbix_mysql_volumeclaim": "mysql-volume-claim",
            "zabbix_mysqlsecret": "zabbix-mysql-secrets"
          }
        },
        {
          "kind": "ZabbixProxyMysql",
          "metadata": {
            "labels": {
              "app": "proxy",
              "vendor": "zabbix"
            },
            "name": "zabbix-proxy-mysql"
          },
          "spec": {
            "db_server_port": 3306,
            "internal_db": true,
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "java_gateway#": 1,
            "mysql_database": "zabbix_proxy",
            "proxy": {
              "cache#": "8M",
              "config_frequency": 3600,
              "data_sender_frequency": 1,
              "db_tls_cipher": "",
              "db_tls_cipher_13": "",
              "db_tls_connect": "",
              "debug_level": 3,
              "enable_remote_commands": false,
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "hostname": "",
              "hostname_item": "system.hostname",
              "housekeeping_frequency": 1,
              "log_remote_commands": true,
              "log_slow_queries": 0,
              "proxy_heartbeat_frequency": 60,
              "proxy_local_buffer": 0,
              "proxy_mode": 0,
              "proxy_offline_buffer": 1,
              "server_host": "zabbix-server",
              "server_port": 10051,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "timeout": 4,
              "tls_accept": "unencrypted",
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_cipher_all": "",
              "tls_cipher_cert_13": "",
              "tls_cipher_psk": "",
              "tls_cipherall_13": "",
              "tls_connect": "unencrypted",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tls_psk_file_name": "",
              "tls_psk_identity": "",
              "tls_server_cert_issuer": "",
              "tls_server_cert_subject": "",
              "tlscipherall": "",
              "tlscipherall13": "",
              "tlsciphercert": "",
              "tlsciphercert13": "",
              "tlscipherpsk": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "zabbix_mysql_volumeclaim": "mysql-volume-claim",
            "zabbix_mysqlsecret": "zabbix-mysql-secrets"
          }
        },
        {
          "kind": "ZabbixProxySqlite",
          "metadata": {
            "labels": {
              "app": "proxy",
              "vendor": "zabbix"
            },
            "name": "zabbix-proxy-sqlite"
          },
          "spec": {
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "java_gateway#": 1,
            "proxy": {
              "cache#": "8M",
              "config_frequency": 3600,
              "data_sender_frequency": 1,
              "debug_level": 3,
              "enable_remote_commands": false,
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "hostname": "",
              "hostname_item": "system.hostname",
              "housekeeping_frequency": 1,
              "log_remote_commands": true,
              "log_slow_queries": 0,
              "proxy_heartbeat_frequency": 60,
              "proxy_local_buffer": 0,
              "proxy_mode": 0,
              "proxy_offline_buffer": 1,
              "server_host": "zabbix-server",
              "server_port": 10051,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "timeout": 4,
              "tls_accept": "unencrypted",
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_cipher_all": "",
              "tls_cipher_cert_13": "",
              "tls_cipher_psk": "",
              "tls_cipherall_13": "",
              "tls_connect": "unencrypted",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tls_psk_file_name": "",
              "tls_psk_identity": "",
              "tls_server_cert_issuer": "",
              "tls_server_cert_subject": "",
              "tlscipherall": "",
              "tlscipherall13": "",
              "tlsciphercert": "",
              "tlsciphercert13": "",
              "tlscipherpsk": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "proxy#": 1
          }
        },
        {
          "kind": "ZabbixServer",
          "metadata": {
            "labels": {
              "app": "server",
              "vendor": "zabbix"
            },
            "name": "zabbix-server"
          },
          "spec": {
            "db_server_host": "mysql-server",
            "db_server_port": 3306,
            "history_storage_url": "",
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "java_gateway#": 1,
            "mysql_database": "zabbix",
            "server": {
              "cache#": "8M",
              "cache_update_frequency": 60,
              "db_tls_cipher": "",
              "db_tls_cipher13": "",
              "db_tls_cipher_13": "",
              "db_tls_connect": "",
              "debug_level": 3,
              "export_file#": "",
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "history_storage_date_index": true,
              "history_storage_types": "",
              "housekeeping_frequency": 1,
              "load_module": "",
              "log_slow_queries": 0,
              "max_housekeeper_delete": 5000,
              "proxy_config_frequency": 3600,
              "proxy_data_frequency": 1,
              "start_alerters": 3,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_escalators": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_lld_processors": 2,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_proxy_pollers": 0,
              "start_timers": 1,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "stats_allowed_ip": "",
              "timeout": 4,
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tlscipherall": "",
              "tlscipherall13": "",
              "tlsciphercert": "",
              "tlsciphercert13": "",
              "tlscipherpsk": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "trend_cache#": "4M",
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "value_cache#": "8M",
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "web": {
              "db_cipher_list": "",
              "db_double_ieee754": true,
              "db_encryption": false,
              "db_verify_host": false,
              "deny_gui_access": false,
              "enable_web_access_log": true,
              "gui_access_ip_range": "",
              "gui_warning_msg": "Zabbix is under maintenance.",
              "history_storage_types": "",
              "max_execution_time": 300,
              "max_input_time": 300,
              "memory_limit": "128M",
              "post_max#": "16M",
              "server_name": "Kubernetes installation",
              "session_name": "zbx_sessionid",
              "sso_settings": "",
              "timezone": "Europe/Riga",
              "upload_max_filesize": "2M"
            },
            "web#": 2,
            "web_enable_route": true,
            "zabbix_mysqlsecret": "zabbix-mysql-secrets"
          }
        },
        {
          "kind": "ZabbixAgent",
          "metadata": {
            "labels": {
              "app": "agent",
              "vendor": "zabbix"
            },
            "name": "zabbix-agent"
          },
          "spec": {
            "active_allow": true,
            "activeservers": "",
            "allow_key": "",
            "allow_privileged": true,
            "buffer#": 100,
            "buffer_send": 5,
            "debug_level": 3,
            "deny_key": "system.run[*]",
            "host_interface": "",
            "host_interface_item": "",
            "hostname": "",
            "hostname_item": "system.hostname",
            "load_module": "",
            "log_remote_commands": true,
            "max_lines_per_second": 20,
            "metadata": "",
            "metadata_item": "",
            "passive_allow": true,
            "passive_servers": "",
            "refresh_active_checks": 120,
            "server_host": "zabbix-server",
            "start_agents": 3,
            "timeout": 3,
            "tls_accept": "unencrypted",
            "tls_ca_file_name": "",
            "tls_cert_file_name": "",
            "tls_cipher_all": "",
            "tls_cipher_cert_13": "",
            "tls_cipher_psk": "",
            "tls_cipherall_13": "",
            "tls_connect": "unencrypted",
            "tls_crl_file_name": "",
            "tls_key_file_name": "",
            "tls_psk_file_name": "",
            "tls_psk_identity": "",
            "tls_server_cert_issuer": "",
            "tls_server_cert_subject": "",
            "tlsciphercert": "",
            "tlscipherpsk13": "",
            "unsafe_user_parameters": false
          }
        },
        {
          "kind": "ZabbixServer",
          "metadata": {
            "labels": {
              "app": "server",
              "vendor": "zabbix"
            },
            "name": "zabbix-server"
          },
          "spec": {
            "db_server_host": "mysql-server",
            "db_server_port": 3306,
            "history_storage_url": "",
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "java_gateway#": 1,
            "mysql_database": "zabbix",
            "server": {
              "cache#": "8M",
              "cache_update_frequency": 60,
              "db_tls_cipher": "",
              "db_tls_cipher13": "",
              "db_tls_cipher_13": "",
              "db_tls_connect": "",
              "debug_level": 3,
              "export_file#": "",
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "history_storage_date_index": true,
              "history_storage_types": "",
              "housekeeping_frequency": 1,
              "load_module": "",
              "log_slow_queries": 0,
              "max_housekeeper_delete": 5000,
              "proxy_config_frequency": 3600,
              "proxy_data_frequency": 1,
              "start_alerters": 3,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_escalators": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_lld_processors": 2,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_proxy_pollers": 0,
              "start_timers": 1,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "stats_allowed_ip": "",
              "timeout": 4,
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tlscipherall": "",
              "tlscipherall13": "",
              "tlsciphercert": "",
              "tlsciphercert13": "",
              "tlscipherpsk": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "trend_cache#": "4M",
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "value_cache#": "8M",
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "web": {
              "db_cipher_list": "",
              "db_double_ieee754": true,
              "db_encryption": false,
              "db_verify_host": false,
              "deny_gui_access": false,
              "enable_web_access_log": true,
              "gui_access_ip_range": "",
              "gui_warning_msg": "Zabbix is under maintenance.",
              "history_storage_types": "",
              "max_execution_time": 300,
              "max_input_time": 300,
              "memory_limit": "128M",
              "post_max#": "16M",
              "server_name": "Kubernetes installation",
              "session_name": "zbx_sessionid",
              "sso_settings": "",
              "timezone": "Europe/Riga",
              "upload_max_filesize": "2M"
            },
            "web#": 2,
            "web_enable_route": true,
            "zabbix_mysqlsecret": "mysql-secrets"
          }
        },
        {
          "kind": "ZabbixProxySqlite",
          "metadata": {
            "labels": {
              "app": "proxy",
              "vendor": "zabbix"
            },
            "name": "zabbix-proxy-sqlite"
          },
          "spec": {
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "java_gateway#": 1,
            "proxy": {
              "cache#": "8M",
              "config_frequency": 3600,
              "data_sender_frequency": 1,
              "debug_level": 3,
              "enable_remote_commands": false,
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "hostname": "",
              "hostname_item": "system.hostname",
              "housekeeping_frequency": 1,
              "log_remote_commands": true,
              "log_slow_queries": 0,
              "proxy_heartbeat_frequency": 60,
              "proxy_local_buffer": 0,
              "proxy_mode": 0,
              "proxy_offline_buffer": 1,
              "server_host": "zabbix-server",
              "server_port": 10051,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "timeout": 4,
              "tls_accept": "unencrypted",
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_cipher_all": "",
              "tls_cipher_cert_13": "",
              "tls_cipher_psk": "",
              "tls_cipherall_13": "",
              "tls_connect": "unencrypted",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tls_psk_file_name": "",
              "tls_psk_identity": "",
              "tls_server_cert_issuer": "",
              "tls_server_cert_subject": "",
              "tlscipherall": "",
              "tlscipherall13": "",
              "tlsciphercert": "",
              "tlsciphercert13": "",
              "tlscipherpsk": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "proxy#": 1
          }
        },
        {
          "kind": "ZabbixProxyMysql",
          "metadata": {
            "labels": {
              "app": "proxy",
              "vendor": "zabbix"
            },
            "name": "zabbix-proxy-mysql"
          },
          "spec": {
            "db_server_port": 3306,
            "internal_db": true,
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "java_gateway#": 1,
            "mysql_database": "zabbix_proxy",
            "proxy": {
              "cache#": "8M",
              "config_frequency": 3600,
              "data_sender_frequency": 1,
              "db_tls_cipher": "",
              "db_tls_cipher_13": "",
              "db_tls_connect": "",
              "debug_level": 3,
              "enable_remote_commands": false,
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "hostname": "",
              "hostname_item": "system.hostname",
              "housekeeping_frequency": 1,
              "log_remote_commands": true,
              "log_slow_queries": 0,
              "proxy_heartbeat_frequency": 60,
              "proxy_local_buffer": 0,
              "proxy_mode": 0,
              "proxy_offline_buffer": 1,
              "server_host": "zabbix-server",
              "server_port": 10051,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "timeout": 4,
              "tls_accept": "unencrypted",
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_cipher_all": "",
              "tls_cipher_cert_13": "",
              "tls_cipher_psk": "",
              "tls_cipherall_13": "",
              "tls_connect": "unencrypted",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tls_psk_file_name": "",
              "tls_psk_identity": "",
              "tls_server_cert_issuer": "",
              "tls_server_cert_subject": "",
              "tlscipherall": "",
              "tlscipherall13": "",
              "tlsciphercert": "",
              "tlsciphercert13": "",
              "tlscipherpsk": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "zabbix_mysql_volumeclaim": "mysql-volume-claim",
            "zabbix_mysqlsecret": "zabbix-mysql-secrets"
          }
        },
        {
          "kind": "ZabbixFull",
          "metadata": {
            "labels": {
              "app": "server",
              "vendor": "zabbix"
            },
            "name": "zabbix-full"
          },
          "spec": {
            "history_storage_url": "",
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "java_gateway#": 1,
            "server": {
              "cache#": "8M",
              "cache_update_frequency": 60,
              "db_tls_cipher": "",
              "db_tls_cipher_13": "",
              "db_tls_connect": "",
              "debug_level": 3,
              "export_file#": "",
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "history_storage_date_index": true,
              "history_storage_types": "",
              "housekeeping_frequency": 1,
              "load_module": "",
              "log_slow_queries": 0,
              "max_housekeeper_delete": 5000,
              "proxy_config_frequency": 3600,
              "proxy_data_frequency": 1,
              "start_alerters": 3,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_escalators": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_lld_processors": 2,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_proxy_pollers": 0,
              "start_timers": 1,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "stats_allowed_ip": "",
              "timeout": 4,
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_cipher_all": "",
              "tls_cipher_cert_13": "",
              "tls_cipher_psk": "",
              "tls_cipherall_13": "",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tlsciphercert": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "trend_cache#": "4M",
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "value_cache#": "8M",
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "web": {
              "db_cipher_list": "",
              "db_double_ieee754": true,
              "db_encryption": false,
              "db_verify_host": false,
              "deny_gui_access": false,
              "enable_web_access_log": true,
              "gui_access_ip_range": "",
              "gui_warning_msg": "Zabbix is under maintenance.",
              "history_storage_types": "",
              "max_execution_time": 300,
              "max_input_time": 300,
              "memory_limit": "128M",
              "post_max#": "16M",
              "server_name": "Kubernetes installation",
              "session_name": "zbx_sessionid",
              "sso_settings": "",
              "timezone": "Europe/Riga",
              "upload_max_filesize": "2M"
            },
            "web#": 2,
            "web_enable_route": true,
            "zabbix_mysql_volumeclaim": "mysql-volume-claim",
            "zabbix_mysqlsecret": "mysql-secrets"
          }
        },
        {
          "kind": "ZabbixAppliance",
          "metadata": {
            "labels": {
              "app": "appliance",
              "vendor": "zabbix"
            },
            "name": "zabbix-appliance"
          },
          "spec": {
            "java_gateway": {
              "debug_level": "info",
              "start_pollers": 5,
              "timeout": 3
            },
            "server": {
              "cache#": "8M",
              "cache_update_frequency": 60,
              "db_tls_cipher": "",
              "db_tls_cipher13": "",
              "db_tls_cipher_13": "",
              "db_tls_connect": "",
              "debug_level": 3,
              "export_file#": "",
              "history_cache#": "16M",
              "history_index_cache#": "4M",
              "history_storage_date_index": true,
              "history_storage_types": "",
              "housekeeping_frequency": 1,
              "load_module": "",
              "log_slow_queries": 0,
              "max_housekeeper_delete": 5000,
              "proxy_config_frequency": 3600,
              "proxy_data_frequency": 1,
              "start_alerters": 3,
              "start_db_syncers": 4,
              "start_discoverers": 1,
              "start_escalators": 1,
              "start_http_pollers": 1,
              "start_ipmi_pollers": 0,
              "start_java_pollers": 5,
              "start_lld_processors": 2,
              "start_pingers": 1,
              "start_pollers": 5,
              "start_pollers_unreachable": 1,
              "start_preprocessors": 3,
              "start_proxy_pollers": 0,
              "start_timers": 1,
              "start_trappers": 5,
              "start_vmware_collectors": 0,
              "stats_allowed_ip": "",
              "timeout": 4,
              "tls_ca_file_name": "",
              "tls_cert_file_name": "",
              "tls_crl_file_name": "",
              "tls_key_file_name": "",
              "tlscipherall": "",
              "tlscipherall13": "",
              "tlsciphercert": "",
              "tlsciphercert13": "",
              "tlscipherpsk": "",
              "tlscipherpsk13": "",
              "trapper_timeout": 300,
              "trend_cache#": "4M",
              "unavailable_delay": 60,
              "unreachable_delay": 15,
              "unreachable_period": 45,
              "value_cache#": "8M",
              "vmware_cache#": "8M",
              "vmware_frequency": 60,
              "vmware_perf_frequency": 60,
              "vmware_timeout": 10
            },
            "web": {
              "db_double_ieee754": true,
              "deny_gui_access": false,
              "enable_web_access_log": true,
              "gui_access_ip_range": "",
              "gui_warning_msg": "Zabbix is under maintenance.",
              "history_storage_types": "",
              "max_execution_time": 300,
              "max_input_time": 300,
              "memory_limit": "128M",
              "post_max#": "16M",
              "server_name": "Kubernetes installation",
              "session_name": "zbx_sessionid",
              "sso_settings": "",
              "timezone": "Europe/Riga",
              "upload_max_filesize": "2M"
            },
            "web_enable_route": true,
            "zabbix_mysql_volumeclaim": "mysql-volume-claim",
            "zabbix_mysqlsecret": "mysql-secrets"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/zabbix/zabbixoperator-certified-bundle@sha256:236d7cc772634f20540dc153f9a7fc66357d7d4852962c5c7b5daff61c99e5c8",
      "bundle_path_digest": "sha256:236d7cc772634f20540dc153f9a7fc66357d7d4852962c5c7b5daff61c99e5c8",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "lts",
      "creation_date": "2021-08-20T14:33:51.487000+00:00",
      "csv_description": "## About this Operator\n\nZabbix helps you to real-time monitoring of millions of metrics collected from tens of thousands of servers, virtual machines and network devices.\nThe Zabbix Operator allows users to easily deploy, manage, and maintain Zabbix deployments on OpenShift. By installing this integration you will be able to deploy Zabbix server / proxies and other components with a single command.\n## Supported Features\n* **Zabbix Server** - Simple Zabbix installation with included Zabbix server, Zabbix web-interface and Zabbix Java Gateway with MySQL database support. The feature does not provide MySQL service and requires an external MySQL database. \n* **Zabbix Server (Full)** - Zabbix installation with included Zabbix server, Zabbix web-interface, Zabbix Java Gateway and MySQL server instance.\n* **Zabbix proxy (SQLite3)** - Very simple way to gain power of Zabbix proxy. The feature has  SQLite3 support for Zabbix proxies and allow to specify amount of proxies. \n* **Zabbix proxy (MySQL)** - Another option of Zabbix proxy. The option support and deliver MySQL database.\n* **Zabbix agent** - Zabbix agent can be deployed on each available node for stability and performance monitoring on remote nodes. It allows to gather metrics with full automation!\n* **Zabbix Appliance** - Zabbix appliance very simple way to test and check Zabbix features. The option provides all core components in one solution. It includes Zabbix server, Zabbix Java Gateway, Zabbix web-interface and MySQL server in deployment. It is very useful for testing Zabbix features!\n## Prerequisites\nAll deployment options are require additional information during deployment. Please, check the following instructions and provide required configuration:\n* **Zabbix Server** - MySQL database host information and MySQL database credentials in specially formatted *Secret*. Additionally it is possible to specify SSL certificates for HTTPS support in *Secret*.\n* **Zabbix Server (Full)** - MySQL database credentials in specially formatted *Secret*. MySQL database volume name information. Additionally it is possible to specify SSL certificates for HTTPS support in *Secret*.\n* **Zabbix proxy (SQLite3)** - Zabbix server host information only.\n* **Zabbix proxy (MySQL)** - MySQL database credentials in specially formatted *Secrets* and Zabbix server host information.\n* **Zabbix agent** - Zabbix server host information only for outgoing and incoming connections. Zabbix agent uses \"privileged\" mode to monitor node resources! For example, running processes.\n* **Zabbix Appliance** - MySQL database credentials in specially formatted *Secret*. MySQL database volume name information. Additionally it is possible to specify SSL certificates for HTTPS support in *Secret*.\n### MySQL credentials\n```\nkind: Secret\napiVersion: v1\nmetadata:\n  name: zabbix-full-secrets\ndata:\n  mysql_root_password: emFiYml4X3N1cGVyX3Jvb3Q= [1]\n  mysql_zabbix_password: emFiYml4X3VzZXJfcGFzc3dvcmQ= [2]\n  mysql_zabbix_username: emFiYml4 [3]\ntype: Opaque\n```\nUsing MySQL root password (*mysqlrootpassword* [1]) Zabbix server / proxy will try to create MySQL database schema with grant permissions to *mysqlzabbixusername* [2] and *mysqlzabbixpassword* [3].\n### SSL certificates for HTTPS\n```\nkind: Secret\napiVersion: v1\nmetadata:\n  name: zabbix-web-sslsecret\ndata:\n  ssl.crt: >-\n   < ssl.crt data>\n  ssl.key: >-\n\t < ssl.key data >\n  dhparam.pem: >-\n   <  dhparam.pem data >\n```\nFiles *ssl.crt*, *ssl.key* and *dhparam.perm* are required for Zabbix web-interface for SSL support.\n",
      "csv_display_name": "Zabbix Operator",
      "csv_metadata_description": "Zabbix operator with multiple deployment variants and different components",
      "csv_name": "zabbix-operator-certified.v0.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:02:28.044000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "zabbix-operator-certified",
      "provided_apis": [
        {
          "group": "kubernetes.zabbix.com",
          "kind": "ZabbixProxyMysql",
          "version": "v1alpha1"
        },
        {
          "group": "kubernetes.zabbix.com",
          "kind": "ZabbixProxySqlite",
          "version": "v1alpha1"
        },
        {
          "group": "kubernetes.zabbix.com",
          "kind": "ZabbixServer",
          "version": "v1alpha1"
        },
        {
          "group": "kubernetes.zabbix.com",
          "kind": "ZabbixAgent",
          "version": "v1alpha1"
        },
        {
          "group": "kubernetes.zabbix.com",
          "kind": "ZabbixAppliance",
          "version": "v1alpha1"
        },
        {
          "group": "kubernetes.zabbix.com",
          "kind": "ZabbixFull",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:c2dee633a667191d3272bf1652412ac1077455f18b90168ddcaa2ac8882f0c66",
          "image": "registry.connect.redhat.com/zabbix/zabbixoperator-certified@sha256:c2dee633a667191d3272bf1652412ac1077455f18b90168ddcaa2ac8882f0c66",
          "name": "zabbixoperator-certified-c2dee633a667191d3272bf1652412ac1077455f18b90168ddcaa2ac8882f0c66-annotation"
        },
        {
          "digest": "sha256:c2dee633a667191d3272bf1652412ac1077455f18b90168ddcaa2ac8882f0c66",
          "image": "registry.connect.redhat.com/zabbix/zabbixoperator-certified@sha256:c2dee633a667191d3272bf1652412ac1077455f18b90168ddcaa2ac8882f0c66",
          "name": "ansible"
        },
        {
          "digest": "sha256:c2dee633a667191d3272bf1652412ac1077455f18b90168ddcaa2ac8882f0c66",
          "image": "registry.connect.redhat.com/zabbix/zabbixoperator-certified@sha256:c2dee633a667191d3272bf1652412ac1077455f18b90168ddcaa2ac8882f0c66",
          "name": "operator"
        },
        {
          "digest": "sha256:2f76e7cbece9d9366b613bcb1079b030ddbcc5e97b2f133b73bd0131f0869725",
          "image": "registry.connect.redhat.com/zabbix/zabbix-server-mysql-50@sha256:2f76e7cbece9d9366b613bcb1079b030ddbcc5e97b2f133b73bd0131f0869725",
          "name": "zbx_server_mysql"
        },
        {
          "digest": "sha256:9f3ba82445ea3d9754f016e21b2ea1c99c1958da928f49aee1b18377af78d365",
          "image": "registry.connect.redhat.com/zabbix/zabbix-web-mysql-50@sha256:9f3ba82445ea3d9754f016e21b2ea1c99c1958da928f49aee1b18377af78d365",
          "name": "zbx_web_mysql"
        },
        {
          "digest": "sha256:a216de448dc10b2797a70f4fc6664c2bd611840b7b6c26f066635d48c606bf06",
          "image": "registry.connect.redhat.com/zabbix/zabbix-java-gateway-50@sha256:a216de448dc10b2797a70f4fc6664c2bd611840b7b6c26f066635d48c606bf06",
          "name": "zbx_java_gateway"
        },
        {
          "digest": "sha256:054ad28ade616642fb50fd38a48df2d00a9c0c6a57df1e367caeceb6469f6c42",
          "image": "registry.connect.redhat.com/zabbix/zabbix-proxy-mysql-50@sha256:054ad28ade616642fb50fd38a48df2d00a9c0c6a57df1e367caeceb6469f6c42",
          "name": "zbx_proxy_mysql"
        },
        {
          "digest": "sha256:713a6223a01ee57efb719d6f491ef779d74104a1bc1b8599eade8b3f4a3318ed",
          "image": "registry.connect.redhat.com/zabbix/zabbix-proxy-sqlite-50@sha256:713a6223a01ee57efb719d6f491ef779d74104a1bc1b8599eade8b3f4a3318ed",
          "name": "zbx_proxy_sqlite"
        },
        {
          "digest": "sha256:229562a27ed45319ef17397fe81cac81129c84eccd2c14f6085531a86ef9b87d",
          "image": "registry.connect.redhat.com/zabbix/zabbix-agent-50@sha256:229562a27ed45319ef17397fe81cac81129c84eccd2c14f6085531a86ef9b87d",
          "name": "zbx_agent"
        },
        {
          "digest": "sha256:7236c68c494f572edf64ced85c3a083b7eafff1c6562e95bdc50e50c37e3e3bf",
          "image": "registry.connect.redhat.com/zabbix/zabbix-appliance-50@sha256:7236c68c494f572edf64ced85c3a083b7eafff1c6562e95bdc50e50c37e3e3bf",
          "name": "zbx_appliance"
        },
        {
          "digest": "sha256:e7541d9fbcb7a71ac8a68cc4efc1806bab74023961c7d3fb562ded320df90c43",
          "image": "registry.redhat.io/rhel8/mysql-80@sha256:e7541d9fbcb7a71ac8a68cc4efc1806bab74023961c7d3fb562ded320df90c43",
          "name": "db_mysql"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.0.2",
      "version_original": "0.0.2"
    },
    {
      "_id": "611fbd53eb34b73652492dc9",
      "alm_examples": [
        {
          "kind": "Neuvector",
          "metadata": {
            "name": "neuvector"
          },
          "spec": {
            "admissionwebhook": {
              "type": "ClusterIP"
            },
            "bottlerocket": {
              "enabled": false,
              "runtimePath": "/run/dockershim.sock"
            },
            "containerd": {
              "enabled": false,
              "path": "/var/run/containerd/containerd.sock"
            },
            "controller": {
              "apisvc": {
                "annotations": {},
                "route": {
                  "enabled": false,
                  "host": "",
                  "termination": "passthrough"
                },
                "type": ""
              },
              "azureFileShare": {
                "enabled": false,
                "secretName": "",
                "shareName": ""
              },
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "configmap": {
                "data": "",
                "enabled": false
              },
              "disruptionbudget": 0,
              "enabled": true,
              "federation": {
                "managedsvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                },
                "mastersvc": {
                  "ingress": {
                    "annotations": {
                      "ingress.kubernetes.io/protocol": "https"
                    },
                    "enabled": false,
                    "host": "",
                    "path": "/",
                    "secretName": "",
                    "tls": false
                  },
                  "route": {
                    "enabled": false,
                    "host": "",
                    "termination": "passthrough"
                  },
                  "type": ""
                }
              },
              "image": "registry.connect.redhat.com/neuvector/controller",
              "ingress": {
                "annotations": {
                  "ingress.kubernetes.io/protocol": "https"
                },
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "pvc": {
                "accessModes": [
                  "ReadWriteMany"
                ],
                "capacity": "",
                "enabled": false,
                "storageClass": ""
              },
              "replicas": 3,
              "resources": {},
              "strategy": {
                "rollingUpdate": {
                  "maxSurge": 1,
                  "maxUnavailable": 0
                },
                "type": "RollingUpdate"
              }
            },
            "crdwebhook": {
              "enabled": true,
              "type": "ClusterIP"
            },
            "crio": {
              "enabled": true,
              "path": "/var/run/crio/crio.sock"
            },
            "cve": {
              "scanner": {
                "dockerPath": "",
                "enabled": true,
                "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
                "priorityClassName": "",
                "replicas": 3,
                "resources": {},
                "strategy": {
                  "rollingUpdate": {
                    "maxSurge": 1,
                    "maxUnavailable": 0
                  },
                  "type": "RollingUpdate"
                }
              },
              "updater": {
                "enabled": true,
                "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
                "priorityClassName": "",
                "schedule": "0 0 * * *"
              }
            },
            "docker": {
              "enabled": false,
              "path": "/var/run/docker.sock"
            },
            "enforcer": {
              "enabled": true,
              "image": "registry.connect.redhat.com/neuvector/enforcer",
              "priorityClassName": "",
              "resources": {},
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master"
                }
              ]
            },
            "k3s": {
              "enabled": false,
              "runtimePath": "/run/k3s/containerd/containerd.sock"
            },
            "manager": {
              "certificate": {
                "keyFile": "tls.key",
                "pemFile": "tls.pem",
                "secret": ""
              },
              "enabled": true,
              "env": {
                "ssl": true
              },
              "image": "registry.connect.redhat.com/neuvector/manager",
              "ingress": {
                "annotations": {},
                "enabled": false,
                "host": "",
                "path": "/",
                "secretName": "",
                "tls": false
              },
              "priorityClassName": "",
              "resources": {},
              "route": {
                "enabled": true,
                "host": "",
                "termination": "passthrough"
              },
              "svc": {
                "annotations": {},
                "loadBalancerIP": "",
                "type": "NodePort"
              }
            },
            "openshift": true,
            "psp": false,
            "resources": {},
            "serviceAccount": "default"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/neuvector/neuvector-operator-bundle@sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "bundle_path_digest": "sha256:6f4ff463c67af42c1c3603e36e512a60dedb95775380a1a6b26c4cded86de5e1",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-20T14:33:55.567000+00:00",
      "csv_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.\n\nThe NeuVector Operator runs  in the openshift container platform to deploy and manage the NeuVector Security cluster components. The NeuVector operator contains all necessary information to deploy NeuVector using helm charts. You simply need to install the NeuVector operator from the OpenShift embeded operator hub and create NeuVector instance. You can modify the NeuVector installation configuration by modifying yaml while creating the NeuVector instance such as imagePullSecrets, tag version, etc. Please refer to [github link](https://github.com/neuvector/neuvector-helm/tree/master/charts/core) for the values that can be modifed during installation. To upgrade to a newer version of NeuVector, just reapply the NeuVector instance with desired tag , which in turn pulls the specified NeuVector image tags and upgrades as per upgrade plan configured on the helm chart.  NeuVector Operator versions are tied to NeuVector product versions. Version 1.2.8 of the NeuVector Certified Operator deploys version 4.3.1 of NeuVector.\n\n**Complete below steps to Grant Service Account Access to the Privileged SCC before installation.**\n\nCreate the NeuVector namespace\n\n         oc new-project  neuvector\nLogin as system:admin account\n\n         oc login -u system:admin\n\nGrant Service Account Access to the Privileged SCC\n\n         oc -n neuvector adm policy add-scc-to-user privileged -z default\n\nThe following info will be added in the Privileged SCC users:\n\n         - system:serviceaccount:neuvector:default\n\nIn OpenShift 4.6+ use the following to check:\n\n         oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide\n         NAME                              ROLE                                          AGE     USERS   GROUPS   SERVICEACCOUNTS\n         system:openshift:scc:privileged   ClusterRole/system:openshift:scc:privileged   9m22s                    neuvector/default\n\n\n**Add NeuVector license from NeuVector WebUI->setting**\n\n\n#Deploying the NeuVector Operator#\n\n\nPlease refer to the instructions [here](https://github.com/neuvector/neuvector-operator/blob/master/README.md)\n\n\n",
      "csv_display_name": "NeuVector Operator",
      "csv_metadata_description": "NeuVector delivers the only cloud-native Kubernetes security platform with uncompromising end-to-end protection from DevOps vulnerability protection to automated run-time security, and featuring a true Layer 7 container firewall.",
      "csv_name": "neuvector-operator.v1.2.8",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:59:35.734000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "neuvector-certified-operator",
      "provided_apis": [
        {
          "group": "apm.neuvector.com",
          "kind": "Neuvector",
          "plural": "neuvectors",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator-9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72-annotation"
        },
        {
          "digest": "sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "image": "registry.connect.redhat.com/neuvector/neuvector-operator@sha256:9d8ab5fc5e3122fe1332ccf624e6897277e2e935690f4b07ca1c491599daec72",
          "name": "neuvector-operator"
        },
        {
          "digest": "sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "image": "registry.connect.redhat.com/neuvector/controller@sha256:55b3d9873846d0a8670b58e8a8a76c426d3aba74d4e5c4fca43d05c1dd296f10",
          "name": "controller"
        },
        {
          "digest": "sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "image": "registry.connect.redhat.com/neuvector/enforcer@sha256:fdd66288454dd01f4f618c8cf04c7da501d4362773266ec6f448d63e26e3a35c",
          "name": "enforcer"
        },
        {
          "digest": "sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "image": "registry.connect.redhat.com/neuvector/manager@sha256:73445c18ea18f131b6fa630a97623a3043d05758205ef93fc02fa920abbedaa6",
          "name": "manager"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "updater"
        },
        {
          "digest": "sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "image": "registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06",
          "name": "scanner-a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06-annotation"
        },
        {
          "digest": "sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "image": "registry.access.redhat.com/ubi8@sha256:091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49",
          "name": "ubi8-091ad37a5a638af2c21d01c2d3f4d489c2368070a6c43371e897013fb0987e49-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.2.8",
      "version_original": "1.2.8"
    },
    {
      "_id": "611fbd60eb34b73652492dd1",
      "alm_examples": [
        {
          "kind": "OpenshiftArtifactoryHa",
          "metadata": {
            "name": "openshiftartifactoryha"
          },
          "spec": {
            "artifactory-ha": {
              "artifactory": {
                "image": {
                  "repository": "registry.connect.redhat.com/jfrog/artifactory-pro",
                  "version": "7.4.3"
                },
                "joinKey": "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE",
                "masterKey": "FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF",
                "node": {
                  "replicaCount": 2,
                  "waitForPrimaryStartup": {
                    "enabled": false
                  }
                }
              },
              "database": {
                "driver": "OVERRIDE",
                "password": "OVERRIDE",
                "type": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "initContainerImage": "registry.redhat.io/ubi8-minimal",
              "nginx": {
                "http": {
                  "externalPort": 80,
                  "internalPort": 8080
                },
                "https": {
                  "externalPort": 443,
                  "internalPort": 8443
                },
                "image": {
                  "repository": "registry.redhat.io/rhel8/nginx-116",
                  "version": "latest"
                },
                "service": {
                  "ssloffload": false
                },
                "tlsSecretName": "OVERRIDE"
              },
              "postgresql": {
                "enabled": false
              },
              "waitForDatabase": false
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/jfrog/artifactory-operator-bundle@sha256:5526e86dd60790139a6620e2464cb3450091dd0e6941600e9dab66a6fc79012e",
      "bundle_path_digest": "sha256:5526e86dd60790139a6620e2464cb3450091dd0e6941600e9dab66a6fc79012e",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:34:08.391000+00:00",
      "csv_description": "Openshift 4 Operator to deploy JFrog Artifactory Enterprise",
      "csv_display_name": "JFrog Artifactory Enterprise Operator",
      "csv_metadata_description": "JFrog Artifactory Enterprise deploys Artifactory in a high availability environment across multiple pods",
      "csv_name": "artifactory-ha-operator.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:55:11.952000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "openshiftartifactoryha-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftArtifactoryHa",
          "plural": "openshiftartifactoryhas",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:a5e9e99889cbe2c4ee96c80d908e80987371b66f9d99c6c6c6155583a9ef505a",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a5e9e99889cbe2c4ee96c80d908e80987371b66f9d99c6c6c6155583a9ef505a",
          "name": "artifactory-operator-a5e9e99889cbe2c4ee96c80d908e80987371b66f9d99c6c6c6155583a9ef505a-annotation"
        },
        {
          "digest": "sha256:a5e9e99889cbe2c4ee96c80d908e80987371b66f9d99c6c6c6155583a9ef505a",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:a5e9e99889cbe2c4ee96c80d908e80987371b66f9d99c6c6c6155583a9ef505a",
          "name": "artifactory-ha-operator"
        },
        {
          "digest": "sha256:97eb6bd2639523ec5f8f7d7e87953ceda515244ef2e7ee4bef08f7eb19faa7ca",
          "image": "registry.connect.redhat.com/jfrog/artifactory-pro@sha256:97eb6bd2639523ec5f8f7d7e87953ceda515244ef2e7ee4bef08f7eb19faa7ca",
          "name": "artifactory_image_repository"
        },
        {
          "digest": "sha256:0ba76a7b26e5ffb95b4354243337ac2b3ff84ae8637c0782631084d1b2f99a33",
          "image": "registry.redhat.io/rhel8/nginx-116@sha256:0ba76a7b26e5ffb95b4354243337ac2b3ff84ae8637c0782631084d1b2f99a33",
          "name": "nginx_image_repository"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "611fbd69bd674341b5c5f10b",
      "alm_examples": [
        {
          "kind": "AciContainersOperator",
          "metadata": {
            "name": "acicnioperator",
            "namespace": "aci-containers-system"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/noiro/aci-operator-bundle@sha256:246f4479569bff74a54ffb0f19b6fec8f99839d0629c7ef9bf2f1c8ea2316e65",
      "bundle_path_digest": "sha256:246f4479569bff74a54ffb0f19b6fec8f99839d0629c7ef9bf2f1c8ea2316e65",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:34:17.721000+00:00",
      "csv_description": "## ACI CNI Plugin\n\nThe Cisco Application Centric Infrastructure (ACI) CNI plugin brings the ACI Networking and Policy model to Kubernetes clusters that reside on-prem or in the cloud. It is fully open source and relies on the Opflex Protocol to program Open vSwitch instances running on the Kubernetes nodes. It provides IP address management, L2/L3 networking, load balancing, and security functions for container workloads.\n\nFeatures of Cisco ACI?\nLinks to learn more about the Plugin?\n## ACI Containers Operator\n\nHow does the Operator work?\nWhat does the Operator do?\nLinks to learn more about the Operator?\n## Prerequisites for enabling this Operator\n\nPlease describe any steps a user needs to take prior to enabling this Operator (e.g. any Secrets or ConfigMaps that need to be in place upfront).\n",
      "csv_display_name": "Cisco ACI Containers Operator",
      "csv_metadata_description": "The Cisco Application Centric Infrastructure (ACI) Containers Operator brings the ACI CNI Plugin and Policy model to Kubernetes clusters.",
      "csv_name": "aci-containers-operator.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:54:44.511000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "aci-containers-operator",
      "provided_apis": [
        {
          "group": "aci.ctrl",
          "kind": "AciContainersOperator",
          "plural": "acicontainersoperators",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:0db556fc8f188159fbb18797d8fcbd96303176d7803acaaf6bf604de8dcc0fd0",
          "image": "quay.io/noiro/aci-containers-operator@sha256:0db556fc8f188159fbb18797d8fcbd96303176d7803acaaf6bf604de8dcc0fd0",
          "name": "aci-containers-operator-0db556fc8f188159fbb18797d8fcbd96303176d7803acaaf6bf604de8dcc0fd0-annotation"
        },
        {
          "digest": "sha256:0db556fc8f188159fbb18797d8fcbd96303176d7803acaaf6bf604de8dcc0fd0",
          "image": "quay.io/noiro/aci-containers-operator@sha256:0db556fc8f188159fbb18797d8fcbd96303176d7803acaaf6bf604de8dcc0fd0",
          "name": "aci-containers-operator"
        },
        {
          "digest": "sha256:0fc4c11704dd26a3b10e73cbffca34addc1a968e7b55032533717c70696db2ab",
          "image": "quay.io/noiro/acc-provision-operator@sha256:0fc4c11704dd26a3b10e73cbffca34addc1a968e7b55032533717c70696db2ab",
          "name": "acc-provision-operator"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "611fbd6f9f14588c41ebe903",
      "alm_examples": [
        {
          "kind": "Tenant",
          "metadata": {
            "name": "minio"
          },
          "spec": {
            "console": {
              "consoleSecret": {
                "name": "console-secret"
              },
              "replicas": 2
            },
            "credsSecret": {
              "name": "minio-creds-secret"
            },
            "pools": {
              "servers": 4,
              "volumeClaimTemplate": {
                "metadata": {
                  "name": "data"
                },
                "spec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Ti"
                    }
                  },
                  "storageClassName": "local-storage"
                }
              },
              "volumesPerServer": 4
            },
            "requestAutoCert": false
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/minio/minio-operator1@sha256:357a5089b211e9653efff6cacc433633993cf3317dcca29eb54f924374b47b88",
      "bundle_path_digest": "sha256:357a5089b211e9653efff6cacc433633993cf3317dcca29eb54f924374b47b88",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:34:23.984000+00:00",
      "csv_description": "## About MinIO\nMinIO object storage platform enables building high performance data infrastructure for machine learning, analytics and application data workloads. MinIO is Enterprise-Grade, Amazon S3 Compatible object storage. Some of the key features are:\n* SQL Select\n* Encryption & WORM\n* Lambda Compute\n* Integration with IAM Tools\n* Erasure Code & Bitrot Protection\n## About MinIO Operator\nMinIO Operator provides the following features:\n* Deploy: Launch MinIO Clusters with specific affinity, toleration and other pre-defined settings.\n* Expansion: Expand the existing tenant and increase the storage capacity.\n* Upgrade: Seamless upgrade of MinIO Tenant to new release as required.\n* Console: Graphical UI to manage MinIO Tenant.\n* Automatic TLS: Deploy TLS enabled MinIO clusters with builtin certificate signing requests.\n",
      "csv_display_name": "MinIO Operator",
      "csv_metadata_description": " MinIO Operator allows creating distributed MinIO Clusters and manage their lifecycle",
      "csv_name": "minio-operator.v4.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:59:27.803000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "minio-operator",
      "provided_apis": [
        {
          "group": "minio.min.io",
          "kind": "Tenant",
          "plural": "tenants",
          "version": "v2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:b30d13b802cd653b25d3a523f103a91fec67ff6b10c443abb389f31e4aade07e",
          "image": "minio/operator@sha256:b30d13b802cd653b25d3a523f103a91fec67ff6b10c443abb389f31e4aade07e",
          "name": "operator-b30d13b802cd653b25d3a523f103a91fec67ff6b10c443abb389f31e4aade07e-annotation"
        },
        {
          "digest": "sha256:0cbfee2437c988997fb12fcfdde075cbc46ff81f7a58cd8ae3fdbd529dd492a6",
          "image": "docker.io/minio/operator@sha256:0cbfee2437c988997fb12fcfdde075cbc46ff81f7a58cd8ae3fdbd529dd492a6",
          "name": "minio-operator"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "4.0.9",
      "version_original": "4.0.9"
    },
    {
      "_id": "611fbd72bd674341b5c5f10e",
      "alm_examples": [
        {
          "kind": "AnzoUnstructured",
          "metadata": {
            "name": "au01"
          },
          "spec": {
            "auWorker": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app": "anzounstructured"
                    }
                  },
                  "serviceName": "au-au01-w",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app": "anzounstructured"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:4db61bdf6dc8b42824a06d2f8d8aeecddde765d123b40a63a7bbf87845719fb2",
                          "name": "w",
                          "resources": {
                            "limits": {
                              "cpu": "2",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "unstructured-operator"
                    }
                  }
                }
              }
            },
            "msLeader": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app": "anzounstructured"
                    }
                  },
                  "serviceName": "au-au01-ms",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app": "anzounstructured"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-microservices-leader@sha256:0579fd97e81e436eaff3b58c5395f13aa924940a3d51f0bdea5163d327620286",
                          "name": "ms",
                          "resources": {
                            "limits": {
                              "cpu": "2",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "unstructured-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-operator-bundle@sha256:800c686058a8b80d2c082741019db9d35535b2ba2b59007cbde0c2dffa143e82",
      "bundle_path_digest": "sha256:800c686058a8b80d2c082741019db9d35535b2ba2b59007cbde0c2dffa143e82",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:34:26.997000+00:00",
      "csv_description": "The Anzo Unstructured Operator provides the way to install and configure an anzo unstructured setup on Red Hat K8S environment.\nCurrently, this is possible only through existing Anzo installation.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzo-unstructured/blob/v2.0.0/README.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzo/userdoc/cloud-deployments.htm )\n\n### Support\n\nWe offer Support to our customers through [ Cambridge Semantics Customer Center ]( https://customercenter.cambridgesemantics.com/ ).",
      "csv_display_name": "Anzo Unstructured Operator",
      "csv_metadata_description": "Kubernetes Operator for Anzo Unstructured",
      "csv_name": "anzounstructured-operator.v2.0.101",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:55:05.915000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "anzounstructured-operator",
      "provided_apis": [
        {
          "group": "anzounstructured.clusters.cambridgesemantics.com",
          "kind": "AnzoUnstructured",
          "plural": "anzounstructureds",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:a47af068a7aa55c12be84c80d263feeed5597ecc434986b4f2fff782b568bb04",
          "image": "registry.connect.redhat.com/cambridgesemantics/unstructured-operator@sha256:a47af068a7aa55c12be84c80d263feeed5597ecc434986b4f2fff782b568bb04",
          "name": "unstructured-operator-a47af068a7aa55c12be84c80d263feeed5597ecc434986b4f2fff782b568bb04-annotation"
        },
        {
          "digest": "sha256:a47af068a7aa55c12be84c80d263feeed5597ecc434986b4f2fff782b568bb04",
          "image": "registry.connect.redhat.com/cambridgesemantics/unstructured-operator@sha256:a47af068a7aa55c12be84c80d263feeed5597ecc434986b4f2fff782b568bb04",
          "name": "manager"
        },
        {
          "digest": "sha256:0579fd97e81e436eaff3b58c5395f13aa924940a3d51f0bdea5163d327620286",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-microservices-leader@sha256:0579fd97e81e436eaff3b58c5395f13aa924940a3d51f0bdea5163d327620286",
          "name": "anzo_microservices_leader"
        },
        {
          "digest": "sha256:4db61bdf6dc8b42824a06d2f8d8aeecddde765d123b40a63a7bbf87845719fb2",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:4db61bdf6dc8b42824a06d2f8d8aeecddde765d123b40a63a7bbf87845719fb2",
          "name": "anzo_unstructured_worker"
        },
        {
          "digest": "sha256:0579fd97e81e436eaff3b58c5395f13aa924940a3d51f0bdea5163d327620286",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-microservices-leader@sha256:0579fd97e81e436eaff3b58c5395f13aa924940a3d51f0bdea5163d327620286",
          "name": "anzo-microservices-leader-0579fd97e81e436eaff3b58c5395f13aa924940a3d51f0bdea5163d327620286-annotation"
        },
        {
          "digest": "sha256:4db61bdf6dc8b42824a06d2f8d8aeecddde765d123b40a63a7bbf87845719fb2",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:4db61bdf6dc8b42824a06d2f8d8aeecddde765d123b40a63a7bbf87845719fb2",
          "name": "anzo-unstructured-worker-4db61bdf6dc8b42824a06d2f8d8aeecddde765d123b40a63a7bbf87845719fb2-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "611fbd75bd674341b5c5f10f",
      "alm_examples": [
        {
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "daemonsets": {
              "priorityClassName": "system-node-critical",
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "nvidia.com/gpu",
                  "operator": "Exists"
                }
              ]
            },
            "dcgm": {
              "enabled": true,
              "hostPort": 5555,
              "image": "dcgm",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c"
            },
            "dcgmExporter": {
              "config": {
                "name": ""
              },
              "env": [
                {
                  "name": "DCGM_EXPORTER_LISTEN",
                  "value": ":9400"
                },
                {
                  "name": "DCGM_EXPORTER_KUBERNETES",
                  "value": "true"
                },
                {
                  "name": "DCGM_EXPORTER_COLLECTORS",
                  "value": "/etc/dcgm-exporter/dcp-metrics-included.csv"
                }
              ],
              "image": "dcgm-exporter",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff"
            },
            "devicePlugin": {
              "args": [],
              "env": [
                {
                  "name": "PASS_DEVICE_SPECS",
                  "value": "true"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                },
                {
                  "name": "DEVICE_LIST_STRATEGY",
                  "value": "envvar"
                },
                {
                  "name": "DEVICE_ID_STRATEGY",
                  "value": "uuid"
                },
                {
                  "name": "NVIDIA_VISIBLE_DEVICES",
                  "value": "all"
                },
                {
                  "name": "NVIDIA_DRIVER_CAPABILITIES",
                  "value": "all"
                }
              ],
              "image": "k8s-device-plugin",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05"
            },
            "driver": {
              "enabled": true,
              "image": "driver",
              "imagePullSecrets": [],
              "licensingConfig": {
                "configMapName": ""
              },
              "manager": {
                "env": [
                  {
                    "name": "DRAIN_USE_FORCE",
                    "value": "false"
                  },
                  {
                    "name": "DRAIN_POD_SELECTOR_LABEL",
                    "value": ""
                  },
                  {
                    "name": "DRAIN_TIMEOUT_SECONDS",
                    "value": "0s"
                  },
                  {
                    "name": "DRAIN_DELETE_EMPTYDIR_DATA",
                    "value": "false"
                  }
                ],
                "image": "k8s-driver-manager",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia/cloud-native",
                "version": "sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b"
              },
              "nodeSelector": {
                "nvidia.com/gpu.deploy.driver": "true"
              },
              "podSecurityContext": {},
              "rdma": {
                "enabled": false
              },
              "repoConfig": {
                "configMapName": "",
                "destinationDir": ""
              },
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
              "virtualTopology": {
                "config": ""
              }
            },
            "gfd": {
              "env": [
                {
                  "name": "GFD_SLEEP_INTERVAL",
                  "value": "60s"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                }
              ],
              "image": "gpu-feature-discovery",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "version": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f"
            },
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "enabled": true,
              "env": [
                {
                  "name": "WITH_REBOOT",
                  "value": "false"
                }
              ],
              "image": "k8s-mig-manager",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec"
            },
            "nodeStatusExporter": {
              "enabled": true,
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585"
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {
                "image": "cuda",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia",
                "version": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce"
              }
            },
            "toolkit": {
              "enabled": true,
              "image": "container-toolkit",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "version": "sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703"
            },
            "validator": {
              "env": [
                {
                  "name": "WITH_WORKLOAD",
                  "value": "true"
                }
              ],
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "version": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:949e1d767985e99cde04a070667ba0b81e8422e71555c0c233c528fd746667d3",
      "bundle_path_digest": "sha256:949e1d767985e99cde04a070667ba0b81e8422e71555c0c233c528fd746667d3",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.8",
      "creation_date": "2021-08-20T14:34:29.087000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/index.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-05T10:57:12.751000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:3a0ff79c4edc24de708f82e5f9dc2946897aa4a50e8827203221ee2281008974",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:3a0ff79c4edc24de708f82e5f9dc2946897aa4a50e8827203221ee2281008974",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:e37404194fa2bc2275827411049422b93d1493991fb925957f170b4b842846ff",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c",
          "image": "nvcr.io/nvidia/cloud-native/dcgm@sha256:28f334d6d5ca6e5cad2cf05a255989834128c952e3c181e6861bd033476d4b2c",
          "name": "dcgm-image"
        },
        {
          "digest": "sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:8f9517b4c83b8730c40134df385088be41519b585176c66727ff6f181ae5e703",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
          "image": "nvcr.io/nvidia/driver@sha256:a62de5e843a41c65cf837e7db5f5b675d03fa2de05e981a859b114336cf183e3",
          "name": "driver-image"
        },
        {
          "digest": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:77b8e58a54c222bee3cc56b2305d4cebfa60722c122858f94301e611f87d7fec",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "image": "nvcr.io/nvidia/cuda@sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:1cce434a1722288bacab5eaa5c194ca2bdbad55679ba871a2814556853339585",
          "name": "gpu-operator-validator-image"
        },
        {
          "digest": "sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b",
          "image": "nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:907ab0fc008bb90149ed059ac3a8ed3d19ae010d52c58c0ddbafce45df468d5b",
          "name": "k8s-driver-manager-image"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.8.0",
      "version_original": "1.8.0"
    },
    {
      "_id": "611fbd829f14588c41ebe90e",
      "alm_examples": [
        {
          "kind": "CloudBeesCI",
          "metadata": {
            "name": "cloudbeesci-sample"
          },
          "spec": {
            "Agents": {
              "Enabled": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-core-agent:2.289.3.2"
              },
              "SeparateNamespace": {
                "Create": false,
                "Enabled": false
              }
            },
            "Hibernation": {
              "Enabled": false,
              "Image": {
                "dockerImage": "cloudbees/managed-master-hibernation-monitor:247.c5dfce00a179"
              },
              "NodeSelector": {},
              "Tolerations": []
            },
            "Master": {
              "Enabled": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-core-mm:2.289.3.2"
              }
            },
            "NetworkPolicy": {
              "Enabled": false,
              "JMXSelectors": [],
              "ingressControllerSelector": []
            },
            "OperationsCenter": {
              "AgentListenerPort": 50000,
              "Annotations": {},
              "CSRF": {
                "ProxyCompatibility": false
              },
              "CasC": {
                "ConfigMapName": "oc-casc-bundle",
                "Enabled": false
              },
              "ContainerPort": 8080,
              "Enabled": true,
              "ExtraConfigMaps": [],
              "ExtraContainers": [],
              "ExtraGroovyConfiguration": {},
              "ExtraVolumeMounts": [],
              "ExtraVolumes": [],
              "HealthProbeLivenessFailureThreshold": 12,
              "HealthProbes": true,
              "Image": {
                "dockerImage": "cloudbees/cloudbees-cloud-core-oc:2.289.3.2"
              },
              "Ingress": {
                "Annotations": {
                  "kubernetes.io/tls-acme": "false"
                },
                "Class": "nginx",
                "tls": {
                  "Enable": false
                }
              },
              "LoadBalancerSourceRanges": [
                "0.0.0.0/0"
              ],
              "Name": "cjoc",
              "NodeSelector": {},
              "Platform": "standard",
              "Protocol": "http",
              "Resources": {
                "Limits": {
                  "Cpu": 1,
                  "Memory": "2G"
                },
                "Requests": {
                  "Cpu": 1,
                  "Memory": "2G"
                }
              },
              "Route": {
                "tls": {
                  "Enable": false,
                  "InsecureEdgeTerminationPolicy": "Redirect",
                  "Termination": "edge"
                }
              },
              "ServiceAgentListenerPort": 50000,
              "ServiceAnnotations": {},
              "ServicePort": 80,
              "ServiceType": "ClusterIP",
              "Tolerations": []
            },
            "Persistence": {
              "AccessMode": "ReadWriteOnce",
              "Annotations": {},
              "Size": "20Gi"
            },
            "PodSecurityPolicy": {
              "Annotations": {},
              "Enabled": false
            },
            "Subdomain": false,
            "ingress-nginx": {
              "Enabled": false,
              "controller": {
                "admissionWebhooks": {
                  "patch": {
                    "nodeSelector": {
                      "kubernetes.io/os": "linux"
                    }
                  }
                },
                "ingressClass": "nginx",
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                },
                "service": {
                  "externalTrafficPolicy": "Local"
                }
              },
              "defaultBackend": {
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                }
              }
            },
            "nginx-ingress": {
              "Enabled": false,
              "controller": {
                "ingressClass": "nginx",
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                },
                "service": {
                  "externalTrafficPolicy": "Local"
                }
              },
              "defaultBackend": {
                "nodeSelector": {
                  "kubernetes.io/os": "linux"
                }
              }
            },
            "rbac": {
              "agentsServiceAccountName": "jenkins-agents",
              "hibernationMonitorServiceAccountName": "managed-master-hibernation-monitor",
              "install": true,
              "masterServiceAccountName": "jenkins",
              "serviceAccountName": "cjoc"
            },
            "sidecarinjector": {
              "Enabled": false
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cloudbees/cloudbees-core-modern@sha256:2cf058c819b40512094f84f4d374457455370ec4994d34cee6e39b9d87841f05",
      "bundle_path_digest": "sha256:2cf058c819b40512094f84f4d374457455370ec4994d34cee6e39b9d87841f05",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:34:42.135000+00:00",
      "csv_description": "CloudBees CI",
      "csv_display_name": "CloudBees CI",
      "csv_metadata_description": "",
      "csv_name": "cloudbees-ci.v3.34.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:55:48.687000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "cloudbees-ci",
      "provided_apis": [
        {
          "group": "charts.cloudbees.com",
          "kind": "CloudBeesCI",
          "plural": "cloudbeescis",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:bd0bd72e9e7807b0fa307de8a93a56967221fe2dd181bac02383f0f9be9120e4",
          "image": "registry.connect.redhat.com/cloudbees/cloudbees-ci-operator@sha256:bd0bd72e9e7807b0fa307de8a93a56967221fe2dd181bac02383f0f9be9120e4",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "3.34.1",
      "version_original": "3.34.1"
    },
    {
      "_id": "611fbdadeb34b73652492dec",
      "alm_examples": [
        {
          "kind": "CitrixIngressController",
          "metadata": {
            "name": "cic"
          },
          "spec": {
            "adcCredentialSecret": "",
            "coeConfig": {
              "distributedTracing": {
                "enable": false,
                "samplingrate": 100
              },
              "endpoint": {
                "server": ""
              },
              "required": false,
              "timeseries": {
                "auditlogs": {
                  "enable": false
                },
                "events": {
                  "enable": false
                },
                "metrics": {
                  "enable": false,
                  "mode": "avro"
                },
                "port": 30002
              },
              "transactions": {
                "enable": false,
                "port": 30001
              }
            },
            "crds": {
              "install": true,
              "retainOnDelete": false
            },
            "defaultSSLCertSecret": "",
            "entityPrefix": "",
            "exporter": {
              "image": "registry.connect.redhat.com/citrix/citrix-adc-metrics-exporter@sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
              "ports": {
                "containerPort": 8888
              },
              "pullPolicy": "IfNotPresent",
              "required": false
            },
            "ignoreNodeExternalIP": false,
            "image": "registry.connect.redhat.com/citrix/citrix-ingress-controller@sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
            "ingressClass": "",
            "ipam": false,
            "kubernetesURL": "",
            "license": {
              "accept": false
            },
            "logLevel": "INFO",
            "logProxy": "",
            "namespaceLabels": "",
            "nodeSelector": {
              "key": "",
              "value": ""
            },
            "nodeWatch": false,
            "nsCookieVersion": "0",
            "nsHTTP2ServerSide": "OFF",
            "nsIP": "x.x.x.x",
            "nsPort": 443,
            "nsProtocol": "HTTPS",
            "nsSNIPS": "",
            "nsVIP": "",
            "openshift": true,
            "podIPsforServiceGroupMembers": false,
            "pullPolicy": "IfNotPresent",
            "routeLabels": "",
            "serviceAccount": {
              "create": true
            },
            "serviceClass": "",
            "setAsDefaultIngressClass": false,
            "tolerations": [],
            "updateIngressStatus": false
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/citrix/citrix-k8s-ingress-bundle@sha256:1cf84c905b444c613f67f5b2a7b80015226b6a99c5723503e87c795a719e6347",
      "bundle_path_digest": "sha256:1cf84c905b444c613f67f5b2a7b80015226b6a99c5723503e87c795a719e6347",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:35:25.690000+00:00",
      "csv_description": "Citrix provides an ingress controller for Citrix ADC MPX (hardware), Citrix ADC VPX (virtualized), and Citrix ADC CPX (containerized) for on-prem and cloud deployments. It automatically configures one or more Citrix ADC based on the Ingress resource configuration. This operator can be used deploy Citrix Ingress Controller in an OpenShift environment.",
      "csv_display_name": "Citrix Ingress Controller",
      "csv_metadata_description": "Citrix provides an ingress controller for Citrix ADC MPX (hardware), Citrix ADC VPX (virtualized), and Citrix ADC CPX (containerized) for on-prem and cloud deployments. It automatically configures one or more Citrix ADC based on the Ingress resource configuration. This operator can be used deploy Citrix Ingress Controller in an Openshift environment.",
      "csv_name": "citrix-ingress-controller-operator.v1.13.20",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:55:43.037000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "citrix-ingress-controller-operator",
      "provided_apis": [
        {
          "group": "citrix.com",
          "kind": "CitrixIngressController",
          "plural": "citrixingresscontrollers",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:a41bf2a59a5678d757c18c2d995e10f035676128726ded4b84d61522c3fca4fa",
          "image": "registry.connect.redhat.com/citrix/citrix-k8s-ingress-controller@sha256:a41bf2a59a5678d757c18c2d995e10f035676128726ded4b84d61522c3fca4fa",
          "name": "citrix-k8s-ingress-controller-a41bf2a59a5678d757c18c2d995e10f035676128726ded4b84d61522c3fca4fa-annotation"
        },
        {
          "digest": "sha256:1fee9f7563afd0878288d60dce2c263a6333b18b40d88898ed0fca92e6e7048f",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:1fee9f7563afd0878288d60dce2c263a6333b18b40d88898ed0fca92e6e7048f",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:a41bf2a59a5678d757c18c2d995e10f035676128726ded4b84d61522c3fca4fa",
          "image": "registry.connect.redhat.com/citrix/citrix-k8s-ingress-controller@sha256:a41bf2a59a5678d757c18c2d995e10f035676128726ded4b84d61522c3fca4fa",
          "name": "manager"
        },
        {
          "digest": "sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
          "image": "registry.connect.redhat.com/citrix/citrix-ingress-controller@sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
          "name": "cic"
        },
        {
          "digest": "sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
          "image": "registry.connect.redhat.com/citrix/citrix-adc-metrics-exporter@sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
          "name": "exporter"
        },
        {
          "digest": "sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
          "image": "registry.connect.redhat.com/citrix/citrix-ingress-controller@sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
          "name": "citrix-ingress-controller-f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0-annotation"
        },
        {
          "digest": "sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
          "image": "registry.connect.redhat.com/citrix/citrix-adc-metrics-exporter@sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
          "name": "citrix-adc-metrics-exporter-c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.13.20",
      "version_original": "1.13.20"
    },
    {
      "_id": "611fbdb2dece23122b7a7881",
      "alm_examples": [
        {
          "kind": "EntandoDeBundle",
          "metadata": {
            "name": "my-bundle",
            "namespace": "my-namespace"
          },
          "spec": {
            "details": {
              "name": "my-bundle"
            }
          }
        },
        {
          "kind": "EntandoDatabaseService",
          "metadata": {
            "name": "my-entando-database-service",
            "namespace": "my-namespace"
          },
          "spec": {
            "createDeployment": true,
            "databaseName": "my_database",
            "dbms": "postgresql"
          }
        },
        {
          "kind": "EntandoKeycloakServer",
          "metadata": {
            "name": "my-keycloak",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "replicas": 1,
            "standardImage": "redhat-sso"
          }
        },
        {
          "kind": "EntandoClusterInfrastructure",
          "metadata": {
            "name": "my-entando-cluster-infrastructure",
            "namespace": "my-namespace"
          },
          "spec": {
            "environmentVariables": [],
            "replicas": 1
          }
        },
        {
          "kind": "EntandoPlugin",
          "metadata": {
            "name": "my-entando-plugin",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "healthCheckPath": "/management/health",
            "image": "entando/entando-avatar-plugin:6.0.5",
            "ingressHostName": "my-plugin.apps-crc.testing",
            "ingressPath": "/avatarPlugin",
            "replicas": 1,
            "roles": [
              {
                "code": "admin",
                "name": "user"
              },
              {
                "code": "user",
                "name": "user"
              }
            ],
            "securityLevel": "strict"
          }
        },
        {
          "kind": "EntandoApp",
          "metadata": {
            "name": "my-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "ingressHostName": "my-app.apps-crc.testing",
            "replicas": 1,
            "standardServerImage": "eap"
          }
        },
        {
          "kind": "EntandoAppPluginLink",
          "metadata": {
            "name": "my-link",
            "namespace": "my-namespace"
          },
          "spec": {
            "entandoAppName": "my-app",
            "entandoPluginName": "my-plugin"
          }
        },
        {
          "kind": "EntandoCompositeApp",
          "metadata": {
            "name": "my-entando-composite-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "components": [
              {
                "kind": "EntandoDatabaseService",
                "metadata": {
                  "name": "inline-entando-database-service"
                },
                "spec": {
                  "createDeployment": true
                }
              },
              {
                "kind": "EntandoKeycloakServer",
                "metadata": {
                  "name": "inline-keycloak"
                },
                "spec": {
                  "standardImage": "redhat-sso"
                }
              },
              {
                "kind": "EntandoClusterInfrastructure",
                "metadata": {
                  "name": "inline-entando-cluster-infrastructure"
                },
                "spec": {}
              },
              {
                "kind": "EntandoApp",
                "metadata": {
                  "name": "inline-app"
                },
                "spec": {
                  "standardServerImage": "eap"
                }
              },
              {
                "kind": "EntandoPlugin",
                "metadata": {
                  "name": "inline-plugin"
                },
                "spec": {
                  "healthCheckPath": "/management/health",
                  "image": "entando/entando-avatar-plugin:6.0.5",
                  "ingressPath": "/avatarPlugin",
                  "roles": [
                    {
                      "code": "admin",
                      "name": "admin"
                    },
                    {
                      "code": "user",
                      "name": "user"
                    }
                  ]
                }
              },
              {
                "kind": "EntandoAppPluginLink",
                "metadata": {
                  "name": "inline-link"
                },
                "spec": {
                  "entandoAppName": "inline-app",
                  "entandoPluginName": "inline-plugin"
                }
              }
            ],
            "dbmsOverride": "postgresql",
            "ingressHostNameOverride": "entando.apps-crc.testing"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/entando/entando-operator@sha256:639e2d35738483c6c092ebe8636bbab50c0f2101bc06f2fe9a1c7f68b3ffc4a6",
      "bundle_path_digest": "sha256:639e2d35738483c6c092ebe8636bbab50c0f2101bc06f2fe9a1c7f68b3ffc4a6",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:35:30.853000+00:00",
      "csv_description": "## Entando\n\nThe Entando platform accelerates the development and lifecycle  management of fully modularized applications on Kubernetes. It  provides tools to help developers create and manage applications  using modular frontend and backend components.\nThe Entando Operator automates the installation, provisioning, and  configuration management of the components that make up an Entando  application. Specifically, the operator manages the following custom  resources:\n**EntandoKeycloakServers** for centralized authentication of frontend  and backend components. The operator can deploy Keycloak, or in certified  environments, Red Hat SSO servers that can then be used by subsequent  deployments as an OpenID Connect provider.\n**EntandoApps** for hosting an Entando application. EntandoApps are hosted on Wildfly or JBoss EAP containers, and can also be used to deploy custom  EntandoApp containers.\n**EntandoPlugins** for deploying microservices to customize or enhance your EntandoApp. Entando microservice plugins are deployed to your cluster, and  then linked to one or more EntandoApps.\n## Using the Operator\nThe Entando Operator can be deployed using the default settings without any  configuration. Once deployed, the operator can be customized by editing  the *configmap* and secrets.\n### ConfigMap: entando-operator-config\nThe 'entando-operator-config' ConfigMap can be added after deployment and any changes to it will be picked up by the operator on subsequent event processing. It supports the following keys:\n\n    entando.k8s.operator.gc.controller.pods: set this to \"false\" to keep controller pods after completion.\n    entando.k8s.operator.compliance.mode: set this to \"community\" if there is no requirement for Red Hat compliance.\n    entando.k8s.operator.image.pull.secrets: a comma separated list containing the names of pull secrets that will be linked to all service accounts.\n    entando.k8s.operator.disable.pvc.garbage.collection: set this to \"false\" if you want Persistent Volume Claims to be deleted with the custom resources they are associated with.\n    entando.k8s.operator.impose.default.limits: set this to \"false\" if there is no need to limit the resource consumption of pods on your cluster.\n    entando.k8s.operator.request.to.limit.ratio: a decimal number that default limits will be multiplied by to calculate default requests for resources.\n    entando.k8s.operator.force.db.password.reset: set this to \"true\" if you plan to delete Secrets from your namespace but you want to retain the Database they point to.\n    entando.k8s.operator.pull.policy.override: specify your preferred pullPolicy for images. The default is Always.\n    entando.tls.secret.name: The name of a standard TLS secret to use for HTTPS Ingresses. See the section entando-tls-secret.\n    entando.ca.secret.name: The name of a secret containing CA certificates. See the section entando-ca-cert-secret.\n    entando.assume.external.https.provider: Set this to \"true\" if your cloud provider handles HTTPS for you.\n    entando.use.auto.cert.generation: Set this to \"true\" to have Openshift use its internal CA to generate certificates for your Routes.\n    entando.default.routing.suffix: The domain name that can be suffixed to deployment names when the ingressHostName is omitted. Needs to be preconfigured on your DNS provider.\n    entando.pod.completion.timeout.seconds: The time it will take before Entando fails a run-to-completion Pod.\n    entando.pod.readiness.timeout.seconds: The time it will take before Entando fails a Service Pod.\n    entando.pod.shutdown.timeout.seconds: The time Entando will give a Pod to shutdown gracefully.\n\n\n### entando-pull-secret\nThe secret to be used to pull Entando images from the Red Hat container registry. The name of this secret is hard coded as it is required for the ClusterServiceVersion of this Operator\n### entando-tls-secret\nA standard Kubernetes TLS secret that will be used on all deployments where no custom TLS secret name is specified.\n### entando-ca-cert-secret\nThis is an opaque secret in the Entando Operator's namespace that contains the certificates of all trusted certificate authorities in your environment. This is generally used mainly for self signed certificates. As is generally the case for opaque secrets, there are no constraints on the keys in this secret. However, limit the files inside the secret to X509 certificates or certificate chains. The Entando Operator will load all of these files into a Java keystore that it then configures as the trust store for each container that uses Java.\n",
      "csv_display_name": "Entando Operator",
      "csv_metadata_description": "Processes EntandoKeycloakServer, EntandoApp and EntandoPlugin custom resources and deploys the relevant containers in the Kubernetes cluster.",
      "csv_name": "entando-k8s-operator.v6.3.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:56:41.264000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "entando-k8s-operator",
      "provided_apis": [
        {
          "group": "entando.org",
          "kind": "EntandoAppPluginLink",
          "plural": "entandoapppluginlinks",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoApp",
          "plural": "entandoapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoClusterInfrastructure",
          "plural": "entandoclusterinfrastructures",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoCompositeApp",
          "plural": "entandocompositeapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDatabaseService",
          "plural": "entandodatabaseservices",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDeBundle",
          "plural": "entandodebundles",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoKeycloakServer",
          "plural": "entandokeycloakservers",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoPlugin",
          "plural": "entandoplugins",
          "version": "v1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "6.3.2",
      "version_original": "6.3.2"
    },
    {
      "_id": "611fbdbedece23122b7a788a",
      "alm_examples": [
        {
          "kind": "NginxIngressController",
          "metadata": {
            "name": "my-nginx-ingress-controller",
            "namespace": "my-nginx-ingress"
          },
          "spec": {
            "image": {
              "pullPolicy": "Always",
              "repository": "docker.io/nginx/nginx-ingress",
              "tag": "1.11.1-ubi"
            },
            "ingressClass": "nginx",
            "nginxPlus": false,
            "serviceType": "NodePort",
            "type": "deployment"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nginx/nginx-ingress-operator-bundle@sha256:39e36214aae291f6724a9e688b16d702bbe57dfcb06e81de6d0d5efb0a5864d6",
      "bundle_path_digest": "sha256:39e36214aae291f6724a9e688b16d702bbe57dfcb06e81de6d0d5efb0a5864d6",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:35:42.106000+00:00",
      "csv_description": "The NGINX Ingress Operator is a Kubernetes/OpenShift component which deploys and manages one or more NGINX/NGINX Plus Ingress Controllers",
      "csv_display_name": "Nginx Ingress Operator",
      "csv_metadata_description": "The NGINX Ingress Operator is a Kubernetes/OpenShift component which deploys and manages one or more NGINX/NGINX Plus Ingress Controllers",
      "csv_name": "nginx-ingress-operator.v0.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:59:51.015000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "nginx-ingress-operator",
      "provided_apis": [
        {
          "group": "k8s.nginx.org",
          "kind": "NginxIngressController",
          "plural": "nginxingresscontrollers",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:5e49760b52a497f9e8a9ef7e0fa0437766d3dfef69a007c8567de7d2f3c49284",
          "image": "registry.connect.redhat.com/nginx/nginx-ingress-operator@sha256:5e49760b52a497f9e8a9ef7e0fa0437766d3dfef69a007c8567de7d2f3c49284",
          "name": "nginx-ingress-operator-5e49760b52a497f9e8a9ef7e0fa0437766d3dfef69a007c8567de7d2f3c49284-annotation"
        },
        {
          "digest": "sha256:5e49760b52a497f9e8a9ef7e0fa0437766d3dfef69a007c8567de7d2f3c49284",
          "image": "registry.connect.redhat.com/nginx/nginx-ingress-operator@sha256:5e49760b52a497f9e8a9ef7e0fa0437766d3dfef69a007c8567de7d2f3c49284",
          "name": "nginx-ingress-operator"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.2.0",
      "version_original": "0.2.0"
    },
    {
      "_id": "611fbdc1eb34b73652492df2",
      "alm_examples": [
        {
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoCompaction": {
                "databaseFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                },
                "parallelCompaction": false,
                "timeWindow": {
                  "abortCompactionOutsideWindow": true,
                  "end": "06:00",
                  "start": "02:00"
                },
                "tombstonePurgeInterval": "72h",
                "viewFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                }
              },
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:69c9d29a8a5ba4eac037051a256a458d5772fca8b813c764ddbe02f80d89b76f",
      "bundle_path_digest": "sha256:69c9d29a8a5ba4eac037051a256a458d5772fca8b813c764ddbe02f80d89b76f",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:35:45.045000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.2.0-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:56:06.905000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:59c872ac4b19b5086a47ce95675f11df2925aa0c5d3dc0afda8df4599a8b0649",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:59c872ac4b19b5086a47ce95675f11df2925aa0c5d3dc0afda8df4599a8b0649",
          "name": "operator-59c872ac4b19b5086a47ce95675f11df2925aa0c5d3dc0afda8df4599a8b0649-annotation"
        },
        {
          "digest": "sha256:59c872ac4b19b5086a47ce95675f11df2925aa0c5d3dc0afda8df4599a8b0649",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:59c872ac4b19b5086a47ce95675f11df2925aa0c5d3dc0afda8df4599a8b0649",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "exporter-b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "name": "server-c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.2.0-1",
      "version_original": "2.2.0-1"
    },
    {
      "_id": "611fbdd5bd674341b5c5f132",
      "alm_examples": [
        {
          "kind": "OpenshiftArtifactoryHa",
          "metadata": {
            "name": "openshiftartifactoryha"
          },
          "spec": {
            "artifactory-ha": {
              "artifactory": {
                "image": {
                  "registry": "registry.connect.redhat.com",
                  "repository": "jfrog/artifactory-pro",
                  "tag": "7.9.0"
                },
                "joinKey": "OVERRIDE",
                "masterKey": "OVERRIDE",
                "node": {
                  "replicaCount": 2,
                  "waitForPrimaryStartup": {
                    "enabled": false
                  }
                },
                "uid": "1000721030"
              },
              "database": {
                "driver": "OVERRIDE",
                "password": "OVERRIDE",
                "type": "OVERRIDE",
                "url": "OVERRIDE",
                "user": "OVERRIDE"
              },
              "initContainerImage": "registry.connect.redhat.com/jfrog/init@sha256:197f5a1e7a3dd934e72a03a106f0de83e992e6926a774e26483c06fa46faeee5",
              "nginx": {
                "gid": "1000720107",
                "http": {
                  "externalPort": 80,
                  "internalPort": 8080
                },
                "https": {
                  "externalPort": 443,
                  "internalPort": 8443
                },
                "image": {
                  "registry": "registry.redhat.io",
                  "repository": "rhel8/nginx-116",
                  "tag": "latest"
                },
                "service": {
                  "ssloffload": false
                },
                "tlsSecretName": "OVERRIDE",
                "uid": "1000720104"
              },
              "postgresql": {
                "enabled": false
              },
              "waitForDatabase": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "quay.io/rhc4tp/artifactory-operator-bundle@sha256:95fd797214f4a3f2580d108287f225c1982af2741e4a96ffa787e29692073d20",
      "bundle_path_digest": "sha256:95fd797214f4a3f2580d108287f225c1982af2741e4a96ffa787e29692073d20",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:36:05.878000+00:00",
      "csv_description": "## Overview\nOpenshift Operator to deploy JFrog Artifactory Enterprise into your Openshift cluster.\n\n## Usage\n\nAn external DB is required. The operator will not deploy a DB but will require you to specify the configuration values to connect to it.\n\nSearch for JFrog and click JFrog Artifactory Enterprise Operator to install.\n\nGo to the Installed Operators.\n\nWait for the JFrog Artifactory Enterprise Operator to complete the installation.\n\nOpen the Operator and click on the provided API: Artifactory HA.\n\nClick Create New Instance and provide the following parameters for your DB configuration:\n\n```\nDATABASE_TYPE\nDATABASE_DRIVER\nDATABASE_URL\nDATABASE_USER\nDATABASE_PASSWORD\n```\nMaster key and Join key must be supplied. To generate a new key for each run the command below:\n``` # Create a key export JOIN_KEY=$(openssl rand -hex 32) echo ${JOIN_KEY} ```\nTo use TLS you will need to first create a k8s tls secret to store your .crt and .key file into.\nThen supply the value of this k8s secret into the TLS_SECRET field.\nClick Create for Artifactory Enterprise to deploy into OpenShift and connect to it on the external IP exposed by the load balancer.\n",
      "csv_display_name": "JFrog Artifactory Enterprise Operator",
      "csv_metadata_description": "JFrog Artifactory Enterprise deploys Artifactory in a high availability environment across multiple pods",
      "csv_name": "artifactory-ha-operator.v1.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:55:13.782000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "openshiftartifactoryha-operator",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "OpenshiftArtifactoryHa",
          "plural": "openshiftartifactoryhas",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:376f2922911113a0e217614e1f0d26672d62258170b7ae1dcad0fdd596e035a6",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:376f2922911113a0e217614e1f0d26672d62258170b7ae1dcad0fdd596e035a6",
          "name": "artifactory-operator-376f2922911113a0e217614e1f0d26672d62258170b7ae1dcad0fdd596e035a6-annotation"
        },
        {
          "digest": "sha256:376f2922911113a0e217614e1f0d26672d62258170b7ae1dcad0fdd596e035a6",
          "image": "registry.connect.redhat.com/jfrog/artifactory-operator@sha256:376f2922911113a0e217614e1f0d26672d62258170b7ae1dcad0fdd596e035a6",
          "name": "artifactory-ha-operator"
        },
        {
          "digest": "sha256:f0b061c4126f58b70ddaf39a3c5cc009be21b75ac4530ec5088a40d1e6f50e3e",
          "image": "registry.connect.redhat.com/jfrog/artifactory-pro@sha256:f0b061c4126f58b70ddaf39a3c5cc009be21b75ac4530ec5088a40d1e6f50e3e",
          "name": "artifactory_image_repository"
        },
        {
          "digest": "sha256:0ba76a7b26e5ffb95b4354243337ac2b3ff84ae8637c0782631084d1b2f99a33",
          "image": "registry.redhat.io/rhel8/nginx-116@sha256:0ba76a7b26e5ffb95b4354243337ac2b3ff84ae8637c0782631084d1b2f99a33",
          "name": "nginx_image_repository"
        },
        {
          "digest": "sha256:197f5a1e7a3dd934e72a03a106f0de83e992e6926a774e26483c06fa46faeee5",
          "image": "registry.connect.redhat.com/jfrog/init@sha256:197f5a1e7a3dd934e72a03a106f0de83e992e6926a774e26483c06fa46faeee5",
          "name": "init-197f5a1e7a3dd934e72a03a106f0de83e992e6926a774e26483c06fa46faeee5-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.1.0",
      "version_original": "1.1.0"
    },
    {
      "_id": "611fbdd99f14588c41ebe93d",
      "alm_examples": [
        {
          "kind": "Runner",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "gitlabUrl": "https://gitlab.com",
            "imagePullPolicy": "Always",
            "tags": "openshift, test",
            "token": "gitlab-dev-runner-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-runner-operator-bundle@sha256:abe1504fedbf38bc1cb45e5393216bae9cca95066ca84fa730c7e8b129930074",
      "bundle_path_digest": "sha256:abe1504fedbf38bc1cb45e5393216bae9cca95066ca84fa730c7e8b129930074",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:36:09.127000+00:00",
      "csv_description": "GitLab Runner is the lightweight, highly-scalable agent that runs your build jobs and sends the results back to a GitLab instance. GitLab Runner works in conjunction with GitLab CI/CD, the open-source continuous integration service included with GitLab.\n\nThe GitLab Runner operator manages the lifecycle of GitLab Runner in Kubernetes or Openshift clusters. The operator aims to automate the tasks needed to run your CI/CD jobs in your container orchestration platform.\n\n## Usage\n\n To link a GitLab Runner instance to a self-hosted GitLab instance or the hosted [GitLab](https://gitlab.com), you first need to:\n\n 1. create a secret containing the `runner-registration-token` from your GitLab project.\n\n   ```\n  cat > gitlab-runner-secret.yml << EOF\n  apiVersion: v1\n  kind: Secret\n  metadata:\n    name: gitlab-runner-secret\n  type: Opaque\n  stringData:\n    runner-registration-token: REPLACE_ME # your project runner secret\n  EOF\n  ```\n\n  ```\n  oc apply -f gitlab-runner-secret.yml\n  ```\n\n 2. Create the Custom Resource Definition (CRD) file and include the following information. The tags value must be openshift for the job to run.\n\n   ```\n   cat > gitlab-runner.yml << EOF\n   apiVersion: apps.gitlab.com/v1beta2\n   kind: Runner\n   metadata:\n     name: gitlab-runner\n   spec:\n     gitlabUrl: https://gitlab.example.com\n     buildImage: alpine\n     token: gitlab-runner-secret\n     tags: openshift\n   EOF\n   ```\n\n  ```\n  oc apply -f gitlab-runner.yml\n  ```\n\n## Full documentation\n\nVisit [Install GitLab Runner on Red Hat OpenShift](https://docs.gitlab.com/runner/install/openshift.html)\n",
      "csv_display_name": "GitLab Runner",
      "csv_metadata_description": "GitLab Runner operator manages lifecycle of GitLab Runner instances",
      "csv_name": "gitlab-runner-operator.v1.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:57:02.968000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gitlab-runner-operator",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "Runner",
          "version": "v1beta2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:f48d93d89905284a35a72ec312ad4313bb1a42deaaab662eb181a65d34a89cd9",
          "image": "registry.connect.redhat.com/gitlab/gitlab-runner@sha256:f48d93d89905284a35a72ec312ad4313bb1a42deaaab662eb181a65d34a89cd9",
          "name": "gitlab-runner"
        },
        {
          "digest": "sha256:9148c4c10685871e3bbed037d9c36e85f0603da38e721c5db2a85b9c6defeac9",
          "image": "registry.connect.redhat.com/gitlab/gitlab-runner-helper@sha256:9148c4c10685871e3bbed037d9c36e85f0603da38e721c5db2a85b9c6defeac9",
          "name": "gitlab-runner-helper"
        },
        {
          "digest": "sha256:6ac79da50858f82949e1dae4a016cbe86b75de977fe747a958d28dcd5c8e4080",
          "image": "registry.connect.redhat.com/gitlab/gitlab-runner-operator@sha256:6ac79da50858f82949e1dae4a016cbe86b75de977fe747a958d28dcd5c8e4080",
          "name": "gitlab-runner-operator"
        },
        {
          "digest": "sha256:dc0f91e256c86c3f7cb930d0e4d48eb68576425bc4bd288fb76decb0577c7e9e",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:dc0f91e256c86c3f7cb930d0e4d48eb68576425bc4bd288fb76decb0577c7e9e",
          "name": "kube-rbac-proxy"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.2.0",
      "version_original": "1.2.0"
    },
    {
      "_id": "611fbdf6bd674341b5c5f140",
      "alm_examples": [
        {
          "kind": "EntandoDeBundle",
          "metadata": {
            "name": "my-bundle",
            "namespace": "my-namespace"
          },
          "spec": {
            "details": {
              "name": "my-bundle"
            }
          }
        },
        {
          "kind": "EntandoDatabaseService",
          "metadata": {
            "name": "my-entando-database-service",
            "namespace": "my-namespace"
          },
          "spec": {
            "createDeployment": true,
            "databaseName": "my_database",
            "dbms": "postgresql"
          }
        },
        {
          "kind": "EntandoKeycloakServer",
          "metadata": {
            "name": "my-keycloak",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "replicas": 1,
            "standardImage": "redhat-sso"
          }
        },
        {
          "kind": "EntandoClusterInfrastructure",
          "metadata": {
            "name": "my-entando-cluster-infrastructure",
            "namespace": "my-namespace"
          },
          "spec": {
            "environmentVariables": [],
            "replicas": 1
          }
        },
        {
          "kind": "EntandoPlugin",
          "metadata": {
            "name": "my-entando-plugin",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "healthCheckPath": "/management/health",
            "image": "entando/entando-avatar-plugin:6.0.5",
            "ingressHostName": "my-plugin.apps-crc.testing",
            "ingressPath": "/avatarPlugin",
            "replicas": 1,
            "roles": [
              {
                "code": "admin",
                "name": "user"
              },
              {
                "code": "user",
                "name": "user"
              }
            ],
            "securityLevel": "strict"
          }
        },
        {
          "kind": "EntandoApp",
          "metadata": {
            "name": "my-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "ingressHostName": "my-app.apps-crc.testing",
            "replicas": 1,
            "standardServerImage": "eap"
          }
        },
        {
          "kind": "EntandoAppPluginLink",
          "metadata": {
            "name": "my-link",
            "namespace": "my-namespace"
          },
          "spec": {
            "entandoAppName": "my-app",
            "entandoPluginName": "my-plugin"
          }
        },
        {
          "kind": "EntandoCompositeApp",
          "metadata": {
            "name": "my-entando-composite-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "components": [
              {
                "kind": "EntandoDatabaseService",
                "metadata": {
                  "name": "inline-entando-database-service"
                },
                "spec": {
                  "createDeployment": true
                }
              },
              {
                "kind": "EntandoKeycloakServer",
                "metadata": {
                  "name": "inline-keycloak"
                },
                "spec": {
                  "standardImage": "redhat-sso"
                }
              },
              {
                "kind": "EntandoClusterInfrastructure",
                "metadata": {
                  "name": "inline-entando-cluster-infrastructure"
                },
                "spec": {}
              },
              {
                "kind": "EntandoApp",
                "metadata": {
                  "name": "inline-app"
                },
                "spec": {
                  "standardServerImage": "eap"
                }
              },
              {
                "kind": "EntandoPlugin",
                "metadata": {
                  "name": "inline-plugin"
                },
                "spec": {
                  "healthCheckPath": "/management/health",
                  "image": "entando/entando-avatar-plugin:6.0.5",
                  "ingressPath": "/avatarPlugin",
                  "roles": [
                    {
                      "code": "admin",
                      "name": "admin"
                    },
                    {
                      "code": "user",
                      "name": "user"
                    }
                  ]
                }
              },
              {
                "kind": "EntandoAppPluginLink",
                "metadata": {
                  "name": "inline-link"
                },
                "spec": {
                  "entandoAppName": "inline-app",
                  "entandoPluginName": "inline-plugin"
                }
              }
            ],
            "dbmsOverride": "postgresql",
            "ingressHostNameOverride": "entando.apps-crc.testing"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/entando/entando-operator@sha256:66f8db604ac4d104cd88d786c4a9437dd1a22b3d7f8fd856c1251191f7d4856a",
      "bundle_path_digest": "sha256:66f8db604ac4d104cd88d786c4a9437dd1a22b3d7f8fd856c1251191f7d4856a",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "beta",
      "creation_date": "2021-08-20T14:36:38.242000+00:00",
      "csv_description": "## Entando\n\nThe Entando platform accelerates the development and lifecycle  management of fully modularized applications on Kubernetes. It  provides tools to help developers create and manage applications  using modular frontend and backend components.\nThe Entando Operator automates the installation, provisioning, and  configuration management of the components that make up an Entando  application. Specifically, the operator manages the following custom  resources:\n**EntandoKeycloakServers** for centralized authentication of frontend  and backend components. The operator can deploy Keycloak, or in certified  environments, Red Hat SSO servers that can then be used by subsequent  deployments as an OpenID Connect provider.\n**EntandoApps** for hosting an Entando application. EntandoApps are hosted on Wildfly or JBoss EAP containers, and can also be used to deploy custom  EntandoApp containers.\n**EntandoPlugins** for deploying microservices to customize or enhance your EntandoApp. Entando microservice plugins are deployed to your cluster, and  then linked to one or more EntandoApps.\n## Using the Operator\nThe Entando Operator can be deployed using the default settings without any  configuration. Once deployed, the operator can be customized by editing  the *configmap* and secrets.\n### ConfigMap: entando-operator-config\nThe 'entando-operator-config' ConfigMap can be added after deployment and any changes to it will be picked up by the operator on subsequent event processing. It supports the following keys:\n\n    entando.k8s.operator.gc.controller.pods: set this to \"false\" to keep controller pods after completion.\n    entando.k8s.operator.compliance.mode: set this to \"community\" if there is no requirement for Red Hat compliance.\n    entando.k8s.operator.image.pull.secrets: a comma separated list containing the names of pull secrets that will be linked to all service accounts.\n    entando.k8s.operator.disable.pvc.garbage.collection: set this to \"false\" if you want Persistent Volume Claims to be deleted with the custom resources they are associated with.\n    entando.k8s.operator.impose.default.limits: set this to \"false\" if there is no need to limit the resource consumption of pods on your cluster.\n    entando.k8s.operator.request.to.limit.ratio: a decimal number that default limits will be multiplied by to calculate default requests for resources.\n    entando.k8s.operator.force.db.password.reset: set this to \"true\" if you plan to delete Secrets from your namespace but you want to retain the Database they point to.\n    entando.k8s.operator.pull.policy.override: specify your preferred pullPolicy for images. The default is Always.\n    entando.tls.secret.name: The name of a standard TLS secret to use for HTTPS Ingresses. See the section entando-tls-secret.\n    entando.ca.secret.name: The name of a secret containing CA certificates. See the section entando-ca-cert-secret.\n    entando.assume.external.https.provider: Set this to \"true\" if your cloud provider handles HTTPS for you.\n    entando.use.auto.cert.generation: Set this to \"true\" to have Openshift use its internal CA to generate certificates for your Routes.\n    entando.default.routing.suffix: The domain name that can be suffixed to deployment names when the ingressHostName is omitted. Needs to be preconfigured on your DNS provider.\n    entando.pod.completion.timeout.seconds: The time it will take before Entando fails a run-to-completion Pod.\n    entando.pod.readiness.timeout.seconds: The time it will take before Entando fails a Service Pod.\n    entando.pod.shutdown.timeout.seconds: The time Entando will give a Pod to shutdown gracefully.\n\n\n### entando-pull-secret\nThe secret to be used to pull Entando images from the Red Hat container registry. The name of this secret is hard coded as it is required for the ClusterServiceVersion of this Operator\n### entando-tls-secret\nA standard Kubernetes TLS secret that will be used on all deployments where no custom TLS secret name is specified.\n### entando-ca-cert-secret\nThis is an opaque secret in the Entando Operator's namespace that contains the certificates of all trusted certificate authorities in your environment. This is generally used mainly for self signed certificates. As is generally the case for opaque secrets, there are no constraints on the keys in this secret. However, limit the files inside the secret to X509 certificates or certificate chains. The Entando Operator will load all of these files into a Java keystore that it then configures as the trust store for each container that uses Java.\n",
      "csv_display_name": "Entando Operator",
      "csv_metadata_description": "Processes EntandoKeycloakServer, EntandoApp and EntandoPlugin custom resources and deploys the relevant containers in the Kubernetes cluster.",
      "csv_name": "entando-k8s-operator.v6.3.2-pr4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-05T10:56:45.573000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "entando-k8s-operator",
      "provided_apis": [
        {
          "group": "entando.org",
          "kind": "EntandoCompositeApp",
          "plural": "entandocompositeapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDatabaseService",
          "plural": "entandodatabaseservices",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDeBundle",
          "plural": "entandodebundles",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoKeycloakServer",
          "plural": "entandokeycloakservers",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoPlugin",
          "plural": "entandoplugins",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoAppPluginLink",
          "plural": "entandoapppluginlinks",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoApp",
          "plural": "entandoapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoClusterInfrastructure",
          "plural": "entandoclusterinfrastructures",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839",
          "image": "entando/entando-k8s-controller-coordinator@sha256:c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839",
          "name": "entando-k8s-controller-coordinator-c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839-annotation"
        },
        {
          "digest": "sha256:c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839",
          "image": "entando/entando-k8s-controller-coordinator@sha256:c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839",
          "name": "entando-operator"
        },
        {
          "digest": "sha256:570bce7dc649ac7ac85e7ecb196aabc281c77a94590aa7df7d9746869ce757ea",
          "image": "registry.redhat.io/rhel8/mysql-80@sha256:570bce7dc649ac7ac85e7ecb196aabc281c77a94590aa7df7d9746869ce757ea",
          "name": "rhel8_mysql_80"
        },
        {
          "digest": "sha256:f4e5c728b644bf1888ec8086424852ed74b5596a511be29e636fb10218fc9b6f",
          "image": "registry.redhat.io/rhel8/postgresql-12@sha256:f4e5c728b644bf1888ec8086424852ed74b5596a511be29e636fb10218fc9b6f",
          "name": "rhel8_postgresql_12"
        },
        {
          "digest": "sha256:1ca90c09302b9cadf8950278d92ad4d7f3a14f02dea8704320794aa77ada8631",
          "image": "docker.io/entando/app-builder@sha256:1ca90c09302b9cadf8950278d92ad4d7f3a14f02dea8704320794aa77ada8631",
          "name": "app_builder"
        },
        {
          "digest": "sha256:668191fc86b090452baaba18ec7bbc3d07df5a99750e0c5b98759599d565cfbe",
          "image": "docker.io/entando/entando-component-manager@sha256:668191fc86b090452baaba18ec7bbc3d07df5a99750e0c5b98759599d565cfbe",
          "name": "entando_component_manager"
        },
        {
          "digest": "sha256:1f0e35d7f7ee8123721cf8e80ac96ac2f594f21a2b68cf9e720917d090c8095e",
          "image": "docker.io/entando/entando-de-app-eap@sha256:1f0e35d7f7ee8123721cf8e80ac96ac2f594f21a2b68cf9e720917d090c8095e",
          "name": "entando_de_app_eap"
        },
        {
          "digest": "sha256:c68d3b129fc625f5d1ee10ed2a107e28c145049d4b513dcdb2c736555025e339",
          "image": "docker.io/entando/entando-k8s-app-controller@sha256:c68d3b129fc625f5d1ee10ed2a107e28c145049d4b513dcdb2c736555025e339",
          "name": "entando_k8s_app_controller"
        },
        {
          "digest": "sha256:aa7ba0861705213434e610530c5bf5da1bdb33f1834d3c346174f5a28145c0bd",
          "image": "docker.io/entando/entando-k8s-app-plugin-link-controller@sha256:aa7ba0861705213434e610530c5bf5da1bdb33f1834d3c346174f5a28145c0bd",
          "name": "entando_k8s_app_plugin_link_controller"
        },
        {
          "digest": "sha256:86dbea6696349d7b2393e12bacef4d1acec2dd3f8551e397fa26c92706a18dd3",
          "image": "docker.io/entando/entando-k8s-cluster-infrastructure-controller@sha256:86dbea6696349d7b2393e12bacef4d1acec2dd3f8551e397fa26c92706a18dd3",
          "name": "entando_k8s_cluster_infrastructure_controller"
        },
        {
          "digest": "sha256:4bc6797bb41b3e407b64530c3cff7a78f3fc2c22863caad9c606ccd650310b61",
          "image": "docker.io/entando/entando-k8s-composite-app-controller@sha256:4bc6797bb41b3e407b64530c3cff7a78f3fc2c22863caad9c606ccd650310b61",
          "name": "entando_k8s_composite_app_controller"
        },
        {
          "digest": "sha256:58ca4ad2fca83a715aba9b68609f9056f633d556eb9845b3aff5c05769369539",
          "image": "docker.io/entando/entando-k8s-database-service-controller@sha256:58ca4ad2fca83a715aba9b68609f9056f633d556eb9845b3aff5c05769369539",
          "name": "entando_k8s_database_service_controller"
        },
        {
          "digest": "sha256:8ae47dbf93201965106663299f06daad544982fa244d4a0f365a1b717727e5ed",
          "image": "docker.io/entando/entando-k8s-dbjob@sha256:8ae47dbf93201965106663299f06daad544982fa244d4a0f365a1b717727e5ed",
          "name": "entando_k8s_dbjob"
        },
        {
          "digest": "sha256:f1ed42001260802353197433cea7df4c2d0e5fd9bcd926bb5ae7c654dfad2ffe",
          "image": "docker.io/entando/entando-k8s-keycloak-controller@sha256:f1ed42001260802353197433cea7df4c2d0e5fd9bcd926bb5ae7c654dfad2ffe",
          "name": "entando_k8s_keycloak_controller"
        },
        {
          "digest": "sha256:a20c753dda86e0f54f210e446b404733045ba426fc7186dc3936daed8224fbcb",
          "image": "docker.io/entando/entando-k8s-plugin-controller@sha256:a20c753dda86e0f54f210e446b404733045ba426fc7186dc3936daed8224fbcb",
          "name": "entando_k8s_plugin_controller"
        },
        {
          "digest": "sha256:775391f0005e64764288bfe007d9ea576f13618c02ea400d030a7502cfb551bd",
          "image": "docker.io/entando/entando-k8s-service@sha256:775391f0005e64764288bfe007d9ea576f13618c02ea400d030a7502cfb551bd",
          "name": "entando_k8s_service"
        },
        {
          "digest": "sha256:3692adb3694b3bf278bed7a678c9d0eaa0aba8e3e3ed4117ca04156525bcac72",
          "image": "docker.io/entando/entando-redhat-sso@sha256:3692adb3694b3bf278bed7a678c9d0eaa0aba8e3e3ed4117ca04156525bcac72",
          "name": "entando_redhat_sso"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "6.3.2-pr4",
      "version_original": "6.3.2-pr4"
    },
    {
      "_id": "611fbe1f9f14588c41ebe965",
      "alm_examples": [
        {
          "kind": "ModelServer",
          "metadata": {
            "name": "model-server-sample"
          },
          "spec": {
            "annotations": {},
            "aws_access_key_id": "",
            "aws_region": "",
            "aws_secret_access_key": "",
            "batch#": "",
            "config_configmap_name": "",
            "file_system_poll_wait_seconds": 0,
            "gcp_creds_secret_name": "",
            "grpc_port": 8080,
            "https_proxy": "",
            "image_name": "registry.connect.redhat.com/intel/openvino-model-server@sha256:4ee88853cd8361e9d0a3461ad552f79c14f22406f1e32c8cbbb9341ae9780671",
            "log_level": "INFO",
            "model_name": "resnet",
            "model_path": "gs://ovms-public-eu/resnet50-binary",
            "model_version_policy": "{\\\"latest\\\": { \\\"num_versions\\\":1 }}",
            "models_host_path": "",
            "models_volume_claim": "",
            "nireq": "",
            "plugin_config": "{\\\"CPU_THROUGHPUT_STREAMS\\\":\\\"1\\\"}",
            "replicas": 1,
            "resources": {
              "limits": {
                "cpu": 4,
                "memory": "250Mi"
              }
            },
            "rest_port": 8081,
            "s3_compat_api_endpoint": "",
            "security_context": {},
            "service_type": "ClusterIP",
            "shape": "",
            "target_device": "CPU"
          }
        },
        {
          "kind": "Notebook",
          "metadata": {
            "name": "openvino-notebook"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/intel/ovms-operator-bundle@sha256:2b863f5777ba23d8ea38f23b6a9c974e87c282bf85acb4a8c1746bd51453937b",
      "bundle_path_digest": "sha256:2b863f5777ba23d8ea38f23b6a9c974e87c282bf85acb4a8c1746bd51453937b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:37:19.211000+00:00",
      "csv_description": "OpenVINO Toolkit Operator manages OpenVINO components in OpenShift.\n\nCurrently there are components ModelServer and Notebook.\n\n# Model Server\n[OpenVINO\u2122 Model Server](https://github.com/openvinotoolkit/model_server) (OVMS) is a scalable, high-performance solution for serving machine learning models optimized for Intel\u00ae architectures. The server provides an inference service via gRPC or REST API - making it easy to deploy new algorithms and AI experiments using the same architecture as [TensorFlow Serving](https://github.com/tensorflow/serving) for any models trained in a framework that is supported by [OpenVINO](https://software.intel.com/en-us/openvino-toolkit).\nModel Server configuration is identical with a [Helm chart](https://github.com/openvinotoolkit/model_server/tree/main/deploy) for OVMS. It supports all the parameters from the helm chart.\n## Using the cluster\nOpenVINO Model Server can be consumed as a `Service`. It is called like with `ModelServer` resource with `-ovms` suffix.\nThe suffix is ommited when `ovms` phrase is included in the name.\nThe service exposes gRPC and REST API interfaces to run inference requests.\n```\noc get pods\nNAME                                        READY   STATUS    RESTARTS   AGE\nmodel-server-sample-ovms-586f6f76df-dpps4   1/1     Running   0          8h\n\noc get services\nNAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nmodel-server-sample-ovms   ClusterIP   172.25.199.210   <none>        8080/TCP,8081/TCP   8h\n```\n## Before you start\nDepending on the deployment configuration there might be pre-requisites for additional records to be created in the cluster. \n## References\nOpenVINO Model Server on [Github](https://github.com/openvinotoolkit/model_server/tree/main/extras/openvino-operator-openshift)\n\n# Notebook\nThe Notebook resource integrates JupyterHub from OpenShift Data Science or Open Data Hub with a container image that includes [developer\ntools](https://github.com/openvinotoolkit/openvino/blob/master/docs/install_guides/pypi-openvino-dev.md)\nfrom the OpenVINO toolkit and a set of Jupyter notebook tutorials. It enables selecting a defined image `openvino-notebook` from\nthe Jupyter Spawner drop-down menu.\n\nCreate the `Notebook` resource in the same project with JupyterHub and RedHat OpenShift Data Science operator.\nIt builds the image in the cluster based on Dockerfile from [openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks).\n",
      "csv_display_name": "OpenVINO Toolkit Operator",
      "csv_metadata_description": "",
      "csv_name": "openvino-operator.v0.0.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:00:13.121000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "ovms-operator",
      "provided_apis": [
        {
          "group": "intel.com",
          "kind": "ModelServer",
          "plural": "modelservers",
          "version": "v1alpha1"
        },
        {
          "group": "intel.com",
          "kind": "Notebook",
          "plural": "notebooks",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:6d0286b8a8f6f3cd9d6cd8319400acf27b70fbb52df5808ec6fe2d9849be7d8c",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:6d0286b8a8f6f3cd9d6cd8319400acf27b70fbb52df5808ec6fe2d9849be7d8c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:8385d6de20d1e59a931c44f6a785b53c96495833756f8ed07602f9ac54d62eb4",
          "image": "registry.connect.redhat.com/intel/ovms-operator@sha256:8385d6de20d1e59a931c44f6a785b53c96495833756f8ed07602f9ac54d62eb4",
          "name": "manager"
        },
        {
          "digest": "sha256:4ee88853cd8361e9d0a3461ad552f79c14f22406f1e32c8cbbb9341ae9780671",
          "image": "registry.connect.redhat.com/intel/openvino-model-server@sha256:4ee88853cd8361e9d0a3461ad552f79c14f22406f1e32c8cbbb9341ae9780671",
          "name": "openvino-model-server-4ee88853cd8361e9d0a3461ad552f79c14f22406f1e32c8cbbb9341ae9780671-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.2.0",
      "version_original": "0.2.0"
    },
    {
      "_id": "611fbe2adece23122b7a78cb",
      "alm_examples": [
        {
          "kind": "SANStorageCSI",
          "metadata": {
            "name": "sanstoragecsi"
          },
          "spec": {
            "driverName": "san.csi.nec.com",
            "images": {
              "csiDriver": "",
              "externalAttacher": "registry.redhat.io/openshift4/ose-csi-external-attacher@sha256:92cb1a244497e71cfc27437f07b95645753f64f7f994db20bf9e5cbfb89ae083",
              "externalProvisioner": "registry.redhat.io/openshift4/ose-csi-external-provisioner@sha256:faefca15575552112c8463ade19ebf5f6ce27a65b9e7286ea74fd6a31a2f2715",
              "livenessProbe": "registry.redhat.io/openshift4/ose-csi-livenessprobe@sha256:241e288cf83443304834af837f9f71a6216c038dd27196748184aa95790b75fe",
              "nodeRegistrar": "registry.redhat.io/openshift4/ose-csi-node-driver-registrar@sha256:dcba3ce367bd546c84d412f57589cd6d2fcde6a882791048ed13f1438b6453c0"
            },
            "parameters": {
              "controller": {
                "logLevel": 5
              },
              "node": {
                "livenessProbePort": 9808,
                "logLevel": 5,
                "maxVolumesPerNode": 1024
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nec/sanstoragecsi-operator-bundle@sha256:a547f2b1018aebda3dadf2599d082dd7cfebc3e255571634d97b3b3f6161436e",
      "bundle_path_digest": "sha256:a547f2b1018aebda3dadf2599d082dd7cfebc3e255571634d97b3b3f6161436e",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:37:30.777000+00:00",
      "csv_description": "This operator deploys the NEC Storage M Series CSI Driver on OpenShift Container Platform.\n\nNEC Storage M Series CSI Driver provides persistent storage for stateful applications using NEC Storage M Series.\nPlease refer to the documentation of the NEC Storage M Series CSI Driver for detail information about supported NEC Storage Platforms and supported CSI features.\n",
      "csv_display_name": "NEC Storage M Series CSI Operator",
      "csv_metadata_description": "An operator for managing the NEC Storage M Series CSI Driver",
      "csv_name": "sanstoragecsi-operator-bundle.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:01:12.922000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "sanstoragecsi-operator-bundle",
      "provided_apis": [
        {
          "group": "csi.nec.com",
          "kind": "SANStorageCSI",
          "plural": "sanstoragecsis",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:1e385777c7ce4bfd48902fbbb8023a6eba3f5dff4dc08e200450bc6f0bcb595c",
          "image": "registry.connect.redhat.com/nec/sanstoragecsi-operator@sha256:1e385777c7ce4bfd48902fbbb8023a6eba3f5dff4dc08e200450bc6f0bcb595c",
          "name": "sanstoragecsi-operator-bundle"
        },
        {
          "digest": "sha256:dcba3ce367bd546c84d412f57589cd6d2fcde6a882791048ed13f1438b6453c0",
          "image": "registry.redhat.io/openshift4/ose-csi-node-driver-registrar@sha256:dcba3ce367bd546c84d412f57589cd6d2fcde6a882791048ed13f1438b6453c0",
          "name": "ose-csi-node-driver-registrar-dcba3ce367bd546c84d412f57589cd6d2fcde6a882791048ed13f1438b6453c0-annotation"
        },
        {
          "digest": "sha256:241e288cf83443304834af837f9f71a6216c038dd27196748184aa95790b75fe",
          "image": "registry.redhat.io/openshift4/ose-csi-livenessprobe@sha256:241e288cf83443304834af837f9f71a6216c038dd27196748184aa95790b75fe",
          "name": "ose-csi-livenessprobe-241e288cf83443304834af837f9f71a6216c038dd27196748184aa95790b75fe-annotation"
        },
        {
          "digest": "sha256:faefca15575552112c8463ade19ebf5f6ce27a65b9e7286ea74fd6a31a2f2715",
          "image": "registry.redhat.io/openshift4/ose-csi-external-provisioner@sha256:faefca15575552112c8463ade19ebf5f6ce27a65b9e7286ea74fd6a31a2f2715",
          "name": "ose-csi-external-provisioner-faefca15575552112c8463ade19ebf5f6ce27a65b9e7286ea74fd6a31a2f2715-annotation"
        },
        {
          "digest": "sha256:92cb1a244497e71cfc27437f07b95645753f64f7f994db20bf9e5cbfb89ae083",
          "image": "registry.redhat.io/openshift4/ose-csi-external-attacher@sha256:92cb1a244497e71cfc27437f07b95645753f64f7f994db20bf9e5cbfb89ae083",
          "name": "ose-csi-external-attacher-92cb1a244497e71cfc27437f07b95645753f64f7f994db20bf9e5cbfb89ae083-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "611fbe5dbd674341b5c5f16e",
      "alm_examples": [
        {
          "kind": "CitrixCpxWithIngressController",
          "metadata": {
            "name": "cpx-cic"
          },
          "spec": {
            "ADMSettings": {
              "ADMFingerPrint": "",
              "ADMIP": "",
              "bandWidth": "",
              "bandWidthLicense": false,
              "cpxCores": "",
              "licenseServerIP": "",
              "licenseServerPort": 27000,
              "loginSecret": "",
              "vCPULicense": false
            },
            "aws": false,
            "azure": false,
            "bgpSettings": {
              "bgpConfig": [
                {
                  "bgpRouter": {
                    "localAS": 100,
                    "neighbor": [
                      {
                        "ASOriginationInterval": 10,
                        "address": "",
                        "advertisementInterval": 10,
                        "remoteAS": 100
                      }
                    ]
                  }
                }
              ],
              "required": false
            },
            "cic": {
              "image": "registry.connect.redhat.com/citrix/citrix-ingress-controller@sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
              "pullPolicy": "Always",
              "required": true
            },
            "coeConfig": {
              "distributedTracing": {
                "enable": false,
                "samplingrate": 100
              },
              "endpoint": {
                "server": ""
              },
              "required": false,
              "timeseries": {
                "auditlogs": {
                  "enable": false
                },
                "events": {
                  "enable": false
                },
                "metrics": {
                  "enable": false,
                  "mode": "avro"
                },
                "port": 5563
              },
              "transactions": {
                "enable": false,
                "port": 5557
              }
            },
            "cpxBgpRouter": false,
            "crds": {
              "install": true,
              "retainOnDelete": false
            },
            "defaultSSLCertSecret": "",
            "entityPrefix": "",
            "exporter": {
              "image": "registry.connect.redhat.com/citrix/citrix-adc-metrics-exporter@sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
              "ports": {
                "containerPort": 8888
              },
              "pullPolicy": "Always",
              "required": false
            },
            "image": "registry.connect.redhat.com/citrix/cpx-13-0@sha256:54c57fa80f0d12d987c8e1fb2cb118539446f1114997e671dfd1604fab24d23d",
            "ingressClass": "",
            "ingressIP": "",
            "ipam": false,
            "kubernetesURL": "",
            "license": {
              "accept": false
            },
            "logLevel": "INFO",
            "logProxy": "",
            "mgmtHttpPort": 9080,
            "mgmtHttpsPort": 9443,
            "namespaceLabels": "",
            "nodeSelector": {
              "key": "",
              "value": ""
            },
            "nsCookieVersion": "0",
            "nsGateway": "192.168.1.1",
            "nsHTTP2ServerSide": "OFF",
            "nsIP": "192.168.1.2",
            "nsProtocol": "http",
            "openshift": true,
            "pullPolicy": "Always",
            "routeLabels": "",
            "serviceAccount": {
              "create": true
            },
            "serviceAnnotations": {
              "aws": {
                "backendProtocol": "",
                "negotiationPolicy": "",
                "proxyProtocol": false,
                "resourcesTags": "",
                "sslCert": "",
                "sslPorts": ""
              },
              "azure": {
                "internalLoadBalancer": false
              }
            },
            "serviceType": {
              "loadBalancer": {
                "enabled": false,
                "externalTrafficPolicy": "Local"
              },
              "nodePort": {
                "enabled": false,
                "httpPort": "",
                "httpsPort": ""
              }
            },
            "setAsDefaultIngressClass": false,
            "sslCertManagedByAWS": false,
            "tolerations": [],
            "updateIngressStatus": false
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/citrix/citrix-k8s-cpx-ingress-bundle@sha256:58f15e99628043753fefa5c8df190cc90e82a6bd3b00aed3c52f0fbf69d81039",
      "bundle_path_digest": "sha256:58f15e99628043753fefa5c8df190cc90e82a6bd3b00aed3c52f0fbf69d81039",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:38:21.727000+00:00",
      "csv_description": "Citrix ADC CPX is a container-based application delivery controller that can be provisioned on a Docker host. Citrix ADC CPX enables customers to leverage Docker engine capabilities and use NetScaler load balancing and traffic management features for container-based applications. Citrix Ingress Controller which will be running in side-car mode with Citrix ADC CPX will automatically configures Citrix ADC CPX based on the Ingress resource configuration. This operator can be used deploy Citrix ADC CPX with Citrix ingress controller in an OpenShift environment.",
      "csv_display_name": "Citrix ADC CPX with Ingress Controller",
      "csv_metadata_description": "Citrix ADC CPX is a container-based application delivery controller that can be provisioned on a Docker host. Citrix ADC CPX enables customers to leverage Docker engine capabilities and use NetScaler load balancing and traffic management features for container-based applications. Citrix Ingress Controller which will be running in side-car mode with Citrix ADC CPX will automatically configures Citrix ADC CPX based on the Ingress resource configuration. This operator can be used deploy Citrix ADC CPX with Citrix ingress controller in an Openshift environment.",
      "csv_name": "citrix-cpx-with-ingress-controller-operator.v1.13.20",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:55:39.176000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "citrix-cpx-with-ingress-controller-operator",
      "provided_apis": [
        {
          "group": "citrix.com",
          "kind": "CitrixCpxWithIngressController",
          "plural": "citrixcpxwithingresscontrollers",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:4ca00f777a37e3070cfa2651685ba284623a78b55e2f81143ec8b58e967f163a",
          "image": "registry.connect.redhat.com/citrix/citrix-k8s-cpx-ingress-controller@sha256:4ca00f777a37e3070cfa2651685ba284623a78b55e2f81143ec8b58e967f163a",
          "name": "citrix-k8s-cpx-ingress-controller-4ca00f777a37e3070cfa2651685ba284623a78b55e2f81143ec8b58e967f163a-annotation"
        },
        {
          "digest": "sha256:10252238f921d720b8af7e2ca8e90be6e8516efefc1633697aa048a74041f1c3",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:10252238f921d720b8af7e2ca8e90be6e8516efefc1633697aa048a74041f1c3",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:4ca00f777a37e3070cfa2651685ba284623a78b55e2f81143ec8b58e967f163a",
          "image": "registry.connect.redhat.com/citrix/citrix-k8s-cpx-ingress-controller@sha256:4ca00f777a37e3070cfa2651685ba284623a78b55e2f81143ec8b58e967f163a",
          "name": "manager"
        },
        {
          "digest": "sha256:54c57fa80f0d12d987c8e1fb2cb118539446f1114997e671dfd1604fab24d23d",
          "image": "registry.connect.redhat.com/citrix/cpx-13-0@sha256:54c57fa80f0d12d987c8e1fb2cb118539446f1114997e671dfd1604fab24d23d",
          "name": "cpx"
        },
        {
          "digest": "sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
          "image": "registry.connect.redhat.com/citrix/citrix-ingress-controller@sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
          "name": "cic"
        },
        {
          "digest": "sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
          "image": "registry.connect.redhat.com/citrix/citrix-adc-metrics-exporter@sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
          "name": "exporter"
        },
        {
          "digest": "sha256:54c57fa80f0d12d987c8e1fb2cb118539446f1114997e671dfd1604fab24d23d",
          "image": "registry.connect.redhat.com/citrix/cpx-13-0@sha256:54c57fa80f0d12d987c8e1fb2cb118539446f1114997e671dfd1604fab24d23d",
          "name": "cpx-13-0-54c57fa80f0d12d987c8e1fb2cb118539446f1114997e671dfd1604fab24d23d-annotation"
        },
        {
          "digest": "sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
          "image": "registry.connect.redhat.com/citrix/citrix-adc-metrics-exporter@sha256:c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe",
          "name": "citrix-adc-metrics-exporter-c495fb88a82bac7cd92e13ebf52c24ebf425772acb7f43ab6bd753ffaddc5dfe-annotation"
        },
        {
          "digest": "sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
          "image": "registry.connect.redhat.com/citrix/citrix-ingress-controller@sha256:f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0",
          "name": "citrix-ingress-controller-f81763b127d4320e6fcefdefc2595e85c02e0be4e7bced0736338039bdff32a0-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.13.20",
      "version_original": "1.13.20"
    },
    {
      "_id": "611fbe6e9f14588c41ebe990",
      "alm_examples": [
        {
          "kind": "ClusterPolicy",
          "metadata": {
            "name": "gpu-cluster-policy"
          },
          "spec": {
            "dcgmExporter": {
              "affinity": {},
              "image": "dcgm-exporter",
              "imagePullSecrets": [],
              "nodeSelector": {
                "nvidia.com/gpu.deploy.dcgm-exporter": "true"
              },
              "podSecurityContext": {},
              "priorityClassName": "system-node-critical",
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:8af02463a8b60b21202d0bf69bc1ee0bb12f684fa367f903d138df6cacc2d0ac"
            },
            "devicePlugin": {
              "affinity": {},
              "args": [],
              "env": [
                {
                  "name": "PASS_DEVICE_SPECS",
                  "value": "true"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                },
                {
                  "name": "DEVICE_LIST_STRATEGY",
                  "value": "envvar"
                },
                {
                  "name": "DEVICE_ID_STRATEGY",
                  "value": "uuid"
                },
                {
                  "name": "NVIDIA_VISIBLE_DEVICES",
                  "value": "all"
                },
                {
                  "name": "NVIDIA_DRIVER_CAPABILITIES",
                  "value": "all"
                }
              ],
              "image": "k8s-device-plugin",
              "imagePullSecrets": [],
              "nodeSelector": {
                "nvidia.com/gpu.deploy.device-plugin": "true"
              },
              "podSecurityContext": {},
              "priorityClassName": "system-node-critical",
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05"
            },
            "driver": {
              "affinity": {},
              "enabled": true,
              "image": "driver",
              "imagePullSecrets": [],
              "licensingConfig": {
                "configMapName": ""
              },
              "nodeSelector": {
                "nvidia.com/gpu.deploy.driver": "true"
              },
              "podSecurityContext": {},
              "priorityClassName": "system-node-critical",
              "repoConfig": {
                "configMapName": "",
                "destinationDir": ""
              },
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:09ba3eca64a80fab010a9fcd647a2675260272a8c3eb515dfed6dc38a2d31ead"
            },
            "gfd": {
              "affinity": {},
              "env": [
                {
                  "name": "GFD_SLEEP_INTERVAL",
                  "value": "60s"
                },
                {
                  "name": "FAIL_ON_INIT_ERROR",
                  "value": "true"
                }
              ],
              "image": "gpu-feature-discovery",
              "imagePullSecrets": [],
              "nodeSelector": {
                "nvidia.com/gpu.deploy.gpu-feature-discovery": "true"
              },
              "podSecurityContext": {},
              "priorityClassName": "system-node-critical",
              "repository": "nvcr.io/nvidia",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f"
            },
            "mig": {
              "strategy": "single"
            },
            "migManager": {
              "affinity": {},
              "enabled": true,
              "env": [
                {
                  "name": "WITH_REBOOT",
                  "value": "false"
                }
              ],
              "image": "k8s-mig-manager",
              "imagePullSecrets": [],
              "nodeSelector": {
                "nvidia.com/gpu.deploy.mig-manager": "true"
              },
              "podSecurityContext": {},
              "priorityClassName": "system-node-critical",
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:495ed3b42e0541590c537ab1b33bda772aad530d3ef6a4f9384d3741a59e2bf8"
            },
            "operator": {
              "defaultRuntime": "crio",
              "deployGFD": true,
              "initContainer": {
                "image": "cuda",
                "imagePullSecrets": [],
                "repository": "nvcr.io/nvidia",
                "version": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce"
              }
            },
            "toolkit": {
              "affinity": {},
              "enabled": true,
              "image": "container-toolkit",
              "imagePullSecrets": [],
              "nodeSelector": {
                "nvidia.com/gpu.deploy.container-toolkit": "true"
              },
              "podSecurityContext": {},
              "priorityClassName": "system-node-critical",
              "repository": "nvcr.io/nvidia/k8s",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:ffa284f1f359d70f0e1d6d8e7752d7c92ef7445b0d74965a8682775de37febf8"
            },
            "validator": {
              "affinity": {},
              "env": [
                {
                  "name": "WITH_WORKLOAD",
                  "value": "true"
                }
              ],
              "image": "gpu-operator-validator",
              "imagePullSecrets": [],
              "nodeSelector": {
                "nvidia.com/gpu.deploy.operator-validator": "true"
              },
              "podSecurityContext": {},
              "priorityClassName": "system-node-critical",
              "repository": "nvcr.io/nvidia/cloud-native",
              "resources": {},
              "securityContext": {},
              "tolerations": [],
              "version": "sha256:aa1f7bd526ae132c46f3ebe6ecfabe675889e240776ccc2155e31e0c48cc659e"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia/gpu-operator-bundle@sha256:380392c15740edd8853ea65b1e19f3b9bd3afea04612f204c244bf5a23b328ce",
      "bundle_path_digest": "sha256:380392c15740edd8853ea65b1e19f3b9bd3afea04612f204c244bf5a23b328ce",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.7",
      "creation_date": "2021-08-20T14:38:38.562000+00:00",
      "csv_description": "Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs, Infiniband adapters and other devices through the [device plugin framework](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/). However, configuring and managing nodes with these hardware resources requires configuration of multiple software components such as drivers, container runtimes or other libraries which are difficult and prone to errors.\nThe NVIDIA GPU Operator uses the [operator framework](https://coreos.com/blog/introducing-operator-framework) within Kubernetes to automate the management of all NVIDIA software components needed to provision and monitor GPUs. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling and NVIDIA DCGM exporter.\nVisit the official site of the [GPU Operator](https://github.com/NVIDIA/gpu-operator) for more information. For getting started with using the GPU Operator with OpenShift, see the instructions [here](https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/index.html).\n",
      "csv_display_name": "NVIDIA GPU Operator",
      "csv_metadata_description": "Automate the management and monitoring of NVIDIA GPUs.",
      "csv_name": "gpu-operator-certified.v1.7.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-05T10:57:10.693000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gpu-operator-certified",
      "provided_apis": [
        {
          "group": "nvidia.com",
          "kind": "ClusterPolicy",
          "plural": "clusterpolicies",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:3a812cf113f416baca9262fa8423f36141f35696eb6e7a51a7abb40f5ccd5f8c",
          "image": "nvcr.io/nvidia/gpu-operator@sha256:3a812cf113f416baca9262fa8423f36141f35696eb6e7a51a7abb40f5ccd5f8c",
          "name": "gpu-operator-image"
        },
        {
          "digest": "sha256:8af02463a8b60b21202d0bf69bc1ee0bb12f684fa367f903d138df6cacc2d0ac",
          "image": "nvcr.io/nvidia/k8s/dcgm-exporter@sha256:8af02463a8b60b21202d0bf69bc1ee0bb12f684fa367f903d138df6cacc2d0ac",
          "name": "dcgm-exporter-image"
        },
        {
          "digest": "sha256:ffa284f1f359d70f0e1d6d8e7752d7c92ef7445b0d74965a8682775de37febf8",
          "image": "nvcr.io/nvidia/k8s/container-toolkit@sha256:ffa284f1f359d70f0e1d6d8e7752d7c92ef7445b0d74965a8682775de37febf8",
          "name": "container-toolkit-image"
        },
        {
          "digest": "sha256:09ba3eca64a80fab010a9fcd647a2675260272a8c3eb515dfed6dc38a2d31ead",
          "image": "nvcr.io/nvidia/driver@sha256:09ba3eca64a80fab010a9fcd647a2675260272a8c3eb515dfed6dc38a2d31ead",
          "name": "driver-image"
        },
        {
          "digest": "sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "image": "nvcr.io/nvidia/k8s-device-plugin@sha256:85def0197f388e5e336b1ab0dbec350816c40108a58af946baa1315f4c96ee05",
          "name": "device-plugin-image"
        },
        {
          "digest": "sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "image": "nvcr.io/nvidia/gpu-feature-discovery@sha256:bfc39d23568458dfd50c0c5323b6d42bdcd038c420fb2a2becd513a3ed3be27f",
          "name": "gpu-feature-discovery-image"
        },
        {
          "digest": "sha256:495ed3b42e0541590c537ab1b33bda772aad530d3ef6a4f9384d3741a59e2bf8",
          "image": "nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:495ed3b42e0541590c537ab1b33bda772aad530d3ef6a4f9384d3741a59e2bf8",
          "name": "mig-manager-image"
        },
        {
          "digest": "sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "image": "nvcr.io/nvidia/cuda@sha256:15674e5c45c97994bc92387bad03a0d52d7c1e983709c471c4fecc8e806dbdce",
          "name": "init-container-image"
        },
        {
          "digest": "sha256:aa1f7bd526ae132c46f3ebe6ecfabe675889e240776ccc2155e31e0c48cc659e",
          "image": "nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:aa1f7bd526ae132c46f3ebe6ecfabe675889e240776ccc2155e31e0c48cc659e",
          "name": "gpu-operator-validator"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.7.1",
      "version_original": "1.7.1"
    },
    {
      "_id": "611fbe929f14588c41ebe999",
      "alm_examples": [
        {
          "kind": "AnzoGraph",
          "metadata": {
            "name": "azg01"
          },
          "spec": {
            "db": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_data": "anzograph-data-grpc",
                      "app_mgmt": "anzograph-mgmt-grpc"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_data": "anzograph-data-grpc",
                        "app_mgmt": "anzograph-mgmt-grpc"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
                          "name": "db",
                          "resources": {
                            "limits": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            },
                            "requests": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            },
            "deployFrontend": false,
            "frontend": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_mgmt": "anzograph-frontend"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_mgmt": "anzograph-frontend"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
                          "name": "frontend",
                          "resources": {
                            "limits": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator-bundle@sha256:9418a3505873ac387f2b5455ec7e696739a2bb6f03232f8c0f59489c605b96f2",
      "bundle_path_digest": "sha256:9418a3505873ac387f2b5455ec7e696739a2bb6f03232f8c0f59489c605b96f2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:39:14.376000+00:00",
      "csv_description": "The AnzoGraph Operator provides the way to install and configure an AnzoGraph\ncluster on Red Hat K8S environment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzograph/blob/v2.0.1/README_openshift_marketplace.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzograph/userdoc/ )\n\n### Support\n\nWe offer Support to our customers with the AnzoGraph db Enterprise Edition License [here]( https://customercenter.cambridgesemantics.com/ ). For AnzoGraph db Free Edition questions, get help from our Anzograph User Community at Stack Overflow. When submitting a question, include the tag 'anzograph'.",
      "csv_display_name": "AnzoGraph Operator",
      "csv_metadata_description": "kubernetes operator for AnzoGraph DB",
      "csv_name": "anzograph-operator.v2.0.102",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:55:03.945000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "anzograph-operator",
      "provided_apis": [
        {
          "group": "anzograph.clusters.cambridgesemantics.com",
          "kind": "AnzoGraph",
          "version": "v2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732",
          "name": "anzograph-operator-a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732-annotation"
        },
        {
          "digest": "sha256:a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:a5b212558f748ddc8156541b546290320b656506e039a18903ad9fa6e638e732",
          "name": "manager"
        },
        {
          "digest": "sha256:e3bdddf811b5ba69f388d023aea1ba538edd00e9f415c33ec555321d39129a36",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph@sha256:e3bdddf811b5ba69f388d023aea1ba538edd00e9f415c33ec555321d39129a36",
          "name": "anzograph_allinone"
        },
        {
          "digest": "sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
          "name": "anzograph_db"
        },
        {
          "digest": "sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
          "name": "anzograph_frontend"
        },
        {
          "digest": "sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3",
          "name": "anzograph-frontend-c1443bcdc8e998fc699f61980a665f1a083350adf2d3d1d55a7cb55e31ca50b3-annotation"
        },
        {
          "digest": "sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5",
          "name": "anzograph-db-8a5bd26551990929fdb776d6b421a9713cb84e03b19c18f81b625582f7f41af5-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.0.1",
      "version_original": "2.0.1"
    },
    {
      "_id": "611fbedd9f14588c41ebe9b3",
      "alm_examples": [
        {
          "kind": "VfunctionServer",
          "metadata": {
            "labels": {
              "app.kubernetes.io/instance": "vfunction-server",
              "app.kubernetes.io/managed-by": "operator",
              "app.kubernetes.io/name": "vfunction",
              "name": "vfunction"
            },
            "name": "example-vfunction-server",
            "namespace": "vfunction"
          },
          "spec": {
            "admin": {
              "email": "admin@mycompany.com",
              "name": "Admin",
              "password": "Password1!"
            },
            "host": "http://my.domain.com",
            "measurements": {
              "auto_scaling": "No",
              "max_number_of_services": "10",
              "min_number_of_services": "2"
            },
            "org_name": "MyCompany",
            "smtp": {
              "identity": "",
              "password": "",
              "url": "",
              "user": ""
            },
            "tls": {
              "crt": "",
              "key": ""
            },
            "upgrade": "Daily"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/vfunction/vfunction-server-operator-bundle@sha256:75cb80116afa46260ad077967679040ee495535933a640f152ed2e2012226d13",
      "bundle_path_digest": "sha256:75cb80116afa46260ad077967679040ee495535933a640f152ed2e2012226d13",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:40:29.445000+00:00",
      "csv_description": "\n## Introduction\n\n[vFunction](https://www.vfunction.com/) is a cloud-native modernization platform that combines dynamic and static code analysis, machine learning, and automation to automatically identify and extract services from existing applications. vFunction is the only platform purpose-built for modernization of Java applications.\n\nThe vFunction modernization process starts by learning the running monolithic application, and surfacing the interdependencies within it. Using AI, the platform analyzes and identifies services that can be separated from the application. This decomposition can present a range of micro, mini, or even macro services, depending on your application environment, each being an independently deployable and scalable application component.\n\nvFunction automates the extraction of these services, enabling you to modernize your monolith, quickly and easily.\n\n## The vFunction platform\n\nThe platform consists of 3 basic components; the server, the controller package, and a tools package. The server runs as an operand on an OpenShift environment. The controller package is installed on the machine that runs the monolithic application which can be either a Linux or a Windows machine, and the tools are run on a development machine, with access to the code of the monolithic application.\n\nThe controller package consists of three elements: the vFunction agent, that collects data during the dynamic analysis phase; the vFunction Viper application, that performs static analysis on the binaries of the application; and the vFunction controller that handles all the communication between the agent, Viper, and the vFunction server.\n\nThe vFunction agent is a mix of a Java and native agent, and needs to run on the JVM that is currently running your application. [Refer to the vFunction OOB Support Matrix document](https://drive.google.com/file/d/1ccq8LFab1FrYAimDUxwgjiCOdk4QuVzs/view) for a list of supported application servers and JVMs.\n\nThis operator installs a vFunction server instance to be connected later with one or more vFunction controllers installed on your application machine(s).\n\n## Before You Start\n\nThe vFunction operator requires a Red Hat OpenShift Kubernetes Application Platform 4.x.\n\nThe cluster recommended configuration (which is the same as the minimal one) should consist of:\n\n- *1 master node*\n\n- *1 worker node*\n\n- *Storage capacity enough for dynamically provision 2 PVs of 50G each*\n\n- *A default StorageClass configured*\n\nvFunction doesn\u2019t provide any encryption mechanism for data stored on any attached storage. If required, encryption can be achieved by the client by using its own external means on the storage itself.\n\nvFunction supports backup of all critical data as part of the application. Nevertheless, for complete backup of entire data it is recommended that the client apply its own storage based backup mechanism.\n\n## Install Prerequisites\n\n###Cluster:\nThe operator was certified and tested on OCP 4.6 and 4.7.\nIf you encounter any issue with other OCP versions or cloud providers please contact info@vfunction.com.\n\n###Storage:\nThe operator creates two new PersistentVolumeClaims (PVCs) during installation time. For their creation, it relies on the default StorageClass to dynamically provision two PersistentVolumes (PVs). The system must have a default StorageClass in place.\n\nBoth PersistentVolumes are accessed with ReadWriteOnce mode.\n\n###Project:\nThe operator should be installed in a new and dedicated project (namespace). If you intend to install more than one vFunction operands in the same cluster, please use different projects for each one. Installation of more than one operand on the same cluster will allow scaling by load balancing multiple applications to different vFunction servers.\n\n## Installation instructions\n\n1. Prepare the YAML file to use in the installation:\n\n    a. Copy the following YAML template into a text editor:\n\n    ```yaml\n        apiVersion: vfunction.com/v1\n        kind: VfunctionServer\n        metadata:\n          name: vfunction\n          namespace: vfunction\n          labels:\n            name: vfunction\n            app.kubernetes.io/name: vfunction\n            app.kubernetes.io/instance: vfunction-server\n            app.kubernetes.io/managed-by: operator\n        spec:\n          host: \"http://my.domain.com\"\n          org_name: \"MyCompany\"\n          upgrade: \"Daily\"\n          admin:\n            email: \"admin@mycompany.com\"\n            name: \"Admin\"\n            password: \"Password1!\"\n          smtp:\n            password: \"\"\n            url: \"\"\n            identity: \"\"\n            user: \"\"\n          measurements:\n            auto_scaling: \"No\"\n            min_number_of_services: \"2\"\n            max_number_of_services: \"10\"\n          tls:\n            crt: |\n              -----BEGIN CERTIFICATE-----\n              ...\n              -----END CERTIFICATE-----\n            key: |\n              -----BEGIN PRIVATE KEY-----\n              ...\n              -----END PRIVATE KEY-----\n    ```\n    b. Customize the template fields by replacing with your information:\n\n    - **host**: Enter the FQDN for accessing the vFunction dashboard.\n                Make sure that you use a domain name and not an IP address. Writing \"https://\" will mandate TLS while writing \"http://\" will expose the server through HTTP.\n    - **org_name**: Enter your organization name.\n    - **upgrade**: Choose auto-upgrade mode\n        - **Daily**: The operator will check for a new version every day at 3 AM and will install it automatically (default).\n        - **Always**: The operator will check for a new version every 10 minutes and will install it automatically.\n        - **Never**: The automatic upgrading mechanism is off.\n        - **Maintenance**: Put the operator in a maintenance mode.\n\n    - **admin.email**: Enter the email address of your administrator.\n    - **admin.name**: Enter the name of your administrator.\n    - **admin.password**: Enter the password you want to use for the vFunction administrator.\n                          The password should be at least 8 characters long, and consist of at least one lowercase letter, at least one uppercase letter, at least one number, and at least one special character.\n    - **smtp.user**: Enter the email address for a designated user for the SMTP server (optional).\n    - **smtp.password**: Enter this user\u2019s password (optional).\n    - **smtp.identity**: Enter this user\u2019s password (optional).\n    - **smtp.url**: Enter the SMTP server URL (optional).\n    - **tls.crt**: If your host FQDN starts with \u201chttps\u201d, paste in the certifications you have for using the TLS connection.\n    - **tls.key**: If your host FQDN starts with \u201chttps\u201d, enter the key you have for using the TLS connection.\n    - **measurements.auto_scaling**: Indicates if the measurement services auto scalling is active.\n    - **measurements.min_num_of_services**: Minimum Number of measurement service pods.\n    - **measurements.max_num_of_services**: Maximum Number of measurement service pods.\n    - **measurements.S3**: Save all measurements data to S3, instead of local PV (optional).\n        - **measurements.S3.bucket**: Measurements S3 bucket name.\n        - **measurements.S3.key**: Measurements S3 key.\n        - **measurements.S3.secret**: Measurements S3 secret.\n        - **measurements.S3.region**: Measurements S3 region.\n    - **backup.mysql.S3**: Backup MySQL to S3 (optional).\n        - **backup.mysql.S3.bucket**: MySQL backup S3 bucket name.\n        - **backup.mysql.S3.key**: MySQL backup S3 key.\n        - **backup.mysql.S3.secret**: MySQL backup S3 secret.\n        - **backup.mysql.S3.region**: MySQL backup S3 region.\n\n1. In the Installed Operators window choose and click on vFunction Operator and then Create Instance.\n1. In the Create vFunctionServer window, paste the YAML file that you created in step 1.\n1. Click Create.\n1. You can now click on your new created vFunction operand in order to see its details and installation progress.\n\n## Verifying the installation\n\nCheck that the operand installed successfully:\n\n1. Check that the \"Operand State\" property shows \u201cWorking\u201d.\n1. Check that the \"Successfully Installed\" property shows \"Yes\".\n\nIf \"Operand State\" shows \"Failed\", it indicates the operand wasn\u2019t installed correctly. Check the events and logs for all pods (operator and image containers) for any issues. In the event that you cannot troubleshoot, contact vFunction support.\n\n## After installation\n\nThe vFunction site is now accessible via the newly created vFunction application custom address, for example, my.domain.com. There are two ways you can access the vFunction dashboard.\n\n### Access using the router canonical hostname\n\n1. Update your DNS provider by creating a canonical name (CNAME) record.\nThis record should point to your host address, and to the \u201dvfunction\u201d subdomain of the  OpenShift canonical router hostname as the alias.\nFor example, ***my.domain.com.   CNAME   vfunction.apps.ocp4.my-openshift.com.***\n2. Find your cluster Router Canonical Hostname address in the newly created vfunction-route-xxx under your vFunction OpenShift project -> **Networking** -> **Routes** -> **vfunction-route-xxx** route -> **Router Canonical Hostname** field.\nYou can now access the dashboard using your defined \"host\" spec property (as above).\n\n### Access using the nginx service location\n\nYou may use this access method if your OpenShift is installed on a provider that supports exposing LoadBalancer-type services.\n1. Update your DNS provider by creating a canonical name (CNAME) record.\nThis record should point to your host address and to the vfunction-nginx-xxx service location.\nFor example, ***my.domain.com.   CNAME   a05951ed7cdf-1394239323.us-east-1.elb.amazonaws.com.***\n1. Point your custom domain to the vfunction-nginx-xxx service's external IP location, which you can find in the OpenShift project > **Networking** -> **Services** -> **vfunction-nginx-xxx** service > **Service Address** > **Location** field.\n\n## Upgrade and Rollback\n\nThe vFunction operator includes a built-in auto-upgrade mechanism.\n\nYou can choose one of 4 upgrading modes:\n- **Daily**: The operator will check for a new version every day at 3 AM and will install it automatically (default).\n- **Always**: The operator will check for a new version every 10 minutes and will install it automatically.\n- **Never**: The automatic upgrading mechanism is off.\n- **Maintenance**: Put the operator in a maintenance mode.\n\nFor an on-demand upgrade,  you can change anytime the upgrading mode from \u201cNever\u201d to \u201cAlways\u201d, wait for the operand to be upgraded, and change again to \u201cNever\u201d.\n\nDuring the upgrade, the \"Operand State\" property will change to \u201cUpgrading...\u201d and back again to \u201cWorking\u201d after a successful upgrade.\n\nIn case of a failure upgrading the operand, an automatic rollback to the last working version will occur. The failed version will be marked as defective and the operator will not try to upgrade to it again.\n\nChoosing the \"Maintenance\" option will tell the operator to take down all pods, so you can safely fix storage issues, return volumes from snapshots, etc.\nAfter finishing the maintenance, return the upgrade mode to your original desired policy.",
      "csv_display_name": "vFunction Operator",
      "csv_metadata_description": "vFunction is a cutting-edge code analysis, machine learning, and automation to boost your Java modernization projects.",
      "csv_name": "vfunction-server-operator.v2.2.469",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:02:17.503000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "vfunction-server-operator",
      "provided_apis": [
        {
          "group": "vfunction.com",
          "kind": "VfunctionServer",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:60d1965460101d0fd90ecc9499d0a6d4c9684a54ada61a43a253d30661fab22f",
          "image": "registry.connect.redhat.com/vfunction/vfunction-server-operator@sha256:60d1965460101d0fd90ecc9499d0a6d4c9684a54ada61a43a253d30661fab22f",
          "name": "vfunction-server-operator-60d1965460101d0fd90ecc9499d0a6d4c9684a54ada61a43a253d30661fab22f-annotation"
        },
        {
          "digest": "sha256:60d1965460101d0fd90ecc9499d0a6d4c9684a54ada61a43a253d30661fab22f",
          "image": "registry.connect.redhat.com/vfunction/vfunction-server-operator@sha256:60d1965460101d0fd90ecc9499d0a6d4c9684a54ada61a43a253d30661fab22f",
          "name": "vfunction-server-operator"
        },
        {
          "digest": "sha256:a6e06ea08fffa33b709d4d1240587bb3ab5b4cc6a28f493a227555e88e645798",
          "image": "registry.connect.redhat.com/vfunction/vfunction-mysql@sha256:a6e06ea08fffa33b709d4d1240587bb3ab5b4cc6a28f493a227555e88e645798",
          "name": "vfunction_mysql_original_image"
        },
        {
          "digest": "sha256:435d1489aaf29a811081ddb51f40ae692e5833f42c31dbced6bc189c0ddc40ab",
          "image": "registry.connect.redhat.com/vfunction/vfunction-nginx@sha256:435d1489aaf29a811081ddb51f40ae692e5833f42c31dbced6bc189c0ddc40ab",
          "name": "vfunction_nginx_original_image"
        },
        {
          "digest": "sha256:b778cb9c724a59e51b4cb9088047be471cdf338f1138c31b81f79e50a491203f",
          "image": "registry.connect.redhat.com/vfunction/vfunction-storage@sha256:b778cb9c724a59e51b4cb9088047be471cdf338f1138c31b81f79e50a491203f",
          "name": "vfunction_storage_original_image"
        },
        {
          "digest": "sha256:21ca95a0209f3c557a45658452d154777f928539a93129148470562c79e68d70",
          "image": "registry.connect.redhat.com/vfunction/vfunction-vfapi-idp@sha256:21ca95a0209f3c557a45658452d154777f928539a93129148470562c79e68d70",
          "name": "vfunction_vfapi_idp_original_image"
        },
        {
          "digest": "sha256:35ffbee4b0eead58706b23cf5a702b1d55d2a7913cd0019cefbff94957d4e5d6",
          "image": "registry.connect.redhat.com/vfunction/vfunction-vfapi-measurements@sha256:35ffbee4b0eead58706b23cf5a702b1d55d2a7913cd0019cefbff94957d4e5d6",
          "name": "vfunction_vfapi_measurements_original_image"
        },
        {
          "digest": "sha256:bcc2c3d6222af95e614318daea3108fffc6bc77aa17b23031cce296a3811b79f",
          "image": "registry.connect.redhat.com/vfunction/vfunction-vfapi-organizations@sha256:bcc2c3d6222af95e614318daea3108fffc6bc77aa17b23031cce296a3811b79f",
          "name": "vfunction_vfapi_organizations_original_image"
        },
        {
          "digest": "sha256:e47cc051953976a0e075e8cfb457cd4749e9b9e4394f2fbecab2458c2ade92ca",
          "image": "registry.connect.redhat.com/vfunction/vfunction-vfapi-users@sha256:e47cc051953976a0e075e8cfb457cd4749e9b9e4394f2fbecab2458c2ade92ca",
          "name": "vfunction_vfapi_users_original_image"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.2.469",
      "version_original": "2.2.469"
    },
    {
      "_id": "611fbedf9f14588c41ebe9b4",
      "alm_examples": [
        {
          "kind": "LinstorController",
          "metadata": {
            "name": "linstor"
          },
          "spec": {
            "controllerImage": "",
            "dbConnectionURL": "etcd://linstor-etcd:2379",
            "drbdRepoCred": "",
            "priorityClassName": ""
          }
        },
        {
          "kind": "LinstorCSIDriver",
          "metadata": {
            "name": "linstor"
          },
          "spec": {
            "controllerEndpoint": "http://linstor:3370",
            "controllerReplicas": 1,
            "csiControllerServiceAccountName": "csi-controller",
            "csiNodeServiceAccountName": "csi-node",
            "imagePullSecret": "",
            "linstorPluginImage": ""
          }
        },
        {
          "kind": "LinstorSatelliteSet",
          "metadata": {
            "name": "linstor-satellites"
          },
          "spec": {
            "automaticStorageType": "None",
            "controllerEndpoint": "http://linstor:3370",
            "drbdRepoCred": "",
            "kernelModuleInjectionMode": "ShippedModules",
            "priorityClassName": "",
            "satelliteImage": ""
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/linbit/linstor-operator-bundle@sha256:20c0b821db27ef8f551212d14ba508f88a116f92a9f9a214e79709744b96678b",
      "bundle_path_digest": "sha256:20c0b821db27ef8f551212d14ba508f88a116f92a9f9a214e79709744b96678b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-20T14:40:31.544000+00:00",
      "csv_description": "LINSTOR is a configuration management system for storage on Linux systems.\nIt manages LVM logical volumes and/or ZFS ZVOLs on a cluster of nodes.\nIt leverages DRBD for replication between different nodes and to provide block\nstorage devices to users and applications. It manages snapshots, encryption and\ncaching of HDD backed data in SSDs via bcache.\n\nLINBIT provides a certified LINSTOR operator to ease deployment of LINSTOR\non Openshift by installing DRBD, managing Satellite and Controller pods,\nand integrating with Openshift to provision persistent storage for your workloads.\n\nFor detailed instructions and more configuration options, see our [user guide].\n\n[user guide]: https://www.linbit.com/drbd-user-guide/linstor-guide-1_0-en/#ch-openshift\n\n## Install\n\nUnlike deployment via the helm chart, the certified Openshift\noperator does not deploy the needed etcd cluster. You must deploy this\nyourself ahead of time. We do this via the etcd operator available in the\nOperatorHub.\n\nIMPORTANT: It is advised that the etcd deployment uses persistent\nstorage of some type. Either use an existing storage provisioner with\na default `StorageClass` or simply use `hostPath` volumes.\n\n### Installing the operator\n\nHit \"Install\", select the stable update channel and a namespace for the\noperator. Use of a new namespace is recommended.\n\nHit \"Install\" again. At this point you should have just one pod, the\noperator pod, running. Next we needs to configure the remaining provided APIs.\n\n#### A note on operator namespaces\nThe LINSTOR operator can only watch for events and manage\ncustom resources that are within the same namespace it is deployed\nwithin (OwnNamsespace). This means the LINSTOR Controller, LINSTOR\nSatellites, and LINSTOR CSI Driver pods all need to be deployed in the\nsame namsepace as the LINSTOR Operator pod.\n\n### Deploying the LINSTOR Controller\n\nNavigate to the left-hand control pane of the Openshift Web\nConsole. Expand the \"Operators\" section, selecting \"Installed Operators\".\nFind the entry for the \"Linstor Operator\", then select the \"LinstorController\"\nfrom the \"Provided APIs\" column on the right.\n\nFrom here you should see a page that says \"No Operands Found\" and will\nfeature a large button on the right which says \"Create\nLinstorController\". Click the \"Create LinstorController\" button.\n\nHere you will be presented with options to configure the LINSTOR\nController. Either via the web-form view or the YAML View. Regardless\nof which view you select, make sure that the `dbConnectionURL` matches\nthe endpoint provided from your etcd deployment. Otherwise, the\ndefaults are usually fine for most purposes.\n\nLastly hit \"Create\", you should now see a linstor-controller pod\nrunning.\n\n### Deploying the LINSTOR Satellites\n\nNext we need to deploy the Satellites Set. Just as before navigate\nto the left-hand control pane of the Openshift Web Console. Expand the\n\"Operators\" section, but this time select \"Installed Operators\". Find\nthe entry for the \"Linstor Operator\", then select the\n\"LinstorSatelliteSet\" from the \"Provided APIs\" column on the right.\n\nFrom here you should see a page that says \"No Operands Found\" and will\nfeature a large button on the right which says \"Create\nLinstorSatelliteSet\". Click the \"Create LinstorSatelliteSet\" button.\n\nHere you will be presented with the options to configure the LINSTOR\nSatellites. The defaults should be enough to get you started.\nMake sure the `controllerEndpoint` matches what is available in the\nopenshift endpoints. The default is usually correct here.\n\nYou can edit the `storagePools` section to configure LINSTOR storage pools,\nincluding preparing the backing devices. See our [storage guide].\n\n[storage guide]: https://www.linbit.com/drbd-user-guide/linstor-guide-1_0-en/#s-kubernetes-storage\n\nBelow is an example manifest:\n\n```yaml\napiVersion: linstor.linbit.com/v1\nkind: LinstorSatelliteSet\nmetadata:\n  name: linstor-satellites\nspec:\n  satelliteImage: ''\n  automaticStorageType: None\n  storagePools:\n    lvmThinPools:\n    - name: openshift-pool\n      volumeGroup: \"\"\n      thinVolume: openshift\n      devicePaths:\n      - /dev/vdb\n  drbdRepoCred: ''\n  kernelModuleInjectionMode: ShippedModules\n  controllerEndpoint: 'http://linstor:3370'\n  priorityClassName: ''\n```\n\nLastly hit \"Create\", you should now see a linstor-node pod\nrunning on every worker node.\n\n### Deploying the LINSTOR CSI driver\n\nLast bit left is the CSI pods to bridge the layer between the CSI and\nLINSTOR. Just as before navigate to the left-hand control pane of the\nOpenshift Web Console. Expand the \"Operators\" section, but this time\nselect \"Installed Operators\". Find the entry for the \"Linstor Operator\",\nthen select the \"LinstorCSIDriver\" from the \"Provided APIs\" column on the\nright.\n\nFrom here you should see a page that says \"No Operands Found\" and will\nfeature a large button on the right which says \"Create\nLinstorCSIDriver\". Click the \"Create LinstorCSIDriver\" button.\n\nAgain, you will be presented with the options. Make sure that the\n`controllerEndpoint` is correct. Otherwise the defaults are fine for\nmost use cases.\n\nLastly hit \"Create\". You will now see a single \"linstor-csi-controller\" pod,\nas well as a \"linstor-csi-node\" pod on all worker nodes.\n\n## Interacting with LINSTOR in Openshift.\n\nThe Controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.\nFor instance:\n\n```\noc exec deployment/linstor-cs-controller -- linstor storage-pool list\n```\n\nThis should only be necessary for investigating problems and accessing advanced functionality.\nRegular operation such as creating volumes should be achieved via the [Openshift/Kubernetes integration].\n\n[Openshift/Kubernetes integration]: https://www.linbit.com/drbd-user-guide/linstor-guide-1_0-en/#s-kubernetes-basic-configuration-and-deployment",
      "csv_display_name": "Linstor Operator",
      "csv_metadata_description": "LINSTOR Kubernetes Operator",
      "csv_name": "linstor-operator.v1.5.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:59:11.917000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "linstor-operator",
      "provided_apis": [
        {
          "group": "linstor.linbit.com",
          "kind": "LinstorCSIDriver",
          "version": "v1"
        },
        {
          "group": "linstor.linbit.com",
          "kind": "LinstorSatelliteSet",
          "version": "v1"
        },
        {
          "group": "linstor.linbit.com",
          "kind": "LinstorController",
          "version": "v1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.5.1",
      "version_original": "1.5.1"
    },
    {
      "_id": "611fbee0dece23122b7a790f",
      "alm_examples": [
        {
          "kind": "EntandoDeBundle",
          "metadata": {
            "name": "my-bundle",
            "namespace": "my-namespace"
          },
          "spec": {
            "details": {
              "name": "my-bundle"
            }
          }
        },
        {
          "kind": "EntandoDatabaseService",
          "metadata": {
            "name": "my-entando-database-service",
            "namespace": "my-namespace"
          },
          "spec": {
            "createDeployment": true,
            "databaseName": "my_database",
            "dbms": "postgresql"
          }
        },
        {
          "kind": "EntandoKeycloakServer",
          "metadata": {
            "name": "my-keycloak",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "replicas": 1,
            "standardImage": "redhat-sso"
          }
        },
        {
          "kind": "EntandoClusterInfrastructure",
          "metadata": {
            "name": "my-entando-cluster-infrastructure",
            "namespace": "my-namespace"
          },
          "spec": {
            "environmentVariables": [],
            "replicas": 1
          }
        },
        {
          "kind": "EntandoPlugin",
          "metadata": {
            "name": "my-entando-plugin",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "healthCheckPath": "/management/health",
            "image": "entando/entando-avatar-plugin:6.0.5",
            "ingressHostName": "my-plugin.apps-crc.testing",
            "ingressPath": "/avatarPlugin",
            "replicas": 1,
            "roles": [
              {
                "code": "admin",
                "name": "user"
              },
              {
                "code": "user",
                "name": "user"
              }
            ],
            "securityLevel": "strict"
          }
        },
        {
          "kind": "EntandoApp",
          "metadata": {
            "name": "my-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "dbms": "postgresql",
            "environmentVariables": [],
            "ingressHostName": "my-app.apps-crc.testing",
            "replicas": 1,
            "standardServerImage": "eap"
          }
        },
        {
          "kind": "EntandoAppPluginLink",
          "metadata": {
            "name": "my-link",
            "namespace": "my-namespace"
          },
          "spec": {
            "entandoAppName": "my-app",
            "entandoPluginName": "my-plugin"
          }
        },
        {
          "kind": "EntandoCompositeApp",
          "metadata": {
            "name": "my-entando-composite-app",
            "namespace": "my-namespace"
          },
          "spec": {
            "components": [
              {
                "kind": "EntandoDatabaseService",
                "metadata": {
                  "name": "inline-entando-database-service"
                },
                "spec": {
                  "createDeployment": true
                }
              },
              {
                "kind": "EntandoKeycloakServer",
                "metadata": {
                  "name": "inline-keycloak"
                },
                "spec": {
                  "standardImage": "redhat-sso"
                }
              },
              {
                "kind": "EntandoClusterInfrastructure",
                "metadata": {
                  "name": "inline-entando-cluster-infrastructure"
                },
                "spec": {}
              },
              {
                "kind": "EntandoApp",
                "metadata": {
                  "name": "inline-app"
                },
                "spec": {
                  "standardServerImage": "eap"
                }
              },
              {
                "kind": "EntandoPlugin",
                "metadata": {
                  "name": "inline-plugin"
                },
                "spec": {
                  "healthCheckPath": "/management/health",
                  "image": "entando/entando-avatar-plugin:6.0.5",
                  "ingressPath": "/avatarPlugin",
                  "roles": [
                    {
                      "code": "admin",
                      "name": "admin"
                    },
                    {
                      "code": "user",
                      "name": "user"
                    }
                  ]
                }
              },
              {
                "kind": "EntandoAppPluginLink",
                "metadata": {
                  "name": "inline-link"
                },
                "spec": {
                  "entandoAppName": "inline-app",
                  "entandoPluginName": "inline-plugin"
                }
              }
            ],
            "dbmsOverride": "postgresql",
            "ingressHostNameOverride": "entando.apps-crc.testing"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/entando/entando-operator@sha256:110de9cf2e6ecc1d3f6bff990ec9eb890b26c6ade5c03a232421d30e6bbace57",
      "bundle_path_digest": "sha256:110de9cf2e6ecc1d3f6bff990ec9eb890b26c6ade5c03a232421d30e6bbace57",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-20T14:40:32.523000+00:00",
      "csv_description": "## Entando\n\nThe Entando platform accelerates the development and lifecycle  management of fully modularized applications on Kubernetes. It  provides tools to help developers create and manage applications  using modular frontend and backend components.\nThe Entando Operator automates the installation, provisioning, and  configuration management of the components that make up an Entando  application. Specifically, the operator manages the following custom  resources:\n**EntandoKeycloakServers** for centralized authentication of frontend  and backend components. The operator can deploy Keycloak, or in certified  environments, Red Hat SSO servers that can then be used by subsequent  deployments as an OpenID Connect provider.\n**EntandoApps** for hosting an Entando application. EntandoApps are hosted on Wildfly or JBoss EAP containers, and can also be used to deploy custom  EntandoApp containers.\n**EntandoPlugins** for deploying microservices to customize or enhance your EntandoApp. Entando microservice plugins are deployed to your cluster, and  then linked to one or more EntandoApps.\n## Using the Operator\nThe Entando Operator can be deployed using the default settings without any  configuration. Once deployed, the operator can be customized by editing  the *configmap* and secrets.\n### ConfigMap: entando-operator-config\nThe 'entando-operator-config' ConfigMap can be added after deployment and any changes to it will be picked up by the operator on subsequent event processing. It supports the following keys:\n\n    entando.k8s.operator.gc.controller.pods: set this to \"false\" to keep controller pods after completion.\n    entando.k8s.operator.compliance.mode: set this to \"community\" if there is no requirement for Red Hat compliance.\n    entando.k8s.operator.image.pull.secrets: a comma separated list containing the names of pull secrets that will be linked to all service accounts.\n    entando.k8s.operator.disable.pvc.garbage.collection: set this to \"false\" if you want Persistent Volume Claims to be deleted with the custom resources they are associated with.\n    entando.k8s.operator.impose.default.limits: set this to \"false\" if there is no need to limit the resource consumption of pods on your cluster.\n    entando.k8s.operator.request.to.limit.ratio: a decimal number that default limits will be multiplied by to calculate default requests for resources.\n    entando.k8s.operator.force.db.password.reset: set this to \"true\" if you plan to delete Secrets from your namespace but you want to retain the Database they point to.\n    entando.k8s.operator.pull.policy.override: specify your preferred pullPolicy for images. The default is Always.\n    entando.tls.secret.name: The name of a standard TLS secret to use for HTTPS Ingresses. See the section entando-tls-secret.\n    entando.ca.secret.name: The name of a secret containing CA certificates. See the section entando-ca-cert-secret.\n    entando.assume.external.https.provider: Set this to \"true\" if your cloud provider handles HTTPS for you.\n    entando.use.auto.cert.generation: Set this to \"true\" to have Openshift use its internal CA to generate certificates for your Routes.\n    entando.default.routing.suffix: The domain name that can be suffixed to deployment names when the ingressHostName is omitted. Needs to be preconfigured on your DNS provider.\n    entando.pod.completion.timeout.seconds: The time it will take before Entando fails a run-to-completion Pod.\n    entando.pod.readiness.timeout.seconds: The time it will take before Entando fails a Service Pod.\n    entando.pod.shutdown.timeout.seconds: The time Entando will give a Pod to shutdown gracefully.\n\n\n### entando-pull-secret\nThe secret to be used to pull Entando images from the Red Hat container registry. The name of this secret is hard coded as it is required for the ClusterServiceVersion of this Operator\n### entando-tls-secret\nA standard Kubernetes TLS secret that will be used on all deployments where no custom TLS secret name is specified.\n### entando-ca-cert-secret\nThis is an opaque secret in the Entando Operator's namespace that contains the certificates of all trusted certificate authorities in your environment. This is generally used mainly for self signed certificates. As is generally the case for opaque secrets, there are no constraints on the keys in this secret. However, limit the files inside the secret to X509 certificates or certificate chains. The Entando Operator will load all of these files into a Java keystore that it then configures as the trust store for each container that uses Java.\n",
      "csv_display_name": "Entando Operator",
      "csv_metadata_description": "Processes EntandoKeycloakServer, EntandoApp and EntandoPlugin custom resources and deploys the relevant containers in the Kubernetes cluster.",
      "csv_name": "entando-k8s-operator.v6.3.2-pr2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-05T10:56:43.421000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "entando-k8s-operator",
      "provided_apis": [
        {
          "group": "entando.org",
          "kind": "EntandoApp",
          "plural": "entandoapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoClusterInfrastructure",
          "plural": "entandoclusterinfrastructures",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoCompositeApp",
          "plural": "entandocompositeapps",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDatabaseService",
          "plural": "entandodatabaseservices",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoDeBundle",
          "plural": "entandodebundles",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoKeycloakServer",
          "plural": "entandokeycloakservers",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoPlugin",
          "plural": "entandoplugins",
          "version": "v1"
        },
        {
          "group": "entando.org",
          "kind": "EntandoAppPluginLink",
          "plural": "entandoapppluginlinks",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839",
          "image": "entando/entando-k8s-controller-coordinator@sha256:c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839",
          "name": "entando-k8s-controller-coordinator-c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839-annotation"
        },
        {
          "digest": "sha256:c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839",
          "image": "entando/entando-k8s-controller-coordinator@sha256:c8a93ebd69af3cf822dd7d4fcce5d4241cd8dca229c4e25c771bcfecca916839",
          "name": "entando-operator"
        },
        {
          "digest": "sha256:570bce7dc649ac7ac85e7ecb196aabc281c77a94590aa7df7d9746869ce757ea",
          "image": "registry.redhat.io/rhel8/mysql-80@sha256:570bce7dc649ac7ac85e7ecb196aabc281c77a94590aa7df7d9746869ce757ea",
          "name": "rhel8_mysql_80"
        },
        {
          "digest": "sha256:f4e5c728b644bf1888ec8086424852ed74b5596a511be29e636fb10218fc9b6f",
          "image": "registry.redhat.io/rhel8/postgresql-12@sha256:f4e5c728b644bf1888ec8086424852ed74b5596a511be29e636fb10218fc9b6f",
          "name": "rhel8_postgresql_12"
        },
        {
          "digest": "sha256:1ca90c09302b9cadf8950278d92ad4d7f3a14f02dea8704320794aa77ada8631",
          "image": "docker.io/entando/app-builder@sha256:1ca90c09302b9cadf8950278d92ad4d7f3a14f02dea8704320794aa77ada8631",
          "name": "app_builder"
        },
        {
          "digest": "sha256:668191fc86b090452baaba18ec7bbc3d07df5a99750e0c5b98759599d565cfbe",
          "image": "docker.io/entando/entando-component-manager@sha256:668191fc86b090452baaba18ec7bbc3d07df5a99750e0c5b98759599d565cfbe",
          "name": "entando_component_manager"
        },
        {
          "digest": "sha256:1f0e35d7f7ee8123721cf8e80ac96ac2f594f21a2b68cf9e720917d090c8095e",
          "image": "docker.io/entando/entando-de-app-eap@sha256:1f0e35d7f7ee8123721cf8e80ac96ac2f594f21a2b68cf9e720917d090c8095e",
          "name": "entando_de_app_eap"
        },
        {
          "digest": "sha256:c68d3b129fc625f5d1ee10ed2a107e28c145049d4b513dcdb2c736555025e339",
          "image": "docker.io/entando/entando-k8s-app-controller@sha256:c68d3b129fc625f5d1ee10ed2a107e28c145049d4b513dcdb2c736555025e339",
          "name": "entando_k8s_app_controller"
        },
        {
          "digest": "sha256:aa7ba0861705213434e610530c5bf5da1bdb33f1834d3c346174f5a28145c0bd",
          "image": "docker.io/entando/entando-k8s-app-plugin-link-controller@sha256:aa7ba0861705213434e610530c5bf5da1bdb33f1834d3c346174f5a28145c0bd",
          "name": "entando_k8s_app_plugin_link_controller"
        },
        {
          "digest": "sha256:86dbea6696349d7b2393e12bacef4d1acec2dd3f8551e397fa26c92706a18dd3",
          "image": "docker.io/entando/entando-k8s-cluster-infrastructure-controller@sha256:86dbea6696349d7b2393e12bacef4d1acec2dd3f8551e397fa26c92706a18dd3",
          "name": "entando_k8s_cluster_infrastructure_controller"
        },
        {
          "digest": "sha256:4bc6797bb41b3e407b64530c3cff7a78f3fc2c22863caad9c606ccd650310b61",
          "image": "docker.io/entando/entando-k8s-composite-app-controller@sha256:4bc6797bb41b3e407b64530c3cff7a78f3fc2c22863caad9c606ccd650310b61",
          "name": "entando_k8s_composite_app_controller"
        },
        {
          "digest": "sha256:58ca4ad2fca83a715aba9b68609f9056f633d556eb9845b3aff5c05769369539",
          "image": "docker.io/entando/entando-k8s-database-service-controller@sha256:58ca4ad2fca83a715aba9b68609f9056f633d556eb9845b3aff5c05769369539",
          "name": "entando_k8s_database_service_controller"
        },
        {
          "digest": "sha256:8ae47dbf93201965106663299f06daad544982fa244d4a0f365a1b717727e5ed",
          "image": "docker.io/entando/entando-k8s-dbjob@sha256:8ae47dbf93201965106663299f06daad544982fa244d4a0f365a1b717727e5ed",
          "name": "entando_k8s_dbjob"
        },
        {
          "digest": "sha256:f1ed42001260802353197433cea7df4c2d0e5fd9bcd926bb5ae7c654dfad2ffe",
          "image": "docker.io/entando/entando-k8s-keycloak-controller@sha256:f1ed42001260802353197433cea7df4c2d0e5fd9bcd926bb5ae7c654dfad2ffe",
          "name": "entando_k8s_keycloak_controller"
        },
        {
          "digest": "sha256:a20c753dda86e0f54f210e446b404733045ba426fc7186dc3936daed8224fbcb",
          "image": "docker.io/entando/entando-k8s-plugin-controller@sha256:a20c753dda86e0f54f210e446b404733045ba426fc7186dc3936daed8224fbcb",
          "name": "entando_k8s_plugin_controller"
        },
        {
          "digest": "sha256:775391f0005e64764288bfe007d9ea576f13618c02ea400d030a7502cfb551bd",
          "image": "docker.io/entando/entando-k8s-service@sha256:775391f0005e64764288bfe007d9ea576f13618c02ea400d030a7502cfb551bd",
          "name": "entando_k8s_service"
        },
        {
          "digest": "sha256:3692adb3694b3bf278bed7a678c9d0eaa0aba8e3e3ed4117ca04156525bcac72",
          "image": "docker.io/entando/entando-redhat-sso@sha256:3692adb3694b3bf278bed7a678c9d0eaa0aba8e3e3ed4117ca04156525bcac72",
          "name": "entando_redhat_sso"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "6.3.2-pr2",
      "version_original": "6.3.2-pr2"
    },
    {
      "_id": "612318869f14588c41ebe9ec",
      "alm_examples": [
        {
          "kind": "AKOConfig",
          "metadata": {
            "name": "ako-sample",
            "namespace": "avi-system"
          },
          "spec": {
            "akoSettings": {
              "apiServerPort": 8080,
              "clusterName": "my-cluster",
              "cniPlugin": "",
              "deleteConfig": false,
              "disableStaticRouteSync": false,
              "enableEVH": false,
              "fullSyncFrequency": "1800",
              "layer7Only": false,
              "logLevel": "INFO",
              "servicesAPI": false
            },
            "controllerSettings": {
              "cloudName": "Default-Cloud",
              "controllerIP": "",
              "controllerVersion": "20.1.5",
              "serviceEngineGroupName": "Default-Group"
            },
            "imagePullPolicy": "IfNotPresent",
            "imageRepository": "projects.registry.vmware.com/ako/amko",
            "l4Settings": {
              "advancedL4": false,
              "autoFQDN": "default",
              "defaultDomain": ""
            },
            "l7Settings": {
              "defaultIngController": true,
              "noPGForSNI": false,
              "passthroughShardSize": "SMALL",
              "serviceType": "ClusterIP",
              "shardVSSize": "LARGE"
            },
            "logFile": "avi.log",
            "mountPath": "/log",
            "networkSettings": {
              "subnetIP": "",
              "subnetPrefix": "",
              "vipNetworkList": [
                {
                  "networkName": ""
                }
              ]
            },
            "nodePortSelector": {
              "key": "",
              "value": ""
            },
            "resources": {
              "limits": {
                "cpu": "250m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/wavefronthq/ako-operator-bundle@sha256:6a41ad6e74de7e5c94af7781a001c4501fae4b7b5505dcbfd75b04053af1e1ce",
      "bundle_path_digest": "sha256:6a41ad6e74de7e5c94af7781a001c4501fae4b7b5505dcbfd75b04053af1e1ce",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-23T03:39:50.214000+00:00",
      "csv_description": "Operator to manage the artifacts of the AKO Controller",
      "csv_display_name": "AKO Operator",
      "csv_metadata_description": "",
      "csv_name": "ako-operator.v1.4.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:37:20.744000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "ako-operator",
      "provided_apis": [
        {
          "group": "ako.vmware.com",
          "kind": "AKOConfig",
          "plural": "akoconfigs",
          "version": "v1alpha1"
        },
        {
          "group": "ako.vmware.com",
          "kind": "AviInfraSetting",
          "plural": "aviinfrasettings",
          "version": "v1alpha1"
        },
        {
          "group": "ako.vmware.com",
          "kind": "HostRule",
          "plural": "hostrules",
          "version": "v1alpha1"
        },
        {
          "group": "ako.vmware.com",
          "kind": "HTTPRule",
          "plural": "httprules",
          "version": "v1alpha1"
        },
        {
          "group": "networking.x-k8s.io",
          "kind": "GatewayClass",
          "plural": "gatewayclasses",
          "version": "v1alpha1"
        },
        {
          "group": "networking.x-k8s.io",
          "kind": "Gateway",
          "plural": "gateways",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:3840a444c7a9e8f799e251779dca616fddc412712084da2620178b705117c493",
          "image": "projects.registry.vmware.com/ako/ako-operator@sha256:3840a444c7a9e8f799e251779dca616fddc412712084da2620178b705117c493",
          "name": "ako-operator"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.4.2",
      "version_original": "1.4.2"
    },
    {
      "_id": "6125de5fbd674341b5c5f476",
      "alm_examples": [
        {
          "kind": "CcmNodeAgent",
          "metadata": {
            "name": "ccmnodeagent-sample"
          },
          "spec": {
            "ccmJoinToken": {
              "secretName": "ccm-join-token",
              "token": ""
            },
            "fullnameOverride": "",
            "image": {
              "ccmNodeAgent": {
                "pullPolicy": "Always",
                "repository": "fortanix/ccm-node-agent-ubi7",
                "tag": "3.5.475"
              },
              "ccmNodeAgentInit": {
                "pullPolicy": "IfNotPresent",
                "repository": "busybox",
                "tag": "latest"
              },
              "sgxSoftwareInstaller": {
                "pullPolicy": "Always",
                "repository": "fortanix/sgx-software-installer-ubi7",
                "tag": "3.5.475"
              }
            },
            "log": {
              "debug": false
            },
            "nameOverride": "",
            "podAnnotations": {},
            "ports": {
              "containerPort": 9092,
              "hostPort": 9092
            },
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "100m",
                "memory": "128Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "128Mi"
              }
            },
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/fortanix/ccm-node-agent-operator-bundle@sha256:8c697c066566c5bf5c0d4c776ec0f7950a6bba26565d20c58a0bca26dd15f3e6",
      "bundle_path_digest": "sha256:8c697c066566c5bf5c0d4c776ec0f7950a6bba26565d20c58a0bca26dd15f3e6",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-25T06:08:31.549000+00:00",
      "csv_description": "CCM Node Agent software enables registration of the compute nodes to Fortanix CCM when installed in the Openshift Cluster Nodes\u200b. \nIt also assists with application attestation and visibility for Fortanix CCM.\n\n## About this Operator\nThis Operator is based on a Helm chart for CCM Node Agent. It enrolls all the nodes in the openshift cluster to a particular CCM account. You have to provide the base64 encode of join token of CCM account while creating CcmNodeAgent resource.\n\n## Prerequisites\nAll the worker nodes in the cluster must have SGX capabilities.\n\n## Installation\n* Choose a namespace to install the Operator. Operator will be available in all namespaces.\n* Select Approval Strategy: Available options are Automatic or Manual. Automatic approval will automatically upgrade the running instance of your Operator without human intervention when a new version is available. In case of Manual approval, when a new version is available, OLM will create an update request. You must manually approve the request to update the Operator to a new version.\n* Generate your CCM Account Join Token: Please log in to https://ccm.fortanix.com, and in the Management Console tab, click the + ENROLL NODE button. In the ENROLL NODE screen, a Join Token will be generated in the text box for \"Get a join token to register an SGX compute node\". This Join Token is used by the compute node to authenticate itself.\n* Base64 encode the Join Token:\n```shell\necho -n <join-token> | openssl base64 -A\n```\n* Create an instance of CcmNodeAgent by setting the `spec.ccmJoinToken.Token` to the base64 encoded CCM account join token.\n* After CcmNodeAgent resource is installed successfully you should be able to see all the worker nodes of the cluster in your CCM account under Compute Node Tab.\n\n## Getting Started\nNow to run your application inside SGX enclaves on your cluster follow the [documentation](https://support.fortanix.com/hc/en-us/articles/360043527431-User-s-Guide-Add-and-Edit-an-Application).\n",
      "csv_display_name": "CCM Node Agent Operator",
      "csv_metadata_description": "CCM Node Agent software enables registration of the compute nodes to Fortanix CCM when installed in the Openshift Cluster Nodes",
      "csv_name": "ccm-node-agent-operator.v0.0.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:57:05.984000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "ccm-node-agent-operator",
      "provided_apis": [
        {
          "group": "charts.operatorhub.io",
          "kind": "CcmNodeAgent",
          "plural": "ccmnodeagents",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "image": "fortanix/ccm-node-agent-operator@sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "name": "ccm-node-agent-operator-73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4-annotation"
        },
        {
          "digest": "sha256:e30f7835cf52dbce9afdbaf55e5a93d8baf9c4b3a3ae488d1d42c1adba7f46af",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:e30f7835cf52dbce9afdbaf55e5a93d8baf9c4b3a3ae488d1d42c1adba7f46af",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "image": "fortanix/ccm-node-agent-operator@sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.0.1",
      "version_original": "0.0.1"
    },
    {
      "_id": "6125dedceb34b736524930f0",
      "alm_examples": [
        {
          "kind": "CcmNodeAgent",
          "metadata": {
            "name": "ccmnodeagent-sample"
          },
          "spec": {
            "ccmJoinToken": {
              "secretName": "ccm-join-token",
              "token": ""
            },
            "fullnameOverride": "",
            "image": {
              "ccmNodeAgent": {
                "pullPolicy": "Always",
                "repository": "fortanix/ccm-node-agent-ubi7",
                "tag": "3.5.475"
              },
              "ccmNodeAgentInit": {
                "pullPolicy": "IfNotPresent",
                "repository": "busybox",
                "tag": "latest"
              },
              "sgxSoftwareInstaller": {
                "pullPolicy": "Always",
                "repository": "fortanix/sgx-software-installer-ubi7",
                "tag": "3.5.475"
              }
            },
            "log": {
              "debug": false
            },
            "nameOverride": "",
            "podAnnotations": {},
            "ports": {
              "containerPort": 9092,
              "hostPort": 9092
            },
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "100m",
                "memory": "128Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "128Mi"
              }
            },
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/fortanix/ccm-node-agent-operator-bundle@sha256:8c697c066566c5bf5c0d4c776ec0f7950a6bba26565d20c58a0bca26dd15f3e6",
      "bundle_path_digest": "sha256:8c697c066566c5bf5c0d4c776ec0f7950a6bba26565d20c58a0bca26dd15f3e6",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-25T06:10:36.462000+00:00",
      "csv_description": "CCM Node Agent software enables registration of the compute nodes to Fortanix CCM when installed in the Openshift Cluster Nodes\u200b. \nIt also assists with application attestation and visibility for Fortanix CCM.\n\n## About this Operator\nThis Operator is based on a Helm chart for CCM Node Agent. It enrolls all the nodes in the openshift cluster to a particular CCM account. You have to provide the base64 encode of join token of CCM account while creating CcmNodeAgent resource.\n\n## Prerequisites\nAll the worker nodes in the cluster must have SGX capabilities.\n\n## Installation\n* Choose a namespace to install the Operator. Operator will be available in all namespaces.\n* Select Approval Strategy: Available options are Automatic or Manual. Automatic approval will automatically upgrade the running instance of your Operator without human intervention when a new version is available. In case of Manual approval, when a new version is available, OLM will create an update request. You must manually approve the request to update the Operator to a new version.\n* Generate your CCM Account Join Token: Please log in to https://ccm.fortanix.com, and in the Management Console tab, click the + ENROLL NODE button. In the ENROLL NODE screen, a Join Token will be generated in the text box for \"Get a join token to register an SGX compute node\". This Join Token is used by the compute node to authenticate itself.\n* Base64 encode the Join Token:\n```shell\necho -n <join-token> | openssl base64 -A\n```\n* Create an instance of CcmNodeAgent by setting the `spec.ccmJoinToken.Token` to the base64 encoded CCM account join token.\n* After CcmNodeAgent resource is installed successfully you should be able to see all the worker nodes of the cluster in your CCM account under Compute Node Tab.\n\n## Getting Started\nNow to run your application inside SGX enclaves on your cluster follow the [documentation](https://support.fortanix.com/hc/en-us/articles/360043527431-User-s-Guide-Add-and-Edit-an-Application).\n",
      "csv_display_name": "CCM Node Agent Operator",
      "csv_metadata_description": "CCM Node Agent software enables registration of the compute nodes to Fortanix CCM when installed in the Openshift Cluster Nodes",
      "csv_name": "ccm-node-agent-operator.v0.0.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:49:47.548000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "ccm-node-agent-operator",
      "provided_apis": [
        {
          "group": "charts.operatorhub.io",
          "kind": "CcmNodeAgent",
          "plural": "ccmnodeagents",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "image": "fortanix/ccm-node-agent-operator@sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "name": "ccm-node-agent-operator-73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4-annotation"
        },
        {
          "digest": "sha256:e30f7835cf52dbce9afdbaf55e5a93d8baf9c4b3a3ae488d1d42c1adba7f46af",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:e30f7835cf52dbce9afdbaf55e5a93d8baf9c4b3a3ae488d1d42c1adba7f46af",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "image": "fortanix/ccm-node-agent-operator@sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "0.0.1",
      "version_original": "0.0.1"
    },
    {
      "_id": "612608dedece23122b7a7e84",
      "alm_examples": [
        {
          "kind": "CcmNodeAgent",
          "metadata": {
            "name": "ccmnodeagent-sample"
          },
          "spec": {
            "ccmJoinToken": {
              "secretName": "ccm-join-token",
              "token": ""
            },
            "fullnameOverride": "",
            "image": {
              "ccmNodeAgent": {
                "pullPolicy": "Always",
                "repository": "fortanix/ccm-node-agent-ubi7",
                "tag": "3.5.475"
              },
              "ccmNodeAgentInit": {
                "pullPolicy": "IfNotPresent",
                "repository": "busybox",
                "tag": "latest"
              },
              "sgxSoftwareInstaller": {
                "pullPolicy": "Always",
                "repository": "fortanix/sgx-software-installer-ubi7",
                "tag": "3.5.475"
              }
            },
            "log": {
              "debug": false
            },
            "nameOverride": "",
            "podAnnotations": {},
            "ports": {
              "containerPort": 9092,
              "hostPort": 9092
            },
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "100m",
                "memory": "128Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "128Mi"
              }
            },
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/fortanix/ccm-node-agent-operator-bundle@sha256:8c697c066566c5bf5c0d4c776ec0f7950a6bba26565d20c58a0bca26dd15f3e6",
      "bundle_path_digest": "sha256:8c697c066566c5bf5c0d4c776ec0f7950a6bba26565d20c58a0bca26dd15f3e6",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-08-25T09:09:50.757000+00:00",
      "csv_description": "CCM Node Agent software enables registration of the compute nodes to Fortanix CCM when installed in the Openshift Cluster Nodes\u200b. \nIt also assists with application attestation and visibility for Fortanix CCM.\n\n## About this Operator\nThis Operator is based on a Helm chart for CCM Node Agent. It enrolls all the nodes in the openshift cluster to a particular CCM account. You have to provide the base64 encode of join token of CCM account while creating CcmNodeAgent resource.\n\n## Prerequisites\nAll the worker nodes in the cluster must have SGX capabilities.\n\n## Installation\n* Choose a namespace to install the Operator. Operator will be available in all namespaces.\n* Select Approval Strategy: Available options are Automatic or Manual. Automatic approval will automatically upgrade the running instance of your Operator without human intervention when a new version is available. In case of Manual approval, when a new version is available, OLM will create an update request. You must manually approve the request to update the Operator to a new version.\n* Generate your CCM Account Join Token: Please log in to https://ccm.fortanix.com, and in the Management Console tab, click the + ENROLL NODE button. In the ENROLL NODE screen, a Join Token will be generated in the text box for \"Get a join token to register an SGX compute node\". This Join Token is used by the compute node to authenticate itself.\n* Base64 encode the Join Token:\n```shell\necho -n <join-token> | openssl base64 -A\n```\n* Create an instance of CcmNodeAgent by setting the `spec.ccmJoinToken.Token` to the base64 encoded CCM account join token.\n* After CcmNodeAgent resource is installed successfully you should be able to see all the worker nodes of the cluster in your CCM account under Compute Node Tab.\n\n## Getting Started\nNow to run your application inside SGX enclaves on your cluster follow the [documentation](https://support.fortanix.com/hc/en-us/articles/360043527431-User-s-Guide-Add-and-Edit-an-Application).\n",
      "csv_display_name": "CCM Node Agent Operator",
      "csv_metadata_description": "CCM Node Agent software enables registration of the compute nodes to Fortanix CCM when installed in the Openshift Cluster Nodes",
      "csv_name": "ccm-node-agent-operator.v0.0.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:12:26.072000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "ccm-node-agent-operator",
      "provided_apis": [
        {
          "group": "charts.operatorhub.io",
          "kind": "CcmNodeAgent",
          "plural": "ccmnodeagents",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "image": "fortanix/ccm-node-agent-operator@sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "name": "ccm-node-agent-operator-73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4-annotation"
        },
        {
          "digest": "sha256:e30f7835cf52dbce9afdbaf55e5a93d8baf9c4b3a3ae488d1d42c1adba7f46af",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:e30f7835cf52dbce9afdbaf55e5a93d8baf9c4b3a3ae488d1d42c1adba7f46af",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "image": "fortanix/ccm-node-agent-operator@sha256:73309ff8e2ba77ec474dcfd19a29b624572bea6595990c64e6325e7987180ec4",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.0.1",
      "version_original": "0.0.1"
    },
    {
      "_id": "6127be69dece23122b7a8369",
      "alm_examples": [
        {
          "kind": "NexusIQ",
          "metadata": {
            "name": "example-nexusiq"
          },
          "spec": {
            "deployment": {
              "postStart": {},
              "preStart": {}
            },
            "fullnameOverride": "",
            "imagePullSecrets": [],
            "ingress": {
              "annotations": {},
              "enabled": false,
              "hosts": [],
              "path": "/",
              "tls": []
            },
            "iq": {
              "adminPort": 8071,
              "applicationPort": 8070,
              "configYaml": {
                "createSampleData": true,
                "server": {
                  "adminConnectors": [
                    {
                      "port": 8071,
                      "type": "http"
                    }
                  ],
                  "applicationConnectors": [
                    {
                      "port": 8070,
                      "type": "http"
                    }
                  ]
                },
                "sonatypeWork": "/sonatype-work"
              },
              "imageName": "registry.connect.redhat.com/sonatype/nexus-iq-server@sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
              "imagePullPolicy": "IfNotPresent",
              "imagePullSecret": "",
              "licenseSecret": "",
              "memory": "1Gi",
              "name": "nxiq"
            },
            "nameOverride": "",
            "persistence": {
              "accessMode": "ReadWriteOnce",
              "storageSize": "1Gi"
            },
            "service": {
              "annotations": {},
              "enabled": false,
              "labels": {},
              "ports": [
                {
                  "name": "nexus-service",
                  "port": 80,
                  "targetPort": 80
                }
              ]
            },
            "serviceAccount": {
              "create": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/sonatype/nxiq-operator-bundle@sha256:16ebccbf15157c2f7ba6d0873eead210f0c10fb4bce32e9f3fedcd1704fe7f30",
      "bundle_path_digest": "sha256:16ebccbf15157c2f7ba6d0873eead210f0c10fb4bce32e9f3fedcd1704fe7f30",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-26T16:16:41.332000+00:00",
      "csv_description": "Nexus Lifecycle is an open source governance platform that continuously identifies risk,\nautomatically enforces policy, and provides visibility throughout the entire SDLC.\nDevelopers leveraging open source require a solution that helps them make safer choices\nwhile still delivering at DevOps speed.\nNexus Lifecycle empowers developers and security teams to choose the best components\nand govern open source usage, ensuring organizations continue to innovate with less risk.\n\n## Core Capabilities\n\n* **Dependency Management**:\n  Generate a precise software bill of materials for all applications\n  to identify open source components and gather contextual direct\n  and transitive dependency information for faster action.\n* **One-Click Remediation**:\n  Upgrade to the best components and versions based on real-time intelligence.\n  Available in your preferred IDE or a GitHub pull request.\n* **Automated Open Source Policy Enforcement**:\n  Create custom security, license, and architectural policies based\n  on application type or organization and contextually enforce those policies\n  across every stage of the SDLC.\n* **Continuous Monitoring**:\n  Real-time notifications of newly discovered defects,\n  as well as vulnerabilities based on component,\n  risk level or application/container affected.\n\n## Extras\n\n* IDE Integrations: Eclipse, IntelliJ, and Visual Studio\n* Source Control Integrations: GitHub, GitLab, and Bitbucket\n* Chrome Browser Extension\n* Red Hat Clair for Container Scanning\n\n\n## Usage\n\nOnce the server instance is created by the operator and running,\nyou'll want to expose the service as you see fit:\n1. Create a Route to the new service for iq.applicationPort (8070).\n2. Visit the URL provided by the Route, login, and set new credentials.\n  The default credentials are `admin`/`admin123`.\n\nThe Nexus IQ Server can be further configured via the NexusIQ custom resource definition:\n\n| Parameter            | Description                                                  | Default           |\n| -------------------- | ------------------------------------------------------------ | ----------------- |\n| `iq.applicationPort` | Port of the application connector. Must match the value in the `configYaml` property | `8070`            |\n| `iq.adminPort`       | Port of the application connector. Must match the value in the `configYaml` property | `8071`            |\n| `iq.memory`          | The amount of RAM to allocate                                | `1Gi`             |\n| `iq.licenseSecret`   | The base-64 encoded license file to be installed at startup  | `\"\"`              |\n| `iq.configYaml`      | A YAML block which will be used as a configuration block for IQ Server. | See example YAML shown when creating a NexusIQ. |\n| `ingress.enabled`                           | Create an ingress for Nexus         | `true`                                  |\n| `ingress.annotations`                       | Annotations to enhance ingress configuration  | `{}`                          |\n| `ingress.tls.enabled`                       | Enable TLS                          | `true`                                 |\n| `ingress.tls.secretName`                    | Name of the secret storing TLS cert, `false` to use the Ingress' default certificate | `nexus-tls`                             |\n| `ingress.path`                              | Path for ingress rules. GCP users should set to `/*` | `/`                    |\n| `deployment.preStart.command`               | Command to run before starting the IQ Server container  | `nil`                   |\n| `deployment.postStart.command`              | Command to run after starting the IQ Server container  | `nil`                    |\n| `deployment.terminationGracePeriodSeconds` | Time to allow for clean shutdown                        | 120                      |\n| `persistence.storageClass` | The provisioner class                        | `-` (disables dynamic provisioning)            |\n| `persistence.storageSize` | The amount of drive space to allocate                        | `1Gi`             |\n| `persistence.accessMode` | Default access mode                        | `ReadWriteOnce`             |\n| `persistence.volumeConfiguration` | A YAML block to configure the persistent volume type. Defaults to `hostPath` which should not be used in production | `hostPath`             |\n\n## Configuring IQ Server\n\nYou can define the `config.yml` for IQ Server in your CRD on startup. \nIt is the `iq.configYaml` property. For more details, see the [Configuring IQ Server](https://help.sonatype.com/iqserver/configuring) help page.\n\n\n## Installing the License\n\nThe license file can be installed via the UI when IQ server is running, or it can be done as a part of the deploy. \nIf you leave the `licenseFile` field empty/commented, IQ Server will start and prompt you to manually install the license \nwhen you first enter the GUI.\n\n### Installing the License Automatically\nTo do it automatically, first encode your `.lic` file in Base 64 with no line breaks, eg:\n\n```bash\nbase64 --wrap=0 mylicense.lic > lic.base64\n```\n\nThen add this value to your CRD file as `iq.licenseSecret`, eg:\n\n```yaml\niq:\n  licenseSecret: bXkgc2FtcGxlIGxpY2Vuc2U=\n```\n\nSpecify the `licenseFile` path in `iq.configYaml` as:\n\n```yaml\niq:\n  configYaml:\n    server:\n      applicationConnectors:\n        - type: http\n          port: 8070\n      adminConnectors:\n        - type: http\n          port: 8071\n    createSampleData: true\n    sonatypeWork: /sonatype-work\n    # add this line and the `licenseSecret` above to autoconfigure licensing\n    licenseFile: /etc/nexus-iq-license/license_lic\n```",
      "csv_display_name": "Nexus IQ Operator",
      "csv_metadata_description": "Nexus Lifecycle is an open source governance platform that continuously identifies risk,\nautomatically enforces policy, and provides visibility throughout the entire SDLC.",
      "csv_name": "nxiq-operator-certified.v1.121.0-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:40:47.109000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "nxiq-operator-certified",
      "provided_apis": [
        {
          "group": "sonatype.com",
          "kind": "NexusIQ",
          "plural": "nexusiqs",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:f397acba37cb3a04cd7815491e0dff2f34d99f13cce82dc7c9522f5e26705ebc",
          "image": "registry.connect.redhat.com/sonatype/nxiq-operator-certified@sha256:f397acba37cb3a04cd7815491e0dff2f34d99f13cce82dc7c9522f5e26705ebc",
          "name": "nxiq-operator-certified-f397acba37cb3a04cd7815491e0dff2f34d99f13cce82dc7c9522f5e26705ebc-annotation"
        },
        {
          "digest": "sha256:f397acba37cb3a04cd7815491e0dff2f34d99f13cce82dc7c9522f5e26705ebc",
          "image": "registry.connect.redhat.com/sonatype/nxiq-operator-certified@sha256:f397acba37cb3a04cd7815491e0dff2f34d99f13cce82dc7c9522f5e26705ebc",
          "name": "nxiq-operator-certified"
        },
        {
          "digest": "sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
          "image": "registry.connect.redhat.com/sonatype/nexus-iq-server@sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
          "name": "iq"
        },
        {
          "digest": "sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
          "image": "registry.connect.redhat.com/sonatype/nexus-iq-server@sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
          "name": "nexus-iq-server-ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.121.0-1",
      "version_original": "1.121.0-1"
    },
    {
      "_id": "6127d9b3dece23122b7a8392",
      "alm_examples": [
        {
          "kind": "Modelbuilder",
          "metadata": {
            "name": "modelbuilder-sample"
          },
          "spec": {
            "backup_storage": {
              "storage#": "20G",
              "storage_class": "ibmc-file-gold-gid"
            },
            "env_type": "prod",
            "in_memory_storage": {
              "storage#": "20G",
              "storage_class": "ibmc-file-gold-gid"
            },
            "license": {
              "accept": false
            },
            "metadata_storage": {
              "storage#": "20G",
              "storage_class": "ibmc-block-bronze"
            },
            "photo_storage": {
              "storage#": "500G",
              "storage_class": "ibmc-file-gold-gid"
            },
            "vm_request_method": "dynamic"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/ibm/modelbuilder-bundle@sha256:cf8245accfa24c18ad9d27792b5bd2379fa476cc1f221c749e10f1b0f770f8ff",
      "bundle_path_digest": "sha256:cf8245accfa24c18ad9d27792b5bd2379fa476cc1f221c749e10f1b0f770f8ff",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-26T18:13:07.816000+00:00",
      "csv_description": "IBM Services Software Model Builder for Vision is an AI training platform that can use the capabilities of IBM Cloud and its GPUs to quickly train computer vision models for compatible mobile apps. IBM Services Software Inspection Workbench iOS and iPadOS app will be used as the exclusive UI for labeling and training. Specifically, IBM Model Builder for Vision will train computer vision models that can then be deployed for use on the IBM Services Software Inspector Portable , IBM Services Software Inspector Wearable and the IBM Maximo Visual Inspection Mobile apps.",
      "csv_display_name": "IBM Model Builder for Vision",
      "csv_metadata_description": "",
      "csv_name": "ibm-modelbuilder-for-vision.v1.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:54:54.067000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "model-builder-for-vision-certified",
      "provided_apis": [
        {
          "group": "modelbuilder.com",
          "kind": "Modelbuilder",
          "plural": "modelbuilders",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:7839f12f1bd45ff5df207a30e254996701ec4a3034b3f29f0e89abe30b89e5e8",
          "image": "registry.connect.redhat.com/ibm/modelbuilder-operator@sha256:7839f12f1bd45ff5df207a30e254996701ec4a3034b3f29f0e89abe30b89e5e8",
          "name": "modelbuilder-operator-7839f12f1bd45ff5df207a30e254996701ec4a3034b3f29f0e89abe30b89e5e8-annotation"
        },
        {
          "digest": "sha256:7839f12f1bd45ff5df207a30e254996701ec4a3034b3f29f0e89abe30b89e5e8",
          "image": "registry.connect.redhat.com/ibm/modelbuilder-operator@sha256:7839f12f1bd45ff5df207a30e254996701ec4a3034b3f29f0e89abe30b89e5e8",
          "name": "modelbuilder"
        },
        {
          "digest": "sha256:8ce51f160748de50a023e6fe8c2a50f3f0088988e1244e25de9d27594f4c0b2e",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:8ce51f160748de50a023e6fe8c2a50f3f0088988e1244e25de9d27594f4c0b2e",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:0c5350728fd5f7739eacaee7f32456d02cf1a2465f9d7fc48bacdcf73dcfa142",
          "image": "registry.redhat.io/rhel8/postgresql-12@sha256:0c5350728fd5f7739eacaee7f32456d02cf1a2465f9d7fc48bacdcf73dcfa142",
          "name": "postgres_cli"
        },
        {
          "digest": "sha256:4c5f605669edc2125a6aea31c66c89cef524b57284eac302f35bbfa4680d1467",
          "image": "registry.connect.redhat.com/crunchydata/pgo-deployer@sha256:4c5f605669edc2125a6aea31c66c89cef524b57284eac302f35bbfa4680d1467",
          "name": "pgo_deploy"
        },
        {
          "digest": "sha256:7d7ac1726d07e5c0cce79fa2b7d6ea7dfac3b94a1873ceeac84a90b2f5a0273d",
          "image": "registry.connect.redhat.com/ibm/modelbuilder-pgo-util@sha256:7d7ac1726d07e5c0cce79fa2b7d6ea7dfac3b94a1873ceeac84a90b2f5a0273d",
          "name": "pgo_client"
        },
        {
          "digest": "sha256:2454f6ee74033c8c6ad55868cee4509c78b6da523110ce28ceaa6b99ed06e582",
          "image": "registry.connect.redhat.com/ibm/modelbuilder-augmentor@sha256:2454f6ee74033c8c6ad55868cee4509c78b6da523110ce28ceaa6b99ed06e582",
          "name": "coreml_augmentor"
        },
        {
          "digest": "sha256:50389b84e5857dfb512740192c151896bc538e1afc037dc823b48fd002419d68",
          "image": "registry.connect.redhat.com/ibm/modelbuilder-image-server@sha256:50389b84e5857dfb512740192c151896bc538e1afc037dc823b48fd002419d68",
          "name": "nginx"
        },
        {
          "digest": "sha256:0f3f57dd27aa346b030bb0b6d5e8ffe3f225e4f89c1187c6b00803d320a60898",
          "image": "registry.connect.redhat.com/ibm/modelbuilder-auth-service@sha256:0f3f57dd27aa346b030bb0b6d5e8ffe3f225e4f89c1187c6b00803d320a60898",
          "name": "mb_auth_coreml"
        },
        {
          "digest": "sha256:e3bbe7f8dd13da47271e98c8e897ceae14ec34a2b18c377d560fbabfbd52d2c6",
          "image": "registry.connect.redhat.com/ibm/modelbuilder-cv-training-service@sha256:e3bbe7f8dd13da47271e98c8e897ceae14ec34a2b18c377d560fbabfbd52d2c6",
          "name": "mb_service_coreml"
        },
        {
          "digest": "sha256:e43443f9b8a5c0c4f228c23f302b37444f74b55539b6fd5f7ca4d6ebc9d5e0f1",
          "image": "registry.redhat.io/rhel8/redis-6@sha256:e43443f9b8a5c0c4f228c23f302b37444f74b55539b6fd5f7ca4d6ebc9d5e0f1",
          "name": "redis"
        },
        {
          "digest": "sha256:f28f53451464900701cc711a88b86372dab2b6d6f59cc40ad302f8e9f462c1b7",
          "image": "registry.connect.redhat.com/ibm/modelbuilder-util@sha256:f28f53451464900701cc711a88b86372dab2b6d6f59cc40ad302f8e9f462c1b7",
          "name": "util"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.0.0",
      "version_original": "1.0.0"
    },
    {
      "_id": "61295ccdeb34b73652493666",
      "alm_examples": [
        {
          "kind": "NexusIQ",
          "metadata": {
            "name": "example-nexusiq"
          },
          "spec": {
            "deployment": {
              "postStart": {},
              "preStart": {}
            },
            "fullnameOverride": "",
            "imagePullSecrets": [],
            "ingress": {
              "annotations": {},
              "enabled": false,
              "hosts": [],
              "path": "/",
              "tls": []
            },
            "iq": {
              "adminPort": 8071,
              "applicationPort": 8070,
              "configYaml": {
                "createSampleData": true,
                "server": {
                  "adminConnectors": [
                    {
                      "port": 8071,
                      "type": "http"
                    }
                  ],
                  "applicationConnectors": [
                    {
                      "port": 8070,
                      "type": "http"
                    }
                  ]
                },
                "sonatypeWork": "/sonatype-work"
              },
              "imageName": "registry.connect.redhat.com/sonatype/nexus-iq-server@sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
              "imagePullPolicy": "IfNotPresent",
              "imagePullSecret": "",
              "licenseSecret": "",
              "memory": "1Gi",
              "name": "nxiq"
            },
            "nameOverride": "",
            "persistence": {
              "accessMode": "ReadWriteOnce",
              "storageSize": "1Gi"
            },
            "service": {
              "annotations": {},
              "enabled": false,
              "labels": {},
              "ports": [
                {
                  "name": "nexus-service",
                  "port": 80,
                  "targetPort": 80
                }
              ]
            },
            "serviceAccount": {
              "create": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/sonatype/nxiq-operator-bundle@sha256:8a3a9689f37c693a1a87bbe2c1f794df9d577fb048c3e4ef74d88e3d5328b594",
      "bundle_path_digest": "sha256:8a3a9689f37c693a1a87bbe2c1f794df9d577fb048c3e4ef74d88e3d5328b594",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-08-27T21:44:45.783000+00:00",
      "csv_description": "Nexus Lifecycle is an open source governance platform that continuously identifies risk,\nautomatically enforces policy, and provides visibility throughout the entire SDLC.\nDevelopers leveraging open source require a solution that helps them make safer choices\nwhile still delivering at DevOps speed.\nNexus Lifecycle empowers developers and security teams to choose the best components\nand govern open source usage, ensuring organizations continue to innovate with less risk.\n\n## Core Capabilities\n\n* **Dependency Management**:\n  Generate a precise software bill of materials for all applications\n  to identify open source components and gather contextual direct\n  and transitive dependency information for faster action.\n* **One-Click Remediation**:\n  Upgrade to the best components and versions based on real-time intelligence.\n  Available in your preferred IDE or a GitHub pull request.\n* **Automated Open Source Policy Enforcement**:\n  Create custom security, license, and architectural policies based\n  on application type or organization and contextually enforce those policies\n  across every stage of the SDLC.\n* **Continuous Monitoring**:\n  Real-time notifications of newly discovered defects,\n  as well as vulnerabilities based on component,\n  risk level or application/container affected.\n\n## Extras\n\n* IDE Integrations: Eclipse, IntelliJ, and Visual Studio\n* Source Control Integrations: GitHub, GitLab, and Bitbucket\n* Chrome Browser Extension\n* Red Hat Clair for Container Scanning\n\n\n## Usage\n\nOnce the server instance is created by the operator and running,\nyou'll want to expose the service as you see fit:\n1. Create a Route to the new service for iq.applicationPort (8070).\n2. Visit the URL provided by the Route, login, and set new credentials.\n  The default credentials are `admin`/`admin123`.\n\nThe Nexus IQ Server can be further configured via the NexusIQ custom resource definition:\n\n| Parameter            | Description                                                  | Default           |\n| -------------------- | ------------------------------------------------------------ | ----------------- |\n| `iq.applicationPort` | Port of the application connector. Must match the value in the `configYaml` property | `8070`            |\n| `iq.adminPort`       | Port of the application connector. Must match the value in the `configYaml` property | `8071`            |\n| `iq.memory`          | The amount of RAM to allocate                                | `1Gi`             |\n| `iq.licenseSecret`   | The base-64 encoded license file to be installed at startup  | `\"\"`              |\n| `iq.configYaml`      | A YAML block which will be used as a configuration block for IQ Server. | See example YAML shown when creating a NexusIQ. |\n| `ingress.enabled`                           | Create an ingress for Nexus         | `true`                                  |\n| `ingress.annotations`                       | Annotations to enhance ingress configuration  | `{}`                          |\n| `ingress.tls.enabled`                       | Enable TLS                          | `true`                                 |\n| `ingress.tls.secretName`                    | Name of the secret storing TLS cert, `false` to use the Ingress' default certificate | `nexus-tls`                             |\n| `ingress.path`                              | Path for ingress rules. GCP users should set to `/*` | `/`                    |\n| `deployment.preStart.command`               | Command to run before starting the IQ Server container  | `nil`                   |\n| `deployment.postStart.command`              | Command to run after starting the IQ Server container  | `nil`                    |\n| `deployment.terminationGracePeriodSeconds` | Time to allow for clean shutdown                        | 120                      |\n| `persistence.storageClass` | The provisioner class                        | `-` (disables dynamic provisioning)            |\n| `persistence.storageSize` | The amount of drive space to allocate                        | `1Gi`             |\n| `persistence.accessMode` | Default access mode                        | `ReadWriteOnce`             |\n| `persistence.volumeConfiguration` | A YAML block to configure the persistent volume type. Defaults to `hostPath` which should not be used in production | `hostPath`             |\n\n## Configuring IQ Server\n\nYou can define the `config.yml` for IQ Server in your CRD on startup. \nIt is the `iq.configYaml` property. For more details, see the [Configuring IQ Server](https://help.sonatype.com/iqserver/configuring) help page.\n\n\n## Installing the License\n\nThe license file can be installed via the UI when IQ server is running, or it can be done as a part of the deploy. \nIf you leave the `licenseFile` field empty/commented, IQ Server will start and prompt you to manually install the license \nwhen you first enter the GUI.\n\n### Installing the License Automatically\nTo do it automatically, first encode your `.lic` file in Base 64 with no line breaks, eg:\n\n```bash\nbase64 --wrap=0 mylicense.lic > lic.base64\n```\n\nThen add this value to your CRD file as `iq.licenseSecret`, eg:\n\n```yaml\niq:\n  licenseSecret: bXkgc2FtcGxlIGxpY2Vuc2U=\n```\n\nSpecify the `licenseFile` path in `iq.configYaml` as:\n\n```yaml\niq:\n  configYaml:\n    server:\n      applicationConnectors:\n        - type: http\n          port: 8070\n      adminConnectors:\n        - type: http\n          port: 8071\n    createSampleData: true\n    sonatypeWork: /sonatype-work\n    # add this line and the `licenseSecret` above to autoconfigure licensing\n    licenseFile: /etc/nexus-iq-license/license_lic\n```",
      "csv_display_name": "Nexus IQ Operator",
      "csv_metadata_description": "Nexus Lifecycle is an open source governance platform that continuously identifies risk,\nautomatically enforces policy, and provides visibility throughout the entire SDLC.",
      "csv_name": "nxiq-operator-certified.v1.121.0-2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:54:19.843000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "nxiq-operator-certified",
      "provided_apis": [
        {
          "group": "sonatype.com",
          "kind": "NexusIQ",
          "plural": "nexusiqs",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:866afa870edc9e18d904c287b8c52daefdac775c01fbbaa4ed8b68e4aba64ed3",
          "image": "registry.connect.redhat.com/sonatype/nxiq-operator-certified@sha256:866afa870edc9e18d904c287b8c52daefdac775c01fbbaa4ed8b68e4aba64ed3",
          "name": "nxiq-operator-certified-866afa870edc9e18d904c287b8c52daefdac775c01fbbaa4ed8b68e4aba64ed3-annotation"
        },
        {
          "digest": "sha256:866afa870edc9e18d904c287b8c52daefdac775c01fbbaa4ed8b68e4aba64ed3",
          "image": "registry.connect.redhat.com/sonatype/nxiq-operator-certified@sha256:866afa870edc9e18d904c287b8c52daefdac775c01fbbaa4ed8b68e4aba64ed3",
          "name": "nxiq-operator-certified"
        },
        {
          "digest": "sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
          "image": "registry.connect.redhat.com/sonatype/nexus-iq-server@sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
          "name": "iq"
        },
        {
          "digest": "sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
          "image": "registry.connect.redhat.com/sonatype/nexus-iq-server@sha256:ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790",
          "name": "nexus-iq-server-ce08dbabd686ab02f246b43dccb708e2c4d81d656a091965dd3b73123091f790-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.121.0-2",
      "version_original": "1.121.0-2"
    },
    {
      "_id": "612969badece23122b7a8403",
      "alm_examples": [
        {
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:98d0ab361f4d3d22de06248949887e6a9eb8b807cd6a709a74a94c5c895fcbb2",
      "bundle_path_digest": "sha256:98d0ab361f4d3d22de06248949887e6a9eb8b807cd6a709a74a94c5c895fcbb2",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2021-08-27T22:39:54.099000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:15:39.766000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "name": "postgres-operator-18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d-annotation"
        },
        {
          "digest": "sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "name": "operator"
        },
        {
          "digest": "sha256:0fa5f4c6031e690838fe40eb618554f0c1878c14f1ab5d97999cc942177eb5ea",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:0fa5f4c6031e690838fe40eb618554f0c1878c14f1ab5d97999cc942177eb5ea",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:4a3b7bcf6461b4548eb124e7ec834f38e937dad4bd1338de27022bd9a3b13f5d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:4a3b7bcf6461b4548eb124e7ec834f38e937dad4bd1338de27022bd9a3b13f5d",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:41b4adf29237184cef74380367ac55397d6df55a98866489beb396bdb2107cdf",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:41b4adf29237184cef74380367ac55397d6df55a98866489beb396bdb2107cdf",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:155ddaaefb03e4bd3410881da27fcc5fa40dcfe7fc9195e1a563facecaae4356",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:155ddaaefb03e4bd3410881da27fcc5fa40dcfe7fc9195e1a563facecaae4356",
          "name": "postgres_12"
        },
        {
          "digest": "sha256:fd9a0e9ecd3913210bdcb49d51d7d225fd2920c8235d703f2a2d629634865e1e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:fd9a0e9ecd3913210bdcb49d51d7d225fd2920c8235d703f2a2d629634865e1e",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:d049d4cd94d7c810f5ca092e148a880b0c4b2283e55e60689dea49b6de967cf2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d049d4cd94d7c810f5ca092e148a880b0c4b2283e55e60689dea49b6de967cf2",
          "name": "postgres_12_gis_2.5"
        },
        {
          "digest": "sha256:d2a1f86dfedaf48efe6a9ade9ae15d901bf67792fa17254714cdd2b9002280a6",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d2a1f86dfedaf48efe6a9ade9ae15d901bf67792fa17254714cdd2b9002280a6",
          "name": "postgres_12_gis_3.0"
        },
        {
          "digest": "sha256:d2a2e5cf820cb037c3f167515c9bee80b4d260fc86fe14c18ca794997941246f",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d2a2e5cf820cb037c3f167515c9bee80b4d260fc86fe14c18ca794997941246f",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:3dc85b5fb89d3481e17ab148c06fb525c5fba0312b79f4707edf308c598ccc61",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:3dc85b5fb89d3481e17ab148c06fb525c5fba0312b79f4707edf308c598ccc61",
          "name": "postgres_13_gis_3.1"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "5.0.2",
      "version_original": "5.0.2"
    },
    {
      "_id": "61296a3d9f14588c41ebf58b",
      "alm_examples": [
        {
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:98d0ab361f4d3d22de06248949887e6a9eb8b807cd6a709a74a94c5c895fcbb2",
      "bundle_path_digest": "sha256:98d0ab361f4d3d22de06248949887e6a9eb8b807cd6a709a74a94c5c895fcbb2",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2021-08-27T22:42:05.091000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:00:40.306000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "name": "postgres-operator-18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d-annotation"
        },
        {
          "digest": "sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "name": "operator"
        },
        {
          "digest": "sha256:0fa5f4c6031e690838fe40eb618554f0c1878c14f1ab5d97999cc942177eb5ea",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:0fa5f4c6031e690838fe40eb618554f0c1878c14f1ab5d97999cc942177eb5ea",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:4a3b7bcf6461b4548eb124e7ec834f38e937dad4bd1338de27022bd9a3b13f5d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:4a3b7bcf6461b4548eb124e7ec834f38e937dad4bd1338de27022bd9a3b13f5d",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:41b4adf29237184cef74380367ac55397d6df55a98866489beb396bdb2107cdf",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:41b4adf29237184cef74380367ac55397d6df55a98866489beb396bdb2107cdf",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:155ddaaefb03e4bd3410881da27fcc5fa40dcfe7fc9195e1a563facecaae4356",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:155ddaaefb03e4bd3410881da27fcc5fa40dcfe7fc9195e1a563facecaae4356",
          "name": "postgres_12"
        },
        {
          "digest": "sha256:fd9a0e9ecd3913210bdcb49d51d7d225fd2920c8235d703f2a2d629634865e1e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:fd9a0e9ecd3913210bdcb49d51d7d225fd2920c8235d703f2a2d629634865e1e",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:d049d4cd94d7c810f5ca092e148a880b0c4b2283e55e60689dea49b6de967cf2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d049d4cd94d7c810f5ca092e148a880b0c4b2283e55e60689dea49b6de967cf2",
          "name": "postgres_12_gis_2.5"
        },
        {
          "digest": "sha256:d2a1f86dfedaf48efe6a9ade9ae15d901bf67792fa17254714cdd2b9002280a6",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d2a1f86dfedaf48efe6a9ade9ae15d901bf67792fa17254714cdd2b9002280a6",
          "name": "postgres_12_gis_3.0"
        },
        {
          "digest": "sha256:d2a2e5cf820cb037c3f167515c9bee80b4d260fc86fe14c18ca794997941246f",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d2a2e5cf820cb037c3f167515c9bee80b4d260fc86fe14c18ca794997941246f",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:3dc85b5fb89d3481e17ab148c06fb525c5fba0312b79f4707edf308c598ccc61",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:3dc85b5fb89d3481e17ab148c06fb525c5fba0312b79f4707edf308c598ccc61",
          "name": "postgres_13_gis_3.1"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "5.0.2",
      "version_original": "5.0.2"
    },
    {
      "_id": "61296b3e9f14588c41ebf58c",
      "alm_examples": [
        {
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:98d0ab361f4d3d22de06248949887e6a9eb8b807cd6a709a74a94c5c895fcbb2",
      "bundle_path_digest": "sha256:98d0ab361f4d3d22de06248949887e6a9eb8b807cd6a709a74a94c5c895fcbb2",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2021-08-27T22:46:22.361000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:16:45.099000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "name": "postgres-operator-18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d-annotation"
        },
        {
          "digest": "sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "name": "operator"
        },
        {
          "digest": "sha256:0fa5f4c6031e690838fe40eb618554f0c1878c14f1ab5d97999cc942177eb5ea",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:0fa5f4c6031e690838fe40eb618554f0c1878c14f1ab5d97999cc942177eb5ea",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:4a3b7bcf6461b4548eb124e7ec834f38e937dad4bd1338de27022bd9a3b13f5d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:4a3b7bcf6461b4548eb124e7ec834f38e937dad4bd1338de27022bd9a3b13f5d",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:41b4adf29237184cef74380367ac55397d6df55a98866489beb396bdb2107cdf",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:41b4adf29237184cef74380367ac55397d6df55a98866489beb396bdb2107cdf",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:155ddaaefb03e4bd3410881da27fcc5fa40dcfe7fc9195e1a563facecaae4356",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:155ddaaefb03e4bd3410881da27fcc5fa40dcfe7fc9195e1a563facecaae4356",
          "name": "postgres_12"
        },
        {
          "digest": "sha256:fd9a0e9ecd3913210bdcb49d51d7d225fd2920c8235d703f2a2d629634865e1e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:fd9a0e9ecd3913210bdcb49d51d7d225fd2920c8235d703f2a2d629634865e1e",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:d049d4cd94d7c810f5ca092e148a880b0c4b2283e55e60689dea49b6de967cf2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d049d4cd94d7c810f5ca092e148a880b0c4b2283e55e60689dea49b6de967cf2",
          "name": "postgres_12_gis_2.5"
        },
        {
          "digest": "sha256:d2a1f86dfedaf48efe6a9ade9ae15d901bf67792fa17254714cdd2b9002280a6",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d2a1f86dfedaf48efe6a9ade9ae15d901bf67792fa17254714cdd2b9002280a6",
          "name": "postgres_12_gis_3.0"
        },
        {
          "digest": "sha256:d2a2e5cf820cb037c3f167515c9bee80b4d260fc86fe14c18ca794997941246f",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d2a2e5cf820cb037c3f167515c9bee80b4d260fc86fe14c18ca794997941246f",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:3dc85b5fb89d3481e17ab148c06fb525c5fba0312b79f4707edf308c598ccc61",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:3dc85b5fb89d3481e17ab148c06fb525c5fba0312b79f4707edf308c598ccc61",
          "name": "postgres_13_gis_3.1"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "5.0.2",
      "version_original": "5.0.2"
    },
    {
      "_id": "61296bb79f14588c41ebf58d",
      "alm_examples": [
        {
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:98d0ab361f4d3d22de06248949887e6a9eb8b807cd6a709a74a94c5c895fcbb2",
      "bundle_path_digest": "sha256:98d0ab361f4d3d22de06248949887e6a9eb8b807cd6a709a74a94c5c895fcbb2",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2021-08-27T22:48:23.546000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T15:09:12.543000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "name": "postgres-operator-18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d-annotation"
        },
        {
          "digest": "sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:18277dfef37345dd8f08f2e47ceecb99e409efadf926d624a57673ac7491818d",
          "name": "operator"
        },
        {
          "digest": "sha256:0fa5f4c6031e690838fe40eb618554f0c1878c14f1ab5d97999cc942177eb5ea",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:0fa5f4c6031e690838fe40eb618554f0c1878c14f1ab5d97999cc942177eb5ea",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:4a3b7bcf6461b4548eb124e7ec834f38e937dad4bd1338de27022bd9a3b13f5d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:4a3b7bcf6461b4548eb124e7ec834f38e937dad4bd1338de27022bd9a3b13f5d",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:41b4adf29237184cef74380367ac55397d6df55a98866489beb396bdb2107cdf",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:41b4adf29237184cef74380367ac55397d6df55a98866489beb396bdb2107cdf",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:155ddaaefb03e4bd3410881da27fcc5fa40dcfe7fc9195e1a563facecaae4356",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:155ddaaefb03e4bd3410881da27fcc5fa40dcfe7fc9195e1a563facecaae4356",
          "name": "postgres_12"
        },
        {
          "digest": "sha256:fd9a0e9ecd3913210bdcb49d51d7d225fd2920c8235d703f2a2d629634865e1e",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-ha@sha256:fd9a0e9ecd3913210bdcb49d51d7d225fd2920c8235d703f2a2d629634865e1e",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:d049d4cd94d7c810f5ca092e148a880b0c4b2283e55e60689dea49b6de967cf2",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d049d4cd94d7c810f5ca092e148a880b0c4b2283e55e60689dea49b6de967cf2",
          "name": "postgres_12_gis_2.5"
        },
        {
          "digest": "sha256:d2a1f86dfedaf48efe6a9ade9ae15d901bf67792fa17254714cdd2b9002280a6",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d2a1f86dfedaf48efe6a9ade9ae15d901bf67792fa17254714cdd2b9002280a6",
          "name": "postgres_12_gis_3.0"
        },
        {
          "digest": "sha256:d2a2e5cf820cb037c3f167515c9bee80b4d260fc86fe14c18ca794997941246f",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:d2a2e5cf820cb037c3f167515c9bee80b4d260fc86fe14c18ca794997941246f",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:3dc85b5fb89d3481e17ab148c06fb525c5fba0312b79f4707edf308c598ccc61",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis-ha@sha256:3dc85b5fb89d3481e17ab148c06fb525c5fba0312b79f4707edf308c598ccc61",
          "name": "postgres_13_gis_3.1"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "5.0.2",
      "version_original": "5.0.2"
    },
    {
      "_id": "613015f07b37b351c172d588",
      "alm_examples": [
        {
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:62250d515fa374b29f57381fea8f4f9a56ff6e00056b38476e629c91606c7369",
      "bundle_path_digest": "sha256:62250d515fa374b29f57381fea8f4f9a56ff6e00056b38476e629c91606c7369",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-02T00:08:16.996000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.3.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:17:41.432000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "8.3.1",
      "version_original": "8.3.1"
    },
    {
      "_id": "613017c5dc2dfaaae870540f",
      "alm_examples": [
        {
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:62250d515fa374b29f57381fea8f4f9a56ff6e00056b38476e629c91606c7369",
      "bundle_path_digest": "sha256:62250d515fa374b29f57381fea8f4f9a56ff6e00056b38476e629c91606c7369",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-02T00:16:05.959000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.3.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:30:03.731000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "8.3.1",
      "version_original": "8.3.1"
    },
    {
      "_id": "61301f93c61b370d4fd4cb3c",
      "alm_examples": [
        {
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:62250d515fa374b29f57381fea8f4f9a56ff6e00056b38476e629c91606c7369",
      "bundle_path_digest": "sha256:62250d515fa374b29f57381fea8f4f9a56ff6e00056b38476e629c91606c7369",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-02T00:49:23.968000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.3.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:21:03.058000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "8.3.1",
      "version_original": "8.3.1"
    },
    {
      "_id": "61301f95539c8cedbde1d182",
      "alm_examples": [
        {
          "kind": "Kubeturbo",
          "metadata": {
            "name": "kubeturbo-release"
          },
          "spec": {
            "serverMeta": {
              "turboServer": "https://Turbo_server_URL"
            },
            "targetConfig": {
              "targetName": "Cluster_Name"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/kubeturbo-operator-bundle@sha256:62250d515fa374b29f57381fea8f4f9a56ff6e00056b38476e629c91606c7369",
      "bundle_path_digest": "sha256:62250d515fa374b29f57381fea8f4f9a56ff6e00056b38476e629c91606c7369",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-02T00:49:25.371000+00:00",
      "csv_description": "### Application Resource Management for Kubernetes\nTurbonomic AI-powered Application Resource Management simultaneously optimizes performance, compliance, and cost in real time.\nSoftware manages the complete application stack, automatically. Applications are continually resourced to perform while satisfying business constraints.\n\nTurbonomic makes workloads smart \u2014 enabling them to self-manage and determines the specific actions that will drive continuous health:\n\n* Continuous placement for Pods (rescheduling)\n* Continuous scaling for applications and  the underlying cluster.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a container \u2014 KubeTurbo \u2014 that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.\nKubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.\nTurbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.",
      "csv_display_name": "Kubeturbo Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "kubeturbo-operator.v8.3.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:58:35.586000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "kubeturbo-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Kubeturbo",
          "plural": "kubeturbos",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "8.3.1",
      "version_original": "8.3.1"
    },
    {
      "_id": "6133af63c61b370d4fd4cca6",
      "alm_examples": [
        {
          "kind": "HXCSIDriver",
          "metadata": {
            "name": "hxcsidriver-sample"
          },
          "spec": {
            "affinity": {},
            "autoscaling": {},
            "fullnameOverride": "",
            "hx": {
              "clientId": "",
              "clusteruuid": "",
              "dockerRegistryName": "",
              "iscsiUrl": "",
              "token": "exampletoken",
              "url": ""
            },
            "image": {
              "attacher": "hxcsi-csi-attacher:3.2.0-cisco1",
              "hxcsi": "hxcsi-ubi8:hxcsi-1.2.1-619",
              "provisioner": "hxcsi-csi-provisioner:2.1.2-cisco1",
              "pullPolicy": "Always",
              "registrar": "hxcsi-csi-node-driver-registrar:2.1.0-cisco1",
              "resizer": "hxcsi-csi-resizer:1.1.0-cisco1",
              "tag": ""
            },
            "imagePullSecrets": [],
            "ingress": {},
            "nameOverride": "",
            "nodeSelector": {},
            "podAnnotations": {},
            "podSecurityContext": {},
            "replicaCount": 1,
            "resources": {},
            "securityContext": {},
            "service": {},
            "serviceAccount": {
              "annotations": {},
              "create": true,
              "name": ""
            },
            "storageClass": {
              "isDefault": "false",
              "name": "hyperflex"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hxcsiadmin/hxcsi-helm-bundle@sha256:fec9c6052c842ebe55a3a758d846ad533afb0baaee88aba60f3110358cf4dde6",
      "bundle_path_digest": "sha256:fec9c6052c842ebe55a3a758d846ad533afb0baaee88aba60f3110358cf4dde6",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-04T17:39:47.735000+00:00",
      "csv_description": "This is a HELM based Operator to deploy HXCSI driver to a RedHat OpenShift cluster",
      "csv_display_name": "Cisco Hyperflex CSI Operator",
      "csv_metadata_description": "Cisco HXCSI Plugin for RedHat OpenShift Platform",
      "csv_name": "cisco-hxcsi-operator.v1.2.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T15:13:51.434000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "cisco-hxcsi-operator",
      "provided_apis": [
        {
          "group": "hxcsi.cisco.com",
          "kind": "HXCSIDriver",
          "plural": "hxcsidrivers",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:10492e703c017eaa4c95e847477a3a7b7e3a512c5a0679a7d67dd01813223d5a",
          "image": "quay.io/hxcsiadmin/hxcsi-helm-bundle@sha256:10492e703c017eaa4c95e847477a3a7b7e3a512c5a0679a7d67dd01813223d5a",
          "name": "hxcsi-helm-bundle-10492e703c017eaa4c95e847477a3a7b7e3a512c5a0679a7d67dd01813223d5a-annotation"
        },
        {
          "digest": "sha256:518f5e036a6c67257c2926d9cad073e46ba0663c1386b97aadbf51d3a27ab559",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:518f5e036a6c67257c2926d9cad073e46ba0663c1386b97aadbf51d3a27ab559",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:4b75276a513a5f1f68ffc84e0b99d086f428846e2e5ef93860c646e9760c241d",
          "image": "quay.io/hxcsiadmin/hxcsi-helm-operator@sha256:4b75276a513a5f1f68ffc84e0b99d086f428846e2e5ef93860c646e9760c241d",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.2.1",
      "version_original": "1.2.1"
    },
    {
      "_id": "6135659f539c8cedbde1d2a4",
      "alm_examples": [
        {
          "kind": "HSPC",
          "metadata": {
            "name": "hspc",
            "namespace": "kube-system"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hitachi/hspc-operator-bundle@sha256:939b68f8608acee4650257aab96f9eb46f3e6aa80ccc6ea1aeb5c37ae7cce66b",
      "bundle_path_digest": "sha256:939b68f8608acee4650257aab96f9eb46f3e6aa80ccc6ea1aeb5c37ae7cce66b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-06T00:49:35.163000+00:00",
      "csv_description": "## About\nHitachi Storage Plug-in for Containers (HSPC) is a plugin that integrates Hitachi VSP storage into Kubernetes based clusters.\nHSPC provides dynamic persistent volume provisioning capabilities from Hitachi's block storage arrays.\n\nFor full documentation, go to our [Knowledge Base](https://knowledge.hitachivantara.com/Documents/Adapters_and_Drivers/Storage_Adapters_and_Drivers/Containers) and refer to the reference guide for HSPC v3.7.0.\n\n## Requirements\n\n### Supported Driver Version\n\n* HSPC v3.7.0\n\n### Supported Platforms\n\n* OpenShift v4.5, v4.6, v4.7\n\n### Supported Operating Systems\n\n* RHEL 7.x",
      "csv_display_name": "Hitachi Storage Plug-in for Containers",
      "csv_metadata_description": "An operator for managing Hitachi Storage Plug-in for Containers CSI driver",
      "csv_name": "hspc-operator.v1.7.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:12:49.387000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "hspc-operator",
      "provided_apis": [
        {
          "group": "csi.hitachi.com",
          "kind": "HSPC",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72",
          "image": "registry.connect.redhat.com/hitachi/hspc-operator@sha256:5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72",
          "name": "hspc-operator-5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72-annotation"
        },
        {
          "digest": "sha256:5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72",
          "image": "registry.connect.redhat.com/hitachi/hspc-operator@sha256:5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.7.0",
      "version_original": "1.7.0"
    },
    {
      "_id": "61383f2e89213684728b27b2",
      "alm_examples": [
        {
          "kind": "FEPCluster",
          "metadata": {
            "name": "new-fep"
          },
          "spec": {
            "fep": {
              "customAnnotations": {
                "allDeployments": {}
              },
              "forceSsl": true,
              "image": {
                "pullPolicy": "IfNotPresent"
              },
              "instances": "1",
              "mcSpec": {
                "limits": {
                  "cpu": "500m",
                  "memory": "700Mi"
                },
                "requests": {
                  "cpu": "200m",
                  "memory": "512Mi"
                }
              },
              "podAntiAffinity": false,
              "podDisruptionBudget": false,
              "servicePort": 27500,
              "syncMode": "off",
              "sysExtraLogging": false
            },
            "fepChildCrVal": {
              "backup": {
                "image": {
                  "pullPolicy": "IfNotPresent"
                },
                "mcSpec": {
                  "limits": {
                    "cpu": "0.2",
                    "memory": "300Mi"
                  },
                  "requests": {
                    "cpu": "0.1",
                    "memory": "200Mi"
                  }
                },
                "pgbackrestParams": " ",
                "postScript": " ",
                "preScript": " ",
                "schedule": {
                  "num": 2
                },
                "schedule1": {
                  "schedule": "15 0 * * 0",
                  "type": "full"
                },
                "schedule2": {
                  "schedule": "15 0 * * 1-6",
                  "type": "incr"
                },
                "schedule3": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule4": {
                  "schedule": " ",
                  "type": " "
                },
                "schedule5": {
                  "schedule": " ",
                  "type": " "
                }
              },
              "customPgAudit": "# define pg audit custom params here to override defaults.\n# if log volume is not defined, log_directory should be \n# changed to '/database/userdata/data/log'\n[output]\nlogger = 'auditlog'\nlog_directory = '/database/log/audit'\n[rule]\n",
              "customPgHba": "# define pg_hba custom rules here to be merged with default rules.\n# TYPE     DATABASE        USER        ADDRESS        METHOD\n",
              "customPgParams": "# define custom postgresql.conf parameters below to override defaults.\n# Current values are as per default FEP deployment\nshared_preload_libraries='pgx_datamasking,vci,pgaudit,pg_prewarm'\nsession_preload_libraries='vci,pg_prewarm'\nmax_prepared_transactions = 100\nmax_worker_processes = 30\nmax_connections = 100\nwork_mem = 1MB\nmaintenance_work_mem = 12MB\nshared_buffers = 128MB\neffective_cache_size = 384MB\ncheckpoint_completion_target = 0.8\n\n# tcp parameters\ntcp_keepalives_idle = 30\ntcp_keepalives_interval = 10\ntcp_keepalives_count = 3\n\n# logging parameters in default fep installation\n# if log volume is not defined, log_directory should be \n# changed to '/database/userdata/data/log'\nlog_directory = '/database/log'\nlog_filename = 'logfile-%a.log'\nlog_file_mode = 0600\nlog_truncate_on_rotation = on\nlog_rotation_age = 1d\nlog_rotation_size = 0\nlog_checkpoints = on\nlog_line_prefix = '%e %t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h'\nlog_lock_waits = on\nlog_autovacuum_min_duration = 60s\nlogging_collector = on\npgaudit.config_file='/opt/app-root/src/pgaudit-cfg/pgaudit.conf'\nlog_replication_commands = on\nlog_min_messages = WARNING\nlog_destination = stderr\n\n# vci parameters in default fep installation\nvci.enable = on\nvci.maintenance_work_mem = 256MB\nvci.max_local_ros = 64MB\nvci.force_max_parallelism = off\n\n# wal_archive parameters in default fep installation\narchive_mode = on\narchive_command = 'pgbackrest --stanza=backupstanza --config=/database/userdata/pgbackrest.conf archive-push %p'\nwal_level = replica\nmax_wal_senders = 12\nwal_keep_segments = 64\n\n",
              "storage": {
                "archivewalVol": {
                  "size": "1Gi"
                },
                "backupVol": {
                  "size": "2Gi"
                },
                "dataVol": {
                  "size": "2Gi"
                },
                "logVol": {
                  "size": "1Gi"
                },
                "tablespaceVol": {
                  "size": "512Mi"
                },
                "walVol": {
                  "size": "1200Mi"
                }
              },
              "sysUsers": {
                "pgAdminPassword": "admin-password",
                "pgdb": "mydb",
                "pgpassword": "mydbpassword",
                "pgreplpassword": "repluserpwd",
                "pgrepluser": "repluser",
                "pguser": "mydbuser",
                "tdepassphrase": "tde-passphrase"
              },
              "systemCertificates": {
                "cacrt": "-----BEGIN CERTIFICATE-----\nMIIDTzCCAjegAwIBAgIUYssQ8I74US5g+1+Z7CHuaDgkZnEwDQYJKoZIhvcNAQEL\nBQAwNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9y\nIEt1YmVybmV0ZXMwHhcNMjEwMjA2MDM1MjI4WhcNMzEwMjA0MDM1MjI4WjA3MRAw\nDgYDVQQKDAdGdWppdHN1MSMwIQYDVQQDDBpGRVAgUm9vdCBDQSBmb3IgS3ViZXJu\nZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMs97gUF0xkUzCgL\n7MiiDju9ySr/ziwjvcYU7jA9ML+SLmftMs3HtcYbAmSntqI+MDBSR/FAJTOoytuT\npV+mCFcGj2YAjDpliHPeNcUpbryy4YMChF3+MovkIwGCksxo5rhiWhGmoBYpA48P\n4Xe8SPlzqMzhFvNeKzyiUhvjutS2Y1Ss38lsTaurFPx64vQ2PaC54XzdwMptXtpb\ntYmWSzCpJWwxZ6lF3vitdA2w0tnBWNyctAd0+RIM/fvArxiIqseAux9t0uogm5to\nlRIhvekuxOpXBPEqtIYQ4j9XUW2JH8vUDnzPkPvjrq+A3Ug8OyyfGVrW7+VYXozu\nc4aP7P0CAwEAAaNTMFEwHQYDVR0OBBYEFBzCutQ7S74WEhS5V2sNEJBGyLpmMB8G\nA1UdIwQYMBaAFBzCutQ7S74WEhS5V2sNEJBGyLpmMA8GA1UdEwEB/wQFMAMBAf8w\nDQYJKoZIhvcNAQELBQADggEBAMDwD85RAaWEBptFgLzKw+9xEUy1vcZaonAuA1qc\nT342XTueyAugxkC11HwdCGgGS34VyctfMGqj4AW6pA2ez4tLrbOps4DmV4sw8uBL\n8pgRDgfly3ob9FEg2wa0hmrwX9jH5Bt4vySUE2785uPAqaspT2UNtTBxS85BUi1T\nsKId2Rtil6an281Z81wyWVI6Jm2D4MG0mbsiGcTPlCtdg/UljvDYymXlAvd4vNhl\nk9hDa13TgDqJKgKdTIcmZoNQdpEVgFcO0h9AEUy5AuLqxHq60dLfZ6ESGPlMI7Lm\ni4PzYbCnBmOe+7TnHcPSyrnehs66Ik+oifRd82eYS7vKjFw=\n-----END CERTIFICATE-----",
                "crt": "-----BEGIN CERTIFICATE-----\nMIIDUTCCAjmgAwIBAgIRAMocW3qMoHrD6qRvMPppMkMwDQYJKoZIhvcNAQELBQAw\nNzEQMA4GA1UECgwHRnVqaXRzdTEjMCEGA1UEAwwaRkVQIFJvb3QgQ0EgZm9yIEt1\nYmVybmV0ZXMwHhcNMjEwMjA2MDQzMjM2WhcNMjYwMjA1MDQzMjM2WjA/MRAwDgYD\nVQQKEwdGdWppdHN1MSswKQYDVQQDEyJGVUpJVFNVIEVudGVycHJpc2UgUG9zdGdy\nZXMgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4AI33yvH\nZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I2e4SceTKi6O3C/I1XuvWlpng\n5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4ANB5JWWqDOjrRT3o7nRPGXfila\nbP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYpmjdbfxabTz07ig0+6/cwKoRR\nxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTFYvmAH7gcdssSFBt8NPlUATHE\nsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6Wdgmu5H2pDml8CDNLDv98Aj7i\n+I5SRKKcVPlnuQIDAQABo1AwTjAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBQcwrrUO0u+FhIUuVdrDRCQRsi6\nZjANBgkqhkiG9w0BAQsFAAOCAQEAm5dxBoI9pScOCvRAchg4CprdRDSJb9K6yB3O\nnCAxnM47iHeXnY3WlnI388kHu8DU7O4ba1tJbGs3KY9KzioPk43pU12jWkO1onoF\n+mTDjx/Ef1cYWA9r5q/LtgTa6Q2sxV4O2x67QW82aAnaxO34dV5zWCPIvAoovZBV\nHRT+BgCg3r2vD1RGKK2nl1aYJtWhO1SZubam+VttdZ/vbM9oOJctxmImsEtBXjkY\nKteePdQtLL5o03JhyXWyRshCq+HMmKf2KgyY8gvydGcP4eLQdBWcW40LcnVq6UjT\n0kJycJEKngMVademq1ZWHGaiYB7hyT6GhgIcHUJ2cKrPgbEh1Q==\n-----END CERTIFICATE-----",
                "key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA4AI33yvHZws+jta6qpV6wzJqF8odIfTIpCfbrVcUUtLFKJ1I\n2e4SceTKi6O3C/I1XuvWlpng5IO65+fQQLO06z1/AuQT78YUn/Wlm9x1aHVsv4AN\nB5JWWqDOjrRT3o7nRPGXfilabP0rGE2mJJcVR9nExJ3IeaktgT3sb8YlXvtchyYp\nmjdbfxabTz07ig0+6/cwKoRRxOK8Uf7f5euE0cI/490J6r5Rs4lgD8sIQNCUFlTF\nYvmAH7gcdssSFBt8NPlUATHEsoFmlW0DKCJWNhTLOht+s6L/1zwTHLjPG2pdkG6W\ndgmu5H2pDml8CDNLDv98Aj7i+I5SRKKcVPlnuQIDAQABAoIBAFPQYKlOzw/+BA0b\nyMIUpdctIMb/54CR/xR0mVw1DbSjigNVPjHUQvB8Y1B2FAITQObgJO06bAv0QdWN\nRb0/v/yYiNJDFjaLjaIAHlO/2+oWrXbFaZqgpVDJhB+e1xaZr2x7XGxm+p925k30\nl6pvIRY+I8JRKvZiV1VZHwL/R3JOtPr++xMZtLVjVOI+f+ySqJ+TZHuAjm49EKxj\ncEmmJ28b7QcziXsvKy00f+zbqLIBKXQdZAFU5eEr1BsDRXdRW+Kf0XIvftuy4BJZ\nvoKT+VGhEvF/qysswL4+6IAO6tpuYnnM0Y2d3sOGoWPkTcQK0MekYKzL/WmtCjNs\n9hodJtECgYEA5EWyhEOf4uOKe5TDp697UCUvXLoOR58FDe/S8XNvScn29jjOkqIg\nOMoqo9xAkJTNTzqn5UUdt1x/pgM2NxlPLFijrc0zQlX3SoOO2ryDd9WNi7YKtN16\nKJqa536WeZu2OEbuAZ+S3GALVy1RPeTNPnUOmKnF06DjDUGzLNCZy10CgYEA+zfw\n952DWuz1U0Z4wvAEqqcgUKXPKrkTXV/iUnjkDkrLYVr0ZofDNTXrdHl+UedFmaOC\ncieZn6DNhcdz5tKtyysGMH3g/qs9PfoGUngvcXsy0Egk04l3x1jc8TTCLqXZXYaQ\nHMsx51n+R58oncPtzYSUOr9qQ6PbC2CstTbFJA0CgYEAjGEsUliAB/jknfEzjXjG\nPdhQUxb8VyE864Az2lah9t/kJzFyIAziAeqZ5GE7t247AGFTBRTHHI8e1Qoemi3P\nWbc9GVIbFs1lIYbcIDpUIyrKPEP8O5QEXtoNLxXTFgAjRGKiVY87spjCAJ+W2ZhO\ne/1it5GYXfgQCYQA2yuBmOUCgYANRkR2YR1axaCk+NlSu6oTdmdPu6M5x7PNQE7O\nOtMaKjua9lppvIzFGAdMDUtueoEEAE7ZR1xnwfB6PDLUpJdIYAqgr1YfPt8qkjaZ\nTv56yZ7CwL0pbF8m6nwqRrZoDp1wwraEvvvxFKFKGY/k3kCHlpTakdjEoDjn3gDi\nRnWeVQKBgCEneMSzucei5LRppRtRaJw/Btll8qlPMlX3W7dxQ3cLwpmLOn0m51Fp\nPIZ44zYK8R6fu4+/sSrlfaIg86Ugeufp6YNxyNROKxUGza5vDIu5OftwWtBeg+UK\nZ8lLWNdX6pp7WMujmF3H1DrkBbauYMUKZ4UxUYtelgHERMePIxwb\n-----END RSA PRIVATE KEY-----"
              }
            }
          }
        },
        {
          "kind": "FEPAction",
          "metadata": {
            "name": "new-fep-action"
          },
          "spec": {
            "fepAction": {
              "args": [
                "new-fep-sts-0"
              ],
              "type": "reload"
            },
            "sysExtraLogging": false,
            "targetClusterName": "new-fep"
          }
        },
        {
          "kind": "FEPPgpool2",
          "metadata": {
            "name": "new-fep-pgpool2"
          },
          "spec": {
            "customparams": "listen_addresses = '*'\npcp_listen_addresses = '*'\nnum_init_children = 32\nreserved_connections = 0\nenable_pool_hba = off\nallow_clear_text_frontend_auth = off\nauthentication_timeout = 80\nbackend_weight0 = 1\nbackend_weight1 = 1\nbackend_flag0 = 'DISALLOW_TO_FAILOVER'\nbackend_flag1 = 'DISALLOW_TO_FAILOVER'\nconnection_cache = on\nmax_pool = 4\nlisten_backlog_multiplier = 2\nserialize_accept = off\nchild_life_time = 300\nclient_idle_limit = 0\nchild_max_connections = 0\nconnection_life_time = 0\nreset_query_list = 'ABORT; DISCARD ALL'\nclient_min_messages = info\nlog_min_messages = debug1\nlog_statement = on\nlog_per_node_statement = on\nlog_client_messages = on\nlog_hostname = on\nlog_connections = on\nlog_line_prefix = '%t: pid %p: '\nload_balance_mode = on\nignore_leading_white_space = on\nwhite_function_list = ''\nblack_function_list = 'currval,lastval,nextval,setval'\nblack_query_pattern_list = ''\ndatabase_redirect_preference_list = ''\napp_name_redirect_preference_list = ''\nallow_sql_comments = off\ndisable_load_balance_on_write = 'transaction'\nstatement_level_load_balance = on\nconnect_timeout = 10000\nsr_check_period = 0\nsr_check_user = ' postgres '\ndelay_threshold = 0\nlog_standby_delay = 'none'\nssl = off\nssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'\nssl_prefer_server_ciphers = off\nssl_ecdh_curve = 'prime256v1'\nssl_dh_params_file = ''\nrelcache_expire = 0\nrelcache_size = 256\ncheck_temp_table = catalog\ncheck_unlogged_table = on\nenable_shared_relcache = on\nrelcache_query_target = master\n\n",
            "fepclustername": "new-fep",
            "imagePullPolicy": "IfNotPresent"
          }
        },
        {
          "kind": "FEPRestore",
          "metadata": {
            "name": "new-fep-restore"
          },
          "spec": {
            "fromFEPcluster": "new-fep",
            "imagePullPolicy": "IfNotPresent",
            "mcSpec": {
              "limits": {
                "cpu": "200m",
                "memory": "300Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "200Mi"
              }
            },
            "restoretype": "latest",
            "toFEPcluster": "new-fep-2"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [
        "amd64",
        "s390x"
      ],
      "bundle_path": "registry.connect.redhat.com/fujitsu-postgres/fujitsu-enterprise-postgres-bundle@sha256:8b801772768fe15c3098890ef53fc8bbd434e80cfdfbd2b2dae8d516fa61f972",
      "bundle_path_digest": "sha256:8b801772768fe15c3098890ef53fc8bbd434e80cfdfbd2b2dae8d516fa61f972",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-08T04:42:22.935000+00:00",
      "csv_description": "FUJITSU Enterprise Postgres 12 delivers an enterprise-grade PostgreSQL on OpenShift Container Platform.\n\nThis solution provides the flexibility of a hybrid cloud solution while delivering an enhanced distribution\nof PostgreSQL to support enterprise-level workloads and provide improved deployment and management,\navailability, performance, data governance and security.\n\nAvailable as a multi-architecture container built for both amd64 and s390x.\n\nUse of the product is subject to Fujitsu evaluation license located at:\nhttps://www.fast.fujitsu.com/fujitsu-enterprise-postgres-trial-version-software-evaluation-license-agreement\nand the license period is 90 days after the download\n",
      "csv_display_name": "FUJITSU Enterprise Postgres 12 Operator",
      "csv_metadata_description": "OpenShift Operator for Fujitsu Enterprise Postgres 12",
      "csv_name": "fujitsu-enterprise-operator.v2.2.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:56:21+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "fep-ansible-operator",
      "provided_apis": [
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPBackup",
          "plural": "fepbackups",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPConfig",
          "plural": "fepconfigs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2Cert",
          "plural": "feppgpool2certs",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPPgpool2",
          "plural": "feppgpool2s",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPRestore",
          "plural": "feprestores",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPUser",
          "plural": "fepusers",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPAction",
          "plural": "fepactions",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCert",
          "plural": "fepcerts",
          "version": "v1"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPCluster",
          "plural": "fepclusters",
          "version": "v2"
        },
        {
          "group": "fep.fujitsu.io",
          "kind": "FEPVolume",
          "plural": "fepvolumes",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:618b61707b38fe8a9636766261a5ea70b45f4c2549dde769c5b20a8ebca8f59d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:618b61707b38fe8a9636766261a5ea70b45f4c2549dde769c5b20a8ebca8f59d",
          "name": "fujitsu-enterprise-postgres-12-operator-618b61707b38fe8a9636766261a5ea70b45f4c2549dde769c5b20a8ebca8f59d-annotation"
        },
        {
          "digest": "sha256:618b61707b38fe8a9636766261a5ea70b45f4c2549dde769c5b20a8ebca8f59d",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-operator@sha256:618b61707b38fe8a9636766261a5ea70b45f4c2549dde769c5b20a8ebca8f59d",
          "name": "fep-ansible-operator"
        },
        {
          "digest": "sha256:a9edcb9a3464c19bd6e7d772a084f05452b893b9ae3dc1dbf30da35dcb681ee3",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-server@sha256:a9edcb9a3464c19bd6e7d772a084f05452b893b9ae3dc1dbf30da35dcb681ee3",
          "name": "fep"
        },
        {
          "digest": "sha256:953833154e75d79b15f96790368ad38fb20f05a48bf1b75be1968632fd5bdb40",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-backup@sha256:953833154e75d79b15f96790368ad38fb20f05a48bf1b75be1968632fd5bdb40",
          "name": "backup"
        },
        {
          "digest": "sha256:0bdff064fcebea38f4d796ddbf0de211e4ba6bb297e42dc514d5ef44980ee313",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-restore@sha256:0bdff064fcebea38f4d796ddbf0de211e4ba6bb297e42dc514d5ef44980ee313",
          "name": "restore"
        },
        {
          "digest": "sha256:2ecd8feb1a9fec0ed015908a5c53a8d4df405703068d0de21effdb3f3d33a8df",
          "image": "quay.io/fujitsu/fujitsu-enterprise-postgres-12-pgpool2@sha256:2ecd8feb1a9fec0ed015908a5c53a8d4df405703068d0de21effdb3f3d33a8df",
          "name": "pgpool2"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "2.2.2",
      "version_original": "2.2.2"
    },
    {
      "_id": "613841292c743eb28fafda89",
      "alm_examples": [
        {
          "kind": "HSPC",
          "metadata": {
            "name": "hspc",
            "namespace": "kube-system"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/hitachi/hspc-operator-bundle@sha256:939b68f8608acee4650257aab96f9eb46f3e6aa80ccc6ea1aeb5c37ae7cce66b",
      "bundle_path_digest": "sha256:939b68f8608acee4650257aab96f9eb46f3e6aa80ccc6ea1aeb5c37ae7cce66b",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-08T04:50:49.373000+00:00",
      "csv_description": "## About\nHitachi Storage Plug-in for Containers (HSPC) is a plugin that integrates Hitachi VSP storage into Kubernetes based clusters.\nHSPC provides dynamic persistent volume provisioning capabilities from Hitachi's block storage arrays.\n\nFor full documentation, go to our [Knowledge Base](https://knowledge.hitachivantara.com/Documents/Adapters_and_Drivers/Storage_Adapters_and_Drivers/Containers) and refer to the reference guide for HSPC v3.7.0.\n\n## Requirements\n\n### Supported Driver Version\n\n* HSPC v3.7.0\n\n### Supported Platforms\n\n* OpenShift v4.5, v4.6, v4.7\n\n### Supported Operating Systems\n\n* RHEL 7.x",
      "csv_display_name": "Hitachi Storage Plug-in for Containers",
      "csv_metadata_description": "An operator for managing Hitachi Storage Plug-in for Containers CSI driver",
      "csv_name": "hspc-operator.v1.7.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:50:37.637000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "hspc-operator",
      "provided_apis": [
        {
          "group": "csi.hitachi.com",
          "kind": "HSPC",
          "plural": "hspcs",
          "version": "v1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72",
          "image": "registry.connect.redhat.com/hitachi/hspc-operator@sha256:5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72",
          "name": "hspc-operator-5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72-annotation"
        },
        {
          "digest": "sha256:5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72",
          "image": "registry.connect.redhat.com/hitachi/hspc-operator@sha256:5a8fce133552bd933fd51663cc37a139b461ea6b9b34fb84224fba2747579e72",
          "name": "manager"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.7.0",
      "version_original": "1.7.0"
    },
    {
      "_id": "6138a23e2c743eb28fafdb31",
      "alm_examples": [
        {
          "kind": "DynaKube",
          "metadata": {
            "name": "dynakube",
            "namespace": "dynatrace"
          },
          "spec": {
            "activeGate": {
              "image": ""
            },
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "classicFullStack": {
              "enabled": true,
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "Exists"
                }
              ]
            },
            "kubernetesMonitoring": {
              "enabled": true
            },
            "oneAgent": {
              "image": ""
            },
            "routing": {
              "enabled": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-operator-bundle@sha256:1a3898915535373eed8f860b8eb906fa2ecaebbb95c4e2d74488e12ca1540f9b",
      "bundle_path_digest": "sha256:1a3898915535373eed8f860b8eb906fa2ecaebbb95c4e2d74488e12ca1540f9b",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-08T11:45:02.456000+00:00",
      "csv_description": "The Dynatrace Operator supports rollout and lifecycle of various Dynatrace components in Kubernetes and OpenShift.\n\nAs of launch, the Dynatrace Operator can be used to deploy a containerized ActiveGate for Kubernetes API monitoring. New capabilities will be added to the Dynatrace Operator over time including metric routing, and API monitoring for AWS, Azure, GCP, and vSphere.\n\nWith v0.2.0 we added the classicFullStack functionality which allows rolling out the OneAgent to your Kubernetes cluster.\nFurthermore, the Dynatrace Operator is now capable of rolling out a containerized ActiveGate for routing the OneAgent traffic.\n\n### Installation\nOnce you've installed the Dynatrace Operator, you can create a DynaKube custom resource.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic dynakube --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nAfter creation of the secret add the DynaKube object in the project where the Dynatrace Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Image Override** - use a copy of the ActiveGate container image from a registry other than Docker's or Red Hat's\n\n#### Kubernetes Monitoring\n* **Kubernetes Monitoring** - when enabled the Dynatrace Operator will create a containerized ActiveGate with the capability to monitor your OpenShift cluster\n* **Replicas** - defines how many replicas of the containerized ActiveGate should be created\n* **NodeSelectors** - select a subset of your cluster's nodes to run the Dynatrace ActiveGate on, based on labels\n* **Environment variables** - define environment variables for the ActiveGate container\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-dynatrace-operator).\n",
      "csv_display_name": "Dynatrace Operator",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-operator.v0.2.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:21:03.577000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "dynatrace-operator",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3",
          "name": "dynatrace-operator-afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3-annotation"
        },
        {
          "digest": "sha256:afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3",
          "name": "dynatrace-operator"
        },
        {
          "digest": "sha256:3975446f23bfd4cab0930d3cba62a795893f9085084e6887121ceaa94391c849",
          "image": "registry.connect.redhat.com/dynatrace/oneagent@sha256:3975446f23bfd4cab0930d3cba62a795893f9085084e6887121ceaa94391c849",
          "name": "dynatrace_oneagent"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.2.2",
      "version_original": "0.2.2"
    },
    {
      "_id": "613dc3dcc9bc35f21dc4a26e",
      "alm_examples": [
        {
          "kind": "Adam",
          "metadata": {
            "name": "example-adam",
            "namespace": "appdynamics"
          },
          "spec": {
            "controllerUrl": "https://saas.appdynamics.com"
          }
        },
        {
          "kind": "Clusteragent",
          "metadata": {
            "name": "k8s-cluster-agent",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "appName": "Cluster1",
            "controllerUrl": "https://saas.appdynamics.com",
            "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
            "serviceAccountName": "appdynamics-cluster-agent",
            "stdoutLogging": "true"
          }
        },
        {
          "kind": "InfraViz",
          "metadata": {
            "name": "appd-infraviz",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "controllerUrl": "https://saas.appdynamics.com",
            "enableDockerViz": "false",
            "enableMasters": true,
            "globalAccount": "customer1_12345",
            "netVizPort": 3892,
            "stdoutLogging": true
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/appdynamics/cluster-agent-operator-bundle@sha256:344e06684771a54b6a74d6d05713160381ffed8a472f0e1dcc4321ac21015c21",
      "bundle_path_digest": "sha256:344e06684771a54b6a74d6d05713160381ffed8a472f0e1dcc4321ac21015c21",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-12T09:09:48.718000+00:00",
      "csv_description": "AppDynamics Operator simplifies the configuration and lifecycle management of the AppDynamics ClusterAgent and the AppDynamics Machine Agent on different Kubernetes distributions and OpenShift. \nThe Operator encapsulates key operational knowledge on how to configure and upgrade the ClusterAgent and the Machine Agent. \nIt knows, for example, which configuration changes are benign and do not require restart of the ClusterAgent, \nwhich minimizes unnecesary load on the cluster API server.\n\n\n## Operator deployment\n\n* Create namespace for AppDynamics\n```\n  kubectl create namespace appdynamics\n```\n\n* Create Secret `cluster-agent-secret`. Set \"controller-key\" \nto the access key to the AppDynamics controller.  \n\n```\nkubectl -n appdynamics create secret generic cluster-agent-secret --from-literal=controller-key=\"<controller-access-key>\"\n```\n\n* Update the image reference in the Operator deployment spec, if necessary.  \n\nFor additional details refer to [this documentation](https://github.com/Appdynamics/appdynamics-operator)\n",
      "csv_display_name": "Appdynamics Operator",
      "csv_metadata_description": "End to end monitoring of applications on Kubernetes and OpenShift clusters with AppDynamics.",
      "csv_name": "appdynamics-operator.v0.6.11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:47:22.172000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "appdynamics-operator",
      "provided_apis": [
        {
          "group": "appdynamics.com",
          "kind": "Adam",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clusteragent",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clustercollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Hostcollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "InfraViz",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "cluster-agent-operator-9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950-annotation"
        },
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "appdynamics-operator"
        },
        {
          "digest": "sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "name": "cluster-agent-5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.6.11",
      "version_original": "0.6.11"
    },
    {
      "_id": "613ee1f2d63f43cf6ee374ea",
      "alm_examples": [
        {
          "kind": "Adam",
          "metadata": {
            "name": "example-adam",
            "namespace": "appdynamics"
          },
          "spec": {
            "controllerUrl": "https://saas.appdynamics.com"
          }
        },
        {
          "kind": "Clusteragent",
          "metadata": {
            "name": "k8s-cluster-agent",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "appName": "Cluster1",
            "controllerUrl": "https://saas.appdynamics.com",
            "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
            "serviceAccountName": "appdynamics-cluster-agent",
            "stdoutLogging": "true"
          }
        },
        {
          "kind": "InfraViz",
          "metadata": {
            "name": "appd-infraviz",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "controllerUrl": "https://saas.appdynamics.com",
            "enableDockerViz": "false",
            "enableMasters": true,
            "globalAccount": "customer1_12345",
            "netVizPort": 3892,
            "stdoutLogging": true
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/appdynamics/cluster-agent-operator-bundle@sha256:344e06684771a54b6a74d6d05713160381ffed8a472f0e1dcc4321ac21015c21",
      "bundle_path_digest": "sha256:344e06684771a54b6a74d6d05713160381ffed8a472f0e1dcc4321ac21015c21",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T05:30:26.281000+00:00",
      "csv_description": "AppDynamics Operator simplifies the configuration and lifecycle management of the AppDynamics ClusterAgent and the AppDynamics Machine Agent on different Kubernetes distributions and OpenShift. \nThe Operator encapsulates key operational knowledge on how to configure and upgrade the ClusterAgent and the Machine Agent. \nIt knows, for example, which configuration changes are benign and do not require restart of the ClusterAgent, \nwhich minimizes unnecesary load on the cluster API server.\n\n\n## Operator deployment\n\n* Create namespace for AppDynamics\n```\n  kubectl create namespace appdynamics\n```\n\n* Create Secret `cluster-agent-secret`. Set \"controller-key\" \nto the access key to the AppDynamics controller.  \n\n```\nkubectl -n appdynamics create secret generic cluster-agent-secret --from-literal=controller-key=\"<controller-access-key>\"\n```\n\n* Update the image reference in the Operator deployment spec, if necessary.  \n\nFor additional details refer to [this documentation](https://github.com/Appdynamics/appdynamics-operator)\n",
      "csv_display_name": "Appdynamics Operator",
      "csv_metadata_description": "End to end monitoring of applications on Kubernetes and OpenShift clusters with AppDynamics.",
      "csv_name": "appdynamics-operator.v0.6.11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:35:49.694000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "appdynamics-operator",
      "provided_apis": [
        {
          "group": "appdynamics.com",
          "kind": "Clustercollector",
          "plural": "clustercollectors",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Hostcollector",
          "plural": "hostcollectors",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "InfraViz",
          "plural": "infravizs",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Adam",
          "plural": "adams",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clusteragent",
          "plural": "clusteragents",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "cluster-agent-operator-9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950-annotation"
        },
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "appdynamics-operator"
        },
        {
          "digest": "sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "name": "cluster-agent-5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "0.6.11",
      "version_original": "0.6.11"
    },
    {
      "_id": "613ee24fa5ebcd070d163d7c",
      "alm_examples": [
        {
          "kind": "DynaKube",
          "metadata": {
            "name": "dynakube",
            "namespace": "dynatrace"
          },
          "spec": {
            "activeGate": {
              "image": ""
            },
            "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
            "classicFullStack": {
              "enabled": true,
              "tolerations": [
                {
                  "effect": "NoSchedule",
                  "key": "node-role.kubernetes.io/master",
                  "operator": "Exists"
                }
              ]
            },
            "kubernetesMonitoring": {
              "enabled": true
            },
            "oneAgent": {
              "image": ""
            },
            "routing": {
              "enabled": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/dynatrace/dynatrace-operator-bundle@sha256:1a3898915535373eed8f860b8eb906fa2ecaebbb95c4e2d74488e12ca1540f9b",
      "bundle_path_digest": "sha256:1a3898915535373eed8f860b8eb906fa2ecaebbb95c4e2d74488e12ca1540f9b",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T05:31:59.788000+00:00",
      "csv_description": "The Dynatrace Operator supports rollout and lifecycle of various Dynatrace components in Kubernetes and OpenShift.\n\nAs of launch, the Dynatrace Operator can be used to deploy a containerized ActiveGate for Kubernetes API monitoring. New capabilities will be added to the Dynatrace Operator over time including metric routing, and API monitoring for AWS, Azure, GCP, and vSphere.\n\nWith v0.2.0 we added the classicFullStack functionality which allows rolling out the OneAgent to your Kubernetes cluster.\nFurthermore, the Dynatrace Operator is now capable of rolling out a containerized ActiveGate for routing the OneAgent traffic.\n\n### Installation\nOnce you've installed the Dynatrace Operator, you can create a DynaKube custom resource.\n\nFirst, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.\n\nFor assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).\n\n``` $ oc -n <project> create secret generic dynakube --from-literal=\"apiToken=API_TOKEN\" --from-literal=\"paasToken=PAAS_TOKEN\" ```\n\nYou may update this Secret at any time to rotate the tokens.\n\nAfter creation of the secret add the DynaKube object in the project where the Dynatrace Operator has been deployed, configured to your needs.\n\n### Required Parameters\n* `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .\n\n### Advanced Options\n* **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster\n* **Image Override** - use a copy of the ActiveGate container image from a registry other than Docker's or Red Hat's\n\n#### Kubernetes Monitoring\n* **Kubernetes Monitoring** - when enabled the Dynatrace Operator will create a containerized ActiveGate with the capability to monitor your OpenShift cluster\n* **Replicas** - defines how many replicas of the containerized ActiveGate should be created\n* **NodeSelectors** - select a subset of your cluster's nodes to run the Dynatrace ActiveGate on, based on labels\n* **Environment variables** - define environment variables for the ActiveGate container\n\nFor a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).\n\n### Help\nYou can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-dynatrace-operator).\n",
      "csv_display_name": "Dynatrace Operator",
      "csv_metadata_description": "",
      "csv_name": "dynatrace-operator.v0.2.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:54:47.325000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "dynatrace-operator",
      "provided_apis": [
        {
          "group": "dynatrace.com",
          "kind": "DynaKube",
          "plural": "dynakubes",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3",
          "name": "dynatrace-operator-afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3-annotation"
        },
        {
          "digest": "sha256:afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3",
          "image": "registry.connect.redhat.com/dynatrace/dynatrace-operator@sha256:afac1c0f4035ac032a0224f1214e12ab58fb6262f90e630598482abbfdccbac3",
          "name": "dynatrace-operator"
        },
        {
          "digest": "sha256:3975446f23bfd4cab0930d3cba62a795893f9085084e6887121ceaa94391c849",
          "image": "registry.connect.redhat.com/dynatrace/oneagent@sha256:3975446f23bfd4cab0930d3cba62a795893f9085084e6887121ceaa94391c849",
          "name": "dynatrace_oneagent"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "0.2.2",
      "version_original": "0.2.2"
    },
    {
      "_id": "613ee336a5ebcd070d163d7d",
      "alm_examples": [
        {
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {
            "autopilot": {
              "enabled": false
            }
          }
        },
        {
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:f77759337cc4364cc169b25af560ba93386cd2516fbfe022b2dfc8305405bdba",
      "bundle_path_digest": "sha256:f77759337cc4364cc169b25af560ba93386cd2516fbfe022b2dfc8305405bdba",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T05:35:50.115000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.5.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-04-04T14:49:09.831000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.5.1",
      "version_original": "1.5.1"
    },
    {
      "_id": "613ee419c9bc35f21dc4a278",
      "alm_examples": [
        {
          "kind": "Kpow",
          "metadata": {
            "name": "kpow-sample"
          },
          "spec": {
            "bootstrap": "",
            "data_policy_configuration": {
              "data_policy_configuration_file": ""
            },
            "environment_name": "change me",
            "github_sso_options": {
              "auth_provider_type": "",
              "openid_api_uri": "",
              "openid_auth_uri": "",
              "openid_client_id": "",
              "openid_client_secret": "",
              "openid_landing_uri": "",
              "openid_token_uri": ""
            },
            "global_access_configuration": {
              "allow_broker_edit": "",
              "allow_connect_create": "",
              "allow_connect_edit": "",
              "allow_group_edit": "",
              "allow_schema_create": "",
              "allow_schema_edit": "",
              "allow_topic_create": "",
              "allow_topic_delete": "",
              "allow_topic_edit": "",
              "allow_topic_inspect": "",
              "allow_topic_produce": ""
            },
            "https_configuration": {
              "enable_https": false,
              "https_keystore": "",
              "https_keystore_password": "",
              "https_keystore_type": "",
              "https_truststore": "",
              "https_truststore_password": "",
              "https_truststore_type": ""
            },
            "kafka_cluster_configuration": {
              "sasl_jaas_config": "",
              "sasl_login_callback_handler_class": "",
              "sasl_mechanism": "",
              "security_protocol": "",
              "ssl_cipher_suites": "",
              "ssl_enabled_protocols": "",
              "ssl_endpoint_identification_algorithm": "",
              "ssl_key_password": "",
              "ssl_keymanager_algorithm": "",
              "ssl_keystore_location": "",
              "ssl_keystore_password": "",
              "ssl_keystore_type": "",
              "ssl_protocol": "",
              "ssl_provider": "",
              "ssl_secure_random_implementation": "",
              "ssl_trustmanager_algorithm": "",
              "ssl_truststore_location": "",
              "ssl_truststore_password": "",
              "ssl_truststore_type": ""
            },
            "kafka_connect_configuration": {
              "connect_auth": "",
              "connect_basic_auth_pass": "",
              "connect_basic_auth_user": "",
              "connect_group_id": "",
              "connect_offset_storage_topic": "",
              "connect_rest_url": ""
            },
            "license_configuration": {
              "license_code": "your-licence-code",
              "license_expiry": "your-license-expiry",
              "license_id": "your-license-id",
              "license_signature": "e.g. aa8ac6df95...",
              "licensee": "your-org"
            },
            "live_mode_configuration": {
              "live_mode_enabled": "",
              "live_mode_interval_ms": "",
              "live_mode_max_concurrent_users": "",
              "live_mode_period_ms": ""
            },
            "okta_sso_options": {
              "auth_provider_type": "",
              "okta_organisation": "",
              "openid_client_id": "",
              "openid_client_secret": "",
              "openid_landing_uri": ""
            },
            "prometheus_egress": false,
            "rbac_configuration": {
              "rbac_configuration_file": ""
            },
            "saml_sso_options": {
              "auth_provider_type": "",
              "saml_acs_url": "",
              "saml_cert": "",
              "saml_metadata_file": "",
              "saml_relying_party_identifier": ""
            },
            "schema_registry_configuration": {
              "schema_registry_auth": "",
              "schema_registry_name": "",
              "schema_registry_password": "",
              "schema_registry_url": "",
              "schema_registry_user": ""
            },
            "serdis_configuration": {
              "available_key_serdes": "",
              "available_value_serdes": "",
              "custom_serdes": "",
              "default_key_serdes": "",
              "default_value_serdes": ""
            },
            "slack_integration": {
              "slack_webhook_url": ""
            },
            "system_configuration": {
              "http_port": "",
              "replication_factor": "",
              "show_splash": "",
              "snapshot_parallelism": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/operatr-io/kpow-operator-bundle@sha256:eb62d828a556a1bfb3381ae46355332e52ca63143a66376002c1c15d52646543",
      "bundle_path_digest": "sha256:eb62d828a556a1bfb3381ae46355332e52ca63143a66376002c1c15d52646543",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T05:39:37.241000+00:00",
      "csv_description": "\nUse kPow to monitor and manage your Kafka Clusters, Schema Registries, and Connect Clusters.\n\nThe minimum required configuration to run kPow is **license details and a bootstrap URL**.\n\nIf you have a Red Hat Marketplace subscription to kPow you will receive a license directly.\n\nSign-up online to receive a [free 30-day trial license](https://kpow.io/try) in minutes.\n\nkPow is compatible with Red Hat AMQ Streams.\n\n----\n\n##kPow CR Spec Details\n\nAfter you have installed the KPow operator, create a CR to get an instance of KPow. You can setup a Kpow instance with custom configuration using the specifications given in the CR. Refer below table to understand the available specifications for custom configuration of KPow. Set the required value for specifications while creating the CR.\n\n-----\n\nRequired Configuration            |            Details / Example\n----------------------------------|-----------------------------\n**License Details**               | If you need a license, start a [free 30-day trial today](https://kpow.io/try).\n`license_id`                      | a620baa9-956e-43bf-9bc5-adf635533c4a\n`license_code`                      | RH_MARKETPLACE\n`licensee`                          | Your Organisation\n`license_expiry`                    | \"2022-28-12\" - quoted to ensure expiry field interpreted as a string, not a date.\n`license_signature`                 | 8094DEC9...\n**Minimum Configuration**         | kPow will start with only license details and a Kafka bootstrap URL.\n`bootstrap`                        | The Bootstrap URL of your Kafka Cluster\n\nOptional Configuration            |            Details / Example\n----------------------------------|-----------------------------\n**System Options**                | General system configuration\n`environment_name`                  | A meaningful name for this installation, e.g. \"Trade Book (Staging)\"\n`http_port`                         | The port that kPow will serve its UI (default: 3000)\n`show_splash`                       | Show the kPow splash screen for each new user session (default: true)\n[**System HTTPS Options**](https://docs.kpow.io/features/https-connections) | the kPow UI can be served over HTTPS\n`enable_https`                      | default: false \n`https_keystore_location`           | e.g. \"/ssl/https.keystore.jks\"\n`https_keystore_type`               | e.g. JKS\n`https_keystore_password`           | e.g. \"ssl-key-pass\"\n`https_truststore_location`         | e.g. \"/ssl/https.truststore.jks\"\n`https_truststore_type`             | e.g. JKS\n`https_truststore_password`         | e.g. \"ssl-trust-pass\"\n[**Kafka Cluster Options**](https://kafka.apache.org/documentation/#adminclientconfigs) | kPow uses the same Kafka Cluster connection options as a standard consumer, producer, or admin client.\n`security_protocol`                 | e.g. SASL_SSL\n`sasl_mechanism`                    | e.g. PLAIN\n`sasl_jaas_config`                  | e.g. \"org.apache.kafka.common.security.plain.Plain..\"\n`sasl_login_callback_handler_class` | e.g. \"com.corp.kafka.security.sasl.oauth.KafkaBrokerTokenCreator\"\n`ssl_keystore_location`             | e.g. \"/ssl/kafka.keystore.jks\"\n`ssl_keystore_password`             | e.g. \"keystore-pass-123\"\n`ssl_key_password`                  | e.g. \"key-pass-123\"\n`ssl_keystore_type`                 | e.g. JKS\n`ssl_keymanager_algorithm`          | e.g. SunX509\n`ssl_truststore_location`           | e.g. \"/ssl/kafka.truststore.jks\"\n`ssl_truststore_password`           | e.g. \"trust-pass-123\"\n`ssl_truststore_type`               | e.g. JKS\n`ssl_trustmanager_algorithm`        | e.g. PKIX\n`ssl_endpoint_identification_algorithm` | e.g. https\n`ssl_provider`                      | e.g. \"default\"\n`ssl_cipher_suites`                 | e.g. \"default: \n`ssl_protocol`                      | e.g. TLS\n`ssl_enabled_protocols`             | e.g. \"TLSv.12,TLSv1.1,TLSv1\"\n`ssl_secure_random_implementation`  | e.g. SHA1PRNG\n[**Schema Registry**](https://docs.kpow.io/config/schema-registry) | Monitor and manage a Schema Registry connected to your Kafka Cluster\n`schema_registry_url`               | e.g. \"https://registry-host\"\n`schema_registry_auth`              | e.g. USER_INFO\n`schema_registry_user`              | e.g. registry-user\n`schema_registry_password`          | e.g. registry-pass\n[**Kafka Connect**](https://docs.kpow.io/config/kafka-connect) | Monitor and manage a Kafka Connect Cluster connected to your Kafka Cluster\n`connect_rest_url`                  | e.g. http://localhost:8083\n`connect_auth`                      | e.g. BASIC\n`connect_basic_auth_user`           | e.g. connect-user\n`connect_basic_auth_pass`           | e.g. connect-pass\n`connect_group_id`                  | e.g. connect-group-id\n`connect_offset_storage_topic`      | e.g. connect-topic\n[**Live Mode**](https://docs.kpow.io/features/live-mode) | View and monitor your Kafka resources in near-realtime\n`live_mode_enabled`                 | default: true\n`live_mode_period_ms`               | The duration of a live mode session, default: 60000ms / one minute\n`live_mode_interval`                | The duration between live mode snapshot gathering, default: 5000ms / five seconds\n`live_mode_max_concurrent_users`    | The number of concurrent live mode sessions that may be active, default: 2\n**Data Inspect**                  | Provide custom serdes, set default serdes, or restrict serdes available to users\n[custom_serdes](https://docs.kpow.io/features/data-inspect/serdes#custom-serdes) | e.g. \"io.kpow.SerdeOne,io.kpow.SerdeTwo\"\n`default_key_serdes`                | e.g. \"JSON\"\n`default_value_serdes`              | e.g. \"AVRO\"\n`available_key_serdes`              | e.g. \"JSON,String,Transit / JSON\"\n`available_value_serdes`            | e.g. \"JSON,String,io.kpow.SerdeOne\"\n[**Prometheus Endpoints**](https://docs.kpow.io/features/prometheus) | kPow provides metrics and offsets via Prometheus Endpoints for scraping into your enterprise monitoring system of choice.\n`prometheus_egress`                 | default: false\n[**Slack Integration**](https://docs.kpow.io/features/slack-integration) | kPow can send audit log records to a slack channel of your choosing\n`slack_webhook_url`                 | e.g \"https://hooks.slack.com/services/...\"\n[**Simple Access Control**](https://docs.kpow.io/user-authorization/simple-access-control) | kPow supports simple control of users actions via global access controls\n`allow_topic_create`                | default: false - Users may create new topics\n`allow_topic_delete`                | default: false - Users may delete existing topics\n`allow_topic_inspect`               | default: false - Users may view topic key and value content\n`allow_topic_produce`              | default: false - Users may produce new messages to topics\n`allow_topic_edit`                  | default: false - Users may edit topic configuration\n`allow_broker_edit`                 | default: false - Users may edit broker configuration\n`allow_group_edit`                  | default: false - Users may edit and delete consumer groups and group offsets\n`allow_schema_edit`                 | default: false - Users may edit schemas and subjects\n`allow_schema_create`               | default: false - Users may create new schemas and subjects\n`allow_connect_create`              | default: false - Users may create new connectors and tasks\n`allow_connect_edit`                | default: false - Users may edit, pause, delete, and restart connectors and tasks\n[**Role Based Access Control**](https://docs.kpow.io/user-authorization/role-based-access-control) | kPow supports Role Based Access Control when used in conjunction with User Authentication\n`rbac_configuration_file`           | e.g. \"/path/to/rbac-config.yaml\"\n[**Data Policies**](https://docs.kpow.io/features/data-policies) | kPow supports masking and redaction of sensitive data in Data Inspect and kREPL results\n`data_policy_configuration_file`       | e.g. \"/path/to/masking/config.yaml\"\n[**User Authentication**](https://docs.kpow.io/user-authentication/overview) | kPow supports a number of methods of User Authentication including SAML, Okta, and OpenID\n[**Okta SSO**](https://docs.kpow.io/user-authentication/openid/okta) | Authenticate users with Okta \n`auth_provider_type`                | \"okta\"\n`okta_organisation`                 | e.g. \"your-organisation\"\n`openid_client_id`                  | The 'Client ID' found in the \"Client Credentials\" section of your Okta integration\n`openid_client_secret`              | The 'Client Secret' found in the \"Client Credentials\" section of your Okta integration\n`openid_landing_uri`                | e.g. \"https://staging.kpow.z-corp.com\"\n[**Github SSO**](https://docs.kpow.io/user-authentication/openid/github) | Authenticate users with Github (including Github Enterprise)\n`auth_provider_type`                | \"github\"\n`openid_token_id`                   | e.g. \"https://github.com/login/oauth/access_token\" or \"[GHE Server URL]/login/oauth/access_token\"\n`openid_auth_uri`                   | e.g. \"https://github.com/login/oauth/authorize\" or \"[GHE Server URL]/login/oauth/authorize\"\n`openid_api_uri`                    | e.g. \"https://api.github.com/user\" or, \"[GHE Server URL]/api/v3/user\"\n`openid_client_id`                  | e.g. The 'Client ID' found in your configured Github Oath App\n`openid_client_secret`              | The 'Client Secret' found in your configured Github Oath App\n`openid_landing_uri`                | e.g. \"https://staging.kpow.z-corp.com/\"\n[**SAML SSO**](https://docs.kpow.io/user-authentication/saml) | Authenticate users with SAML (custom, Azure AD, AWS SSO, Okta, etc)\n`auth_provider_type`                | \"saml\"\n`saml_relying_party_identifier`     | e.g. \"kpow.io\"\n`saml_asc_url`                      | e.g. \"https://kpow.corp.com/saml\"\n`saml_metadata_file`                | e.g. \"/opt/kpow/aws-metadata.xml\" (optional)\n`saml_cert`                         | e.g. \"/var/certs/saml-cert.cer\"\n\n----\n\n## Providing Configuration Files to kPow\n\nSome of the configuration above is provided as files (e.g. YAML or Keystores). You will need to provide these iles in the form of secrets or configmaps.\n\nFor Example the specification ```https_keystore_location``` needs a keystore file in location *\"/ssl/https.keystore.jks\"*.\n\nThe Operator expectes the file in the form of a secret to be available in the namespace where its installed. \n\nPlease follow below instructions to know how to create the Secret or configmap for such CR Specifications listed above:\n\n**Command to Create ConfigMaps from a file:**\n\n```\n  oc create configmap <name of configmap> --from-file <file name> -n <project-name>\n```\n\n**Command to create secrets from a file:**\n\n```\n  oc create secret generic <name of secret> --from-file <file name> -n <project-name>\n```\n\n  **NOTE**:The configmaps and secrets must be created prior to running the operator and must adhere to the names mentioned below.\t\n  \n----    \nCR Spec          \t  \t\t\t\t\t\t\t    \t|       CR Value\t\t\t|  Creation command\n--------------------------------------------------------|---------------------------|-----------------------------\n`rbac_configuration.rbac_configuration_file`\t\t\t\t|/opt/kpow/rbac-config.yaml\t|oc create rbac-configuration --from-file rbac-config.yaml -n <project-name\\>\n`data_policy_configuration. data_policy_configuration_file`|/opt/kpow/data-config.yml\t|oc create data-policy-configuration --from-file data-config.yml -n <project-name\\>\n`kafka_cluster_configuration.ssl_keystore_location`\t\t|/ssl/kafka.keystore.jks\t|oc create secret generic kafka-keystore --from-file kafka.keystore.jks -n <project-name\\>\n`kafka_cluster_configuration.ssl_truststore_location`\t\t|/ssl/kafka.truststore.jks\t|oc create secret generic kafka-truststore --from-file kafka.truststore.jks -n <project-name\\>\n`https_configuration.https_keystore`\t\t\t\t\t\t|/ssl/https.keystore.jks\t|oc create secret generic https-keystore --from-file https.keystore.jks -n <project-name\\>\n`https_configuration.https_truststore`\t\t\t\t\t|/ssl/https.truststore.jks\t|oc create secret generic https-truststore --from-file https.truststore.jks -n   <project-name\\>\n`saml_sso_options.saml_metadata_file`\t\t\t\t\t\t|/opt/kpow/aws-metadata.xml\t|oc create saml-metadata --from-file aws-metadata.xml -n <project-name\\>\n`saml_sso_options.saml_cert`\t\t\t\t\t\t\t\t|/var/certs/saml-cert.cer\t|oc create secret generic saml-cert --from-file saml-cert.cer -n <project-name\\>\n",
      "csv_display_name": "kPow for Apache Kafka",
      "csv_metadata_description": "kPow is a toolkit for engineers who build, manage, and maintain systems powered by Apache Kafka",
      "csv_name": "kpow-io.v1.0.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:32:44.444000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "kpow-io-certified",
      "provided_apis": [
        {
          "group": "kpow.io",
          "kind": "Kpow",
          "plural": "kpows",
          "version": "v1beta1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "image": "registry.connect.redhat.com/operatr-io/kpow@sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "name": "kpow-9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185-annotation"
        },
        {
          "digest": "sha256:d1b17c96188a5f2748755f4de355a0f103e93b488108dbefb4c4b5714759f510",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:d1b17c96188a5f2748755f4de355a0f103e93b488108dbefb4c4b5714759f510",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:84fc5cafcf88e5334fbdd540da6d340ca401a69360560c7dde448c03c21a724f",
          "image": "registry.connect.redhat.com/operatr-io/kpow-operator@sha256:84fc5cafcf88e5334fbdd540da6d340ca401a69360560c7dde448c03c21a724f",
          "name": "manager"
        },
        {
          "digest": "sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "image": "registry.connect.redhat.com/operatr-io/kpow@sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "name": "kpow"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.0.4",
      "version_original": "1.0.4"
    },
    {
      "_id": "613ee473c9bc35f21dc4a279",
      "alm_examples": [
        {
          "kind": "StorageCluster",
          "metadata": {
            "annotations": {
              "portworx.io/is-openshift": "true"
            },
            "name": "portworx",
            "namespace": "test-operator"
          },
          "spec": {
            "autopilot": {
              "enabled": false
            }
          }
        },
        {
          "kind": "StorageNode",
          "metadata": {
            "name": "example",
            "namespace": "test-operator"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/portworx/portworx-certified-bundle@sha256:f77759337cc4364cc169b25af560ba93386cd2516fbfe022b2dfc8305405bdba",
      "bundle_path_digest": "sha256:f77759337cc4364cc169b25af560ba93386cd2516fbfe022b2dfc8305405bdba",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-13T05:41:07.839000+00:00",
      "csv_description": "Portworx-Enterprise is the most widely-used and reliable cloud-native\nstorage solution for production workloads and provides high-availability,\ndata protection and security for containerized applications.\n\nPortworx Enterprise enables you to migrate entire applications, including\ndata, between clusters in a single data center or cloud, or between clouds,\nwith a single kubectl command.\n\nThe cloud native storage and data management platform that enterprises trust\nto manage data in containers now has an operator which simplifies the install,\nconfiguration, upgrades and manages the Portworx Enterprise cluster lifecycle.\n\nLearn more about the Portworx Enterprise\n[the data platform for Kubernetes](https://portworx.com/products/introduction)\n\nTo learn more about the platform features, please visit our\n[product features page](https://portworx.com/products/features)\n\n### About Portworx\n\nPortworx is the solution for running stateful containers in production,\ndesigned with DevOps in mind. With Portworx, users can manage any database\nor stateful service on any infrastructure using any container scheduler,\nincluding Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves\nthe five most common problems DevOps teams encounter when running stateful\nservices in production: persistence, high availability, data automation,\nsecurity, and support for multiple data stores and infrastructure.\n\n### How to install StorageCluster\n\nTo customize your cluster's configuration (specification), use the\n[Spec Generator](https://central.portworx.com/) from PX-Central.\n\n### Prerequisite\n\nEnsure ports 17001-17020 on worker nodes are reachable from master and other worker nodes.\n\n### Tutorials\n\n* [Portworx Enterprise on Openshift](https://portworx.com/openshift)\n\n* [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)\n\n* [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)\n\n* [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)\n\n* [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)\n\n* [PostgreSQL on Kubernetes](https://portworx.com/postgres-kubernetes/)\n\n* [MongoDB on Kubernetes](https://portworx.com/mongodb-kubernetes/)\n\n* [Cassandra on Kubernetes](https://portworx.com/cassandra-kubernetes/)\n\n* [Kubernetes backup and recovery](https://portworx.com/kubernetes-backup/)\n\n* [Disaster Recovery for Kubernetes](https://portworx.com/kubernetes-disaster-recovery/)\n\n### Uninstall\n\nDeleting the StorageCluster object for Portworx cluster does not stop Portworx\nservice running on the nodes, to avoid application downtime.\n\nTo uninstall Portworx completely without wiping the data, you should add the\nfollowing delete strategy to the StorageCluster spec:\n```\nspec:\n  deleteStrategy:\n    type: Uninstall\n```\n**Caution:** To uninstall Portworx and **wipe all the data**, you should use the following\ndelete strategy:\n```\nspec:\n  deleteStrategy:\n    type: UninstallAndWipe\n```\n",
      "csv_display_name": "Portworx Enterprise",
      "csv_metadata_description": "Cloud native storage solution for production workloads",
      "csv_name": "portworx-operator.v1.5.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:54:57.194000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.5",
      "organization": "certified-operators",
      "package": "portworx-certified",
      "provided_apis": [
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageCluster",
          "plural": "storageclusters",
          "version": "v1alpha1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1"
        },
        {
          "group": "core.libopenstorage.org",
          "kind": "StorageNode",
          "plural": "storagenodes",
          "version": "v1alpha1"
        }
      ],
      "related_images": [],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.5",
      "version": "1.5.1",
      "version_original": "1.5.1"
    },
    {
      "_id": "613f28f8d63f43cf6ee37505",
      "alm_examples": [
        {
          "kind": "Kpow",
          "metadata": {
            "name": "kpow-sample"
          },
          "spec": {
            "bootstrap": "",
            "data_policy_configuration": {
              "data_policy_configuration_file": ""
            },
            "environment_name": "change me",
            "github_sso_options": {
              "auth_provider_type": "",
              "openid_api_uri": "",
              "openid_auth_uri": "",
              "openid_client_id": "",
              "openid_client_secret": "",
              "openid_landing_uri": "",
              "openid_token_uri": ""
            },
            "global_access_configuration": {
              "allow_broker_edit": "",
              "allow_connect_create": "",
              "allow_connect_edit": "",
              "allow_group_edit": "",
              "allow_schema_create": "",
              "allow_schema_edit": "",
              "allow_topic_create": "",
              "allow_topic_delete": "",
              "allow_topic_edit": "",
              "allow_topic_inspect": "",
              "allow_topic_produce": ""
            },
            "https_configuration": {
              "enable_https": false,
              "https_keystore": "",
              "https_keystore_password": "",
              "https_keystore_type": "",
              "https_truststore": "",
              "https_truststore_password": "",
              "https_truststore_type": ""
            },
            "kafka_cluster_configuration": {
              "sasl_jaas_config": "",
              "sasl_login_callback_handler_class": "",
              "sasl_mechanism": "",
              "security_protocol": "",
              "ssl_cipher_suites": "",
              "ssl_enabled_protocols": "",
              "ssl_endpoint_identification_algorithm": "",
              "ssl_key_password": "",
              "ssl_keymanager_algorithm": "",
              "ssl_keystore_location": "",
              "ssl_keystore_password": "",
              "ssl_keystore_type": "",
              "ssl_protocol": "",
              "ssl_provider": "",
              "ssl_secure_random_implementation": "",
              "ssl_trustmanager_algorithm": "",
              "ssl_truststore_location": "",
              "ssl_truststore_password": "",
              "ssl_truststore_type": ""
            },
            "kafka_connect_configuration": {
              "connect_auth": "",
              "connect_basic_auth_pass": "",
              "connect_basic_auth_user": "",
              "connect_group_id": "",
              "connect_offset_storage_topic": "",
              "connect_rest_url": ""
            },
            "license_configuration": {
              "license_code": "your-licence-code",
              "license_expiry": "your-license-expiry",
              "license_id": "your-license-id",
              "license_signature": "e.g. aa8ac6df95...",
              "licensee": "your-org"
            },
            "live_mode_configuration": {
              "live_mode_enabled": "",
              "live_mode_interval_ms": "",
              "live_mode_max_concurrent_users": "",
              "live_mode_period_ms": ""
            },
            "okta_sso_options": {
              "auth_provider_type": "",
              "okta_organisation": "",
              "openid_client_id": "",
              "openid_client_secret": "",
              "openid_landing_uri": ""
            },
            "prometheus_egress": false,
            "rbac_configuration": {
              "rbac_configuration_file": ""
            },
            "saml_sso_options": {
              "auth_provider_type": "",
              "saml_acs_url": "",
              "saml_cert": "",
              "saml_metadata_file": "",
              "saml_relying_party_identifier": ""
            },
            "schema_registry_configuration": {
              "schema_registry_auth": "",
              "schema_registry_name": "",
              "schema_registry_password": "",
              "schema_registry_url": "",
              "schema_registry_user": ""
            },
            "serdis_configuration": {
              "available_key_serdes": "",
              "available_value_serdes": "",
              "custom_serdes": "",
              "default_key_serdes": "",
              "default_value_serdes": ""
            },
            "slack_integration": {
              "slack_webhook_url": ""
            },
            "system_configuration": {
              "http_port": "",
              "replication_factor": "",
              "show_splash": "",
              "snapshot_parallelism": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/operatr-io/kpow-operator-bundle@sha256:eb62d828a556a1bfb3381ae46355332e52ca63143a66376002c1c15d52646543",
      "bundle_path_digest": "sha256:eb62d828a556a1bfb3381ae46355332e52ca63143a66376002c1c15d52646543",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T10:33:28.374000+00:00",
      "csv_description": "\nUse kPow to monitor and manage your Kafka Clusters, Schema Registries, and Connect Clusters.\n\nThe minimum required configuration to run kPow is **license details and a bootstrap URL**.\n\nIf you have a Red Hat Marketplace subscription to kPow you will receive a license directly.\n\nSign-up online to receive a [free 30-day trial license](https://kpow.io/try) in minutes.\n\nkPow is compatible with Red Hat AMQ Streams.\n\n----\n\n##kPow CR Spec Details\n\nAfter you have installed the KPow operator, create a CR to get an instance of KPow. You can setup a Kpow instance with custom configuration using the specifications given in the CR. Refer below table to understand the available specifications for custom configuration of KPow. Set the required value for specifications while creating the CR.\n\n-----\n\nRequired Configuration            |            Details / Example\n----------------------------------|-----------------------------\n**License Details**               | If you need a license, start a [free 30-day trial today](https://kpow.io/try).\n`license_id`                      | a620baa9-956e-43bf-9bc5-adf635533c4a\n`license_code`                      | RH_MARKETPLACE\n`licensee`                          | Your Organisation\n`license_expiry`                    | \"2022-28-12\" - quoted to ensure expiry field interpreted as a string, not a date.\n`license_signature`                 | 8094DEC9...\n**Minimum Configuration**         | kPow will start with only license details and a Kafka bootstrap URL.\n`bootstrap`                        | The Bootstrap URL of your Kafka Cluster\n\nOptional Configuration            |            Details / Example\n----------------------------------|-----------------------------\n**System Options**                | General system configuration\n`environment_name`                  | A meaningful name for this installation, e.g. \"Trade Book (Staging)\"\n`http_port`                         | The port that kPow will serve its UI (default: 3000)\n`show_splash`                       | Show the kPow splash screen for each new user session (default: true)\n[**System HTTPS Options**](https://docs.kpow.io/features/https-connections) | the kPow UI can be served over HTTPS\n`enable_https`                      | default: false \n`https_keystore_location`           | e.g. \"/ssl/https.keystore.jks\"\n`https_keystore_type`               | e.g. JKS\n`https_keystore_password`           | e.g. \"ssl-key-pass\"\n`https_truststore_location`         | e.g. \"/ssl/https.truststore.jks\"\n`https_truststore_type`             | e.g. JKS\n`https_truststore_password`         | e.g. \"ssl-trust-pass\"\n[**Kafka Cluster Options**](https://kafka.apache.org/documentation/#adminclientconfigs) | kPow uses the same Kafka Cluster connection options as a standard consumer, producer, or admin client.\n`security_protocol`                 | e.g. SASL_SSL\n`sasl_mechanism`                    | e.g. PLAIN\n`sasl_jaas_config`                  | e.g. \"org.apache.kafka.common.security.plain.Plain..\"\n`sasl_login_callback_handler_class` | e.g. \"com.corp.kafka.security.sasl.oauth.KafkaBrokerTokenCreator\"\n`ssl_keystore_location`             | e.g. \"/ssl/kafka.keystore.jks\"\n`ssl_keystore_password`             | e.g. \"keystore-pass-123\"\n`ssl_key_password`                  | e.g. \"key-pass-123\"\n`ssl_keystore_type`                 | e.g. JKS\n`ssl_keymanager_algorithm`          | e.g. SunX509\n`ssl_truststore_location`           | e.g. \"/ssl/kafka.truststore.jks\"\n`ssl_truststore_password`           | e.g. \"trust-pass-123\"\n`ssl_truststore_type`               | e.g. JKS\n`ssl_trustmanager_algorithm`        | e.g. PKIX\n`ssl_endpoint_identification_algorithm` | e.g. https\n`ssl_provider`                      | e.g. \"default\"\n`ssl_cipher_suites`                 | e.g. \"default: \n`ssl_protocol`                      | e.g. TLS\n`ssl_enabled_protocols`             | e.g. \"TLSv.12,TLSv1.1,TLSv1\"\n`ssl_secure_random_implementation`  | e.g. SHA1PRNG\n[**Schema Registry**](https://docs.kpow.io/config/schema-registry) | Monitor and manage a Schema Registry connected to your Kafka Cluster\n`schema_registry_url`               | e.g. \"https://registry-host\"\n`schema_registry_auth`              | e.g. USER_INFO\n`schema_registry_user`              | e.g. registry-user\n`schema_registry_password`          | e.g. registry-pass\n[**Kafka Connect**](https://docs.kpow.io/config/kafka-connect) | Monitor and manage a Kafka Connect Cluster connected to your Kafka Cluster\n`connect_rest_url`                  | e.g. http://localhost:8083\n`connect_auth`                      | e.g. BASIC\n`connect_basic_auth_user`           | e.g. connect-user\n`connect_basic_auth_pass`           | e.g. connect-pass\n`connect_group_id`                  | e.g. connect-group-id\n`connect_offset_storage_topic`      | e.g. connect-topic\n[**Live Mode**](https://docs.kpow.io/features/live-mode) | View and monitor your Kafka resources in near-realtime\n`live_mode_enabled`                 | default: true\n`live_mode_period_ms`               | The duration of a live mode session, default: 60000ms / one minute\n`live_mode_interval`                | The duration between live mode snapshot gathering, default: 5000ms / five seconds\n`live_mode_max_concurrent_users`    | The number of concurrent live mode sessions that may be active, default: 2\n**Data Inspect**                  | Provide custom serdes, set default serdes, or restrict serdes available to users\n[custom_serdes](https://docs.kpow.io/features/data-inspect/serdes#custom-serdes) | e.g. \"io.kpow.SerdeOne,io.kpow.SerdeTwo\"\n`default_key_serdes`                | e.g. \"JSON\"\n`default_value_serdes`              | e.g. \"AVRO\"\n`available_key_serdes`              | e.g. \"JSON,String,Transit / JSON\"\n`available_value_serdes`            | e.g. \"JSON,String,io.kpow.SerdeOne\"\n[**Prometheus Endpoints**](https://docs.kpow.io/features/prometheus) | kPow provides metrics and offsets via Prometheus Endpoints for scraping into your enterprise monitoring system of choice.\n`prometheus_egress`                 | default: false\n[**Slack Integration**](https://docs.kpow.io/features/slack-integration) | kPow can send audit log records to a slack channel of your choosing\n`slack_webhook_url`                 | e.g \"https://hooks.slack.com/services/...\"\n[**Simple Access Control**](https://docs.kpow.io/user-authorization/simple-access-control) | kPow supports simple control of users actions via global access controls\n`allow_topic_create`                | default: false - Users may create new topics\n`allow_topic_delete`                | default: false - Users may delete existing topics\n`allow_topic_inspect`               | default: false - Users may view topic key and value content\n`allow_topic_produce`              | default: false - Users may produce new messages to topics\n`allow_topic_edit`                  | default: false - Users may edit topic configuration\n`allow_broker_edit`                 | default: false - Users may edit broker configuration\n`allow_group_edit`                  | default: false - Users may edit and delete consumer groups and group offsets\n`allow_schema_edit`                 | default: false - Users may edit schemas and subjects\n`allow_schema_create`               | default: false - Users may create new schemas and subjects\n`allow_connect_create`              | default: false - Users may create new connectors and tasks\n`allow_connect_edit`                | default: false - Users may edit, pause, delete, and restart connectors and tasks\n[**Role Based Access Control**](https://docs.kpow.io/user-authorization/role-based-access-control) | kPow supports Role Based Access Control when used in conjunction with User Authentication\n`rbac_configuration_file`           | e.g. \"/path/to/rbac-config.yaml\"\n[**Data Policies**](https://docs.kpow.io/features/data-policies) | kPow supports masking and redaction of sensitive data in Data Inspect and kREPL results\n`data_policy_configuration_file`       | e.g. \"/path/to/masking/config.yaml\"\n[**User Authentication**](https://docs.kpow.io/user-authentication/overview) | kPow supports a number of methods of User Authentication including SAML, Okta, and OpenID\n[**Okta SSO**](https://docs.kpow.io/user-authentication/openid/okta) | Authenticate users with Okta \n`auth_provider_type`                | \"okta\"\n`okta_organisation`                 | e.g. \"your-organisation\"\n`openid_client_id`                  | The 'Client ID' found in the \"Client Credentials\" section of your Okta integration\n`openid_client_secret`              | The 'Client Secret' found in the \"Client Credentials\" section of your Okta integration\n`openid_landing_uri`                | e.g. \"https://staging.kpow.z-corp.com\"\n[**Github SSO**](https://docs.kpow.io/user-authentication/openid/github) | Authenticate users with Github (including Github Enterprise)\n`auth_provider_type`                | \"github\"\n`openid_token_id`                   | e.g. \"https://github.com/login/oauth/access_token\" or \"[GHE Server URL]/login/oauth/access_token\"\n`openid_auth_uri`                   | e.g. \"https://github.com/login/oauth/authorize\" or \"[GHE Server URL]/login/oauth/authorize\"\n`openid_api_uri`                    | e.g. \"https://api.github.com/user\" or, \"[GHE Server URL]/api/v3/user\"\n`openid_client_id`                  | e.g. The 'Client ID' found in your configured Github Oath App\n`openid_client_secret`              | The 'Client Secret' found in your configured Github Oath App\n`openid_landing_uri`                | e.g. \"https://staging.kpow.z-corp.com/\"\n[**SAML SSO**](https://docs.kpow.io/user-authentication/saml) | Authenticate users with SAML (custom, Azure AD, AWS SSO, Okta, etc)\n`auth_provider_type`                | \"saml\"\n`saml_relying_party_identifier`     | e.g. \"kpow.io\"\n`saml_asc_url`                      | e.g. \"https://kpow.corp.com/saml\"\n`saml_metadata_file`                | e.g. \"/opt/kpow/aws-metadata.xml\" (optional)\n`saml_cert`                         | e.g. \"/var/certs/saml-cert.cer\"\n\n----\n\n## Providing Configuration Files to kPow\n\nSome of the configuration above is provided as files (e.g. YAML or Keystores). You will need to provide these iles in the form of secrets or configmaps.\n\nFor Example the specification ```https_keystore_location``` needs a keystore file in location *\"/ssl/https.keystore.jks\"*.\n\nThe Operator expectes the file in the form of a secret to be available in the namespace where its installed. \n\nPlease follow below instructions to know how to create the Secret or configmap for such CR Specifications listed above:\n\n**Command to Create ConfigMaps from a file:**\n\n```\n  oc create configmap <name of configmap> --from-file <file name> -n <project-name>\n```\n\n**Command to create secrets from a file:**\n\n```\n  oc create secret generic <name of secret> --from-file <file name> -n <project-name>\n```\n\n  **NOTE**:The configmaps and secrets must be created prior to running the operator and must adhere to the names mentioned below.\t\n  \n----    \nCR Spec          \t  \t\t\t\t\t\t\t    \t|       CR Value\t\t\t|  Creation command\n--------------------------------------------------------|---------------------------|-----------------------------\n`rbac_configuration.rbac_configuration_file`\t\t\t\t|/opt/kpow/rbac-config.yaml\t|oc create rbac-configuration --from-file rbac-config.yaml -n <project-name\\>\n`data_policy_configuration. data_policy_configuration_file`|/opt/kpow/data-config.yml\t|oc create data-policy-configuration --from-file data-config.yml -n <project-name\\>\n`kafka_cluster_configuration.ssl_keystore_location`\t\t|/ssl/kafka.keystore.jks\t|oc create secret generic kafka-keystore --from-file kafka.keystore.jks -n <project-name\\>\n`kafka_cluster_configuration.ssl_truststore_location`\t\t|/ssl/kafka.truststore.jks\t|oc create secret generic kafka-truststore --from-file kafka.truststore.jks -n <project-name\\>\n`https_configuration.https_keystore`\t\t\t\t\t\t|/ssl/https.keystore.jks\t|oc create secret generic https-keystore --from-file https.keystore.jks -n <project-name\\>\n`https_configuration.https_truststore`\t\t\t\t\t|/ssl/https.truststore.jks\t|oc create secret generic https-truststore --from-file https.truststore.jks -n   <project-name\\>\n`saml_sso_options.saml_metadata_file`\t\t\t\t\t\t|/opt/kpow/aws-metadata.xml\t|oc create saml-metadata --from-file aws-metadata.xml -n <project-name\\>\n`saml_sso_options.saml_cert`\t\t\t\t\t\t\t\t|/var/certs/saml-cert.cer\t|oc create secret generic saml-cert --from-file saml-cert.cer -n <project-name\\>\n",
      "csv_display_name": "kPow for Apache Kafka",
      "csv_metadata_description": "kPow is a toolkit for engineers who build, manage, and maintain systems powered by Apache Kafka",
      "csv_name": "kpow-io.v1.0.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T15:06:36.641000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "kpow-io-certified",
      "provided_apis": [
        {
          "group": "kpow.io",
          "kind": "Kpow",
          "version": "v1beta1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "image": "registry.connect.redhat.com/operatr-io/kpow@sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "name": "kpow-9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185-annotation"
        },
        {
          "digest": "sha256:d1b17c96188a5f2748755f4de355a0f103e93b488108dbefb4c4b5714759f510",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:d1b17c96188a5f2748755f4de355a0f103e93b488108dbefb4c4b5714759f510",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:84fc5cafcf88e5334fbdd540da6d340ca401a69360560c7dde448c03c21a724f",
          "image": "registry.connect.redhat.com/operatr-io/kpow-operator@sha256:84fc5cafcf88e5334fbdd540da6d340ca401a69360560c7dde448c03c21a724f",
          "name": "manager"
        },
        {
          "digest": "sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "image": "registry.connect.redhat.com/operatr-io/kpow@sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "name": "kpow"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.0.4",
      "version_original": "1.0.4"
    },
    {
      "_id": "613f2b1fd63f43cf6ee37507",
      "alm_examples": [
        {
          "kind": "Adam",
          "metadata": {
            "name": "example-adam",
            "namespace": "appdynamics"
          },
          "spec": {
            "controllerUrl": "https://saas.appdynamics.com"
          }
        },
        {
          "kind": "Clusteragent",
          "metadata": {
            "name": "k8s-cluster-agent",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "appName": "Cluster1",
            "controllerUrl": "https://saas.appdynamics.com",
            "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
            "serviceAccountName": "appdynamics-cluster-agent",
            "stdoutLogging": "true"
          }
        },
        {
          "kind": "InfraViz",
          "metadata": {
            "name": "appd-infraviz",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "controllerUrl": "https://saas.appdynamics.com",
            "enableDockerViz": "false",
            "enableMasters": true,
            "globalAccount": "customer1_12345",
            "netVizPort": 3892,
            "stdoutLogging": true
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/appdynamics/cluster-agent-operator-bundle@sha256:344e06684771a54b6a74d6d05713160381ffed8a472f0e1dcc4321ac21015c21",
      "bundle_path_digest": "sha256:344e06684771a54b6a74d6d05713160381ffed8a472f0e1dcc4321ac21015c21",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T10:42:39.961000+00:00",
      "csv_description": "AppDynamics Operator simplifies the configuration and lifecycle management of the AppDynamics ClusterAgent and the AppDynamics Machine Agent on different Kubernetes distributions and OpenShift. \nThe Operator encapsulates key operational knowledge on how to configure and upgrade the ClusterAgent and the Machine Agent. \nIt knows, for example, which configuration changes are benign and do not require restart of the ClusterAgent, \nwhich minimizes unnecesary load on the cluster API server.\n\n\n## Operator deployment\n\n* Create namespace for AppDynamics\n```\n  kubectl create namespace appdynamics\n```\n\n* Create Secret `cluster-agent-secret`. Set \"controller-key\" \nto the access key to the AppDynamics controller.  \n\n```\nkubectl -n appdynamics create secret generic cluster-agent-secret --from-literal=controller-key=\"<controller-access-key>\"\n```\n\n* Update the image reference in the Operator deployment spec, if necessary.  \n\nFor additional details refer to [this documentation](https://github.com/Appdynamics/appdynamics-operator)\n",
      "csv_display_name": "Appdynamics Operator",
      "csv_metadata_description": "End to end monitoring of applications on Kubernetes and OpenShift clusters with AppDynamics.",
      "csv_name": "appdynamics-operator.v0.6.11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:18:58.687000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "appdynamics-operator",
      "provided_apis": [
        {
          "group": "appdynamics.com",
          "kind": "Clusteragent",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clustercollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Hostcollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "InfraViz",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Adam",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "cluster-agent-operator-9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950-annotation"
        },
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "appdynamics-operator"
        },
        {
          "digest": "sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "name": "cluster-agent-5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.6.11",
      "version_original": "0.6.11"
    },
    {
      "_id": "613f2bd8a5ebcd070d163d90",
      "alm_examples": [
        {
          "kind": "Kpow",
          "metadata": {
            "name": "kpow-sample"
          },
          "spec": {
            "bootstrap": "",
            "data_policy_configuration": {
              "data_policy_configuration_file": ""
            },
            "environment_name": "change me",
            "github_sso_options": {
              "auth_provider_type": "",
              "openid_api_uri": "",
              "openid_auth_uri": "",
              "openid_client_id": "",
              "openid_client_secret": "",
              "openid_landing_uri": "",
              "openid_token_uri": ""
            },
            "global_access_configuration": {
              "allow_broker_edit": "",
              "allow_connect_create": "",
              "allow_connect_edit": "",
              "allow_group_edit": "",
              "allow_schema_create": "",
              "allow_schema_edit": "",
              "allow_topic_create": "",
              "allow_topic_delete": "",
              "allow_topic_edit": "",
              "allow_topic_inspect": "",
              "allow_topic_produce": ""
            },
            "https_configuration": {
              "enable_https": false,
              "https_keystore": "",
              "https_keystore_password": "",
              "https_keystore_type": "",
              "https_truststore": "",
              "https_truststore_password": "",
              "https_truststore_type": ""
            },
            "kafka_cluster_configuration": {
              "sasl_jaas_config": "",
              "sasl_login_callback_handler_class": "",
              "sasl_mechanism": "",
              "security_protocol": "",
              "ssl_cipher_suites": "",
              "ssl_enabled_protocols": "",
              "ssl_endpoint_identification_algorithm": "",
              "ssl_key_password": "",
              "ssl_keymanager_algorithm": "",
              "ssl_keystore_location": "",
              "ssl_keystore_password": "",
              "ssl_keystore_type": "",
              "ssl_protocol": "",
              "ssl_provider": "",
              "ssl_secure_random_implementation": "",
              "ssl_trustmanager_algorithm": "",
              "ssl_truststore_location": "",
              "ssl_truststore_password": "",
              "ssl_truststore_type": ""
            },
            "kafka_connect_configuration": {
              "connect_auth": "",
              "connect_basic_auth_pass": "",
              "connect_basic_auth_user": "",
              "connect_group_id": "",
              "connect_offset_storage_topic": "",
              "connect_rest_url": ""
            },
            "license_configuration": {
              "license_code": "your-licence-code",
              "license_expiry": "your-license-expiry",
              "license_id": "your-license-id",
              "license_signature": "e.g. aa8ac6df95...",
              "licensee": "your-org"
            },
            "live_mode_configuration": {
              "live_mode_enabled": "",
              "live_mode_interval_ms": "",
              "live_mode_max_concurrent_users": "",
              "live_mode_period_ms": ""
            },
            "okta_sso_options": {
              "auth_provider_type": "",
              "okta_organisation": "",
              "openid_client_id": "",
              "openid_client_secret": "",
              "openid_landing_uri": ""
            },
            "prometheus_egress": false,
            "rbac_configuration": {
              "rbac_configuration_file": ""
            },
            "saml_sso_options": {
              "auth_provider_type": "",
              "saml_acs_url": "",
              "saml_cert": "",
              "saml_metadata_file": "",
              "saml_relying_party_identifier": ""
            },
            "schema_registry_configuration": {
              "schema_registry_auth": "",
              "schema_registry_name": "",
              "schema_registry_password": "",
              "schema_registry_url": "",
              "schema_registry_user": ""
            },
            "serdis_configuration": {
              "available_key_serdes": "",
              "available_value_serdes": "",
              "custom_serdes": "",
              "default_key_serdes": "",
              "default_value_serdes": ""
            },
            "slack_integration": {
              "slack_webhook_url": ""
            },
            "system_configuration": {
              "http_port": "",
              "replication_factor": "",
              "show_splash": "",
              "snapshot_parallelism": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/operatr-io/kpow-operator-bundle@sha256:eb62d828a556a1bfb3381ae46355332e52ca63143a66376002c1c15d52646543",
      "bundle_path_digest": "sha256:eb62d828a556a1bfb3381ae46355332e52ca63143a66376002c1c15d52646543",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T10:45:44.534000+00:00",
      "csv_description": "\nUse kPow to monitor and manage your Kafka Clusters, Schema Registries, and Connect Clusters.\n\nThe minimum required configuration to run kPow is **license details and a bootstrap URL**.\n\nIf you have a Red Hat Marketplace subscription to kPow you will receive a license directly.\n\nSign-up online to receive a [free 30-day trial license](https://kpow.io/try) in minutes.\n\nkPow is compatible with Red Hat AMQ Streams.\n\n----\n\n##kPow CR Spec Details\n\nAfter you have installed the KPow operator, create a CR to get an instance of KPow. You can setup a Kpow instance with custom configuration using the specifications given in the CR. Refer below table to understand the available specifications for custom configuration of KPow. Set the required value for specifications while creating the CR.\n\n-----\n\nRequired Configuration            |            Details / Example\n----------------------------------|-----------------------------\n**License Details**               | If you need a license, start a [free 30-day trial today](https://kpow.io/try).\n`license_id`                      | a620baa9-956e-43bf-9bc5-adf635533c4a\n`license_code`                      | RH_MARKETPLACE\n`licensee`                          | Your Organisation\n`license_expiry`                    | \"2022-28-12\" - quoted to ensure expiry field interpreted as a string, not a date.\n`license_signature`                 | 8094DEC9...\n**Minimum Configuration**         | kPow will start with only license details and a Kafka bootstrap URL.\n`bootstrap`                        | The Bootstrap URL of your Kafka Cluster\n\nOptional Configuration            |            Details / Example\n----------------------------------|-----------------------------\n**System Options**                | General system configuration\n`environment_name`                  | A meaningful name for this installation, e.g. \"Trade Book (Staging)\"\n`http_port`                         | The port that kPow will serve its UI (default: 3000)\n`show_splash`                       | Show the kPow splash screen for each new user session (default: true)\n[**System HTTPS Options**](https://docs.kpow.io/features/https-connections) | the kPow UI can be served over HTTPS\n`enable_https`                      | default: false \n`https_keystore_location`           | e.g. \"/ssl/https.keystore.jks\"\n`https_keystore_type`               | e.g. JKS\n`https_keystore_password`           | e.g. \"ssl-key-pass\"\n`https_truststore_location`         | e.g. \"/ssl/https.truststore.jks\"\n`https_truststore_type`             | e.g. JKS\n`https_truststore_password`         | e.g. \"ssl-trust-pass\"\n[**Kafka Cluster Options**](https://kafka.apache.org/documentation/#adminclientconfigs) | kPow uses the same Kafka Cluster connection options as a standard consumer, producer, or admin client.\n`security_protocol`                 | e.g. SASL_SSL\n`sasl_mechanism`                    | e.g. PLAIN\n`sasl_jaas_config`                  | e.g. \"org.apache.kafka.common.security.plain.Plain..\"\n`sasl_login_callback_handler_class` | e.g. \"com.corp.kafka.security.sasl.oauth.KafkaBrokerTokenCreator\"\n`ssl_keystore_location`             | e.g. \"/ssl/kafka.keystore.jks\"\n`ssl_keystore_password`             | e.g. \"keystore-pass-123\"\n`ssl_key_password`                  | e.g. \"key-pass-123\"\n`ssl_keystore_type`                 | e.g. JKS\n`ssl_keymanager_algorithm`          | e.g. SunX509\n`ssl_truststore_location`           | e.g. \"/ssl/kafka.truststore.jks\"\n`ssl_truststore_password`           | e.g. \"trust-pass-123\"\n`ssl_truststore_type`               | e.g. JKS\n`ssl_trustmanager_algorithm`        | e.g. PKIX\n`ssl_endpoint_identification_algorithm` | e.g. https\n`ssl_provider`                      | e.g. \"default\"\n`ssl_cipher_suites`                 | e.g. \"default: \n`ssl_protocol`                      | e.g. TLS\n`ssl_enabled_protocols`             | e.g. \"TLSv.12,TLSv1.1,TLSv1\"\n`ssl_secure_random_implementation`  | e.g. SHA1PRNG\n[**Schema Registry**](https://docs.kpow.io/config/schema-registry) | Monitor and manage a Schema Registry connected to your Kafka Cluster\n`schema_registry_url`               | e.g. \"https://registry-host\"\n`schema_registry_auth`              | e.g. USER_INFO\n`schema_registry_user`              | e.g. registry-user\n`schema_registry_password`          | e.g. registry-pass\n[**Kafka Connect**](https://docs.kpow.io/config/kafka-connect) | Monitor and manage a Kafka Connect Cluster connected to your Kafka Cluster\n`connect_rest_url`                  | e.g. http://localhost:8083\n`connect_auth`                      | e.g. BASIC\n`connect_basic_auth_user`           | e.g. connect-user\n`connect_basic_auth_pass`           | e.g. connect-pass\n`connect_group_id`                  | e.g. connect-group-id\n`connect_offset_storage_topic`      | e.g. connect-topic\n[**Live Mode**](https://docs.kpow.io/features/live-mode) | View and monitor your Kafka resources in near-realtime\n`live_mode_enabled`                 | default: true\n`live_mode_period_ms`               | The duration of a live mode session, default: 60000ms / one minute\n`live_mode_interval`                | The duration between live mode snapshot gathering, default: 5000ms / five seconds\n`live_mode_max_concurrent_users`    | The number of concurrent live mode sessions that may be active, default: 2\n**Data Inspect**                  | Provide custom serdes, set default serdes, or restrict serdes available to users\n[custom_serdes](https://docs.kpow.io/features/data-inspect/serdes#custom-serdes) | e.g. \"io.kpow.SerdeOne,io.kpow.SerdeTwo\"\n`default_key_serdes`                | e.g. \"JSON\"\n`default_value_serdes`              | e.g. \"AVRO\"\n`available_key_serdes`              | e.g. \"JSON,String,Transit / JSON\"\n`available_value_serdes`            | e.g. \"JSON,String,io.kpow.SerdeOne\"\n[**Prometheus Endpoints**](https://docs.kpow.io/features/prometheus) | kPow provides metrics and offsets via Prometheus Endpoints for scraping into your enterprise monitoring system of choice.\n`prometheus_egress`                 | default: false\n[**Slack Integration**](https://docs.kpow.io/features/slack-integration) | kPow can send audit log records to a slack channel of your choosing\n`slack_webhook_url`                 | e.g \"https://hooks.slack.com/services/...\"\n[**Simple Access Control**](https://docs.kpow.io/user-authorization/simple-access-control) | kPow supports simple control of users actions via global access controls\n`allow_topic_create`                | default: false - Users may create new topics\n`allow_topic_delete`                | default: false - Users may delete existing topics\n`allow_topic_inspect`               | default: false - Users may view topic key and value content\n`allow_topic_produce`              | default: false - Users may produce new messages to topics\n`allow_topic_edit`                  | default: false - Users may edit topic configuration\n`allow_broker_edit`                 | default: false - Users may edit broker configuration\n`allow_group_edit`                  | default: false - Users may edit and delete consumer groups and group offsets\n`allow_schema_edit`                 | default: false - Users may edit schemas and subjects\n`allow_schema_create`               | default: false - Users may create new schemas and subjects\n`allow_connect_create`              | default: false - Users may create new connectors and tasks\n`allow_connect_edit`                | default: false - Users may edit, pause, delete, and restart connectors and tasks\n[**Role Based Access Control**](https://docs.kpow.io/user-authorization/role-based-access-control) | kPow supports Role Based Access Control when used in conjunction with User Authentication\n`rbac_configuration_file`           | e.g. \"/path/to/rbac-config.yaml\"\n[**Data Policies**](https://docs.kpow.io/features/data-policies) | kPow supports masking and redaction of sensitive data in Data Inspect and kREPL results\n`data_policy_configuration_file`       | e.g. \"/path/to/masking/config.yaml\"\n[**User Authentication**](https://docs.kpow.io/user-authentication/overview) | kPow supports a number of methods of User Authentication including SAML, Okta, and OpenID\n[**Okta SSO**](https://docs.kpow.io/user-authentication/openid/okta) | Authenticate users with Okta \n`auth_provider_type`                | \"okta\"\n`okta_organisation`                 | e.g. \"your-organisation\"\n`openid_client_id`                  | The 'Client ID' found in the \"Client Credentials\" section of your Okta integration\n`openid_client_secret`              | The 'Client Secret' found in the \"Client Credentials\" section of your Okta integration\n`openid_landing_uri`                | e.g. \"https://staging.kpow.z-corp.com\"\n[**Github SSO**](https://docs.kpow.io/user-authentication/openid/github) | Authenticate users with Github (including Github Enterprise)\n`auth_provider_type`                | \"github\"\n`openid_token_id`                   | e.g. \"https://github.com/login/oauth/access_token\" or \"[GHE Server URL]/login/oauth/access_token\"\n`openid_auth_uri`                   | e.g. \"https://github.com/login/oauth/authorize\" or \"[GHE Server URL]/login/oauth/authorize\"\n`openid_api_uri`                    | e.g. \"https://api.github.com/user\" or, \"[GHE Server URL]/api/v3/user\"\n`openid_client_id`                  | e.g. The 'Client ID' found in your configured Github Oath App\n`openid_client_secret`              | The 'Client Secret' found in your configured Github Oath App\n`openid_landing_uri`                | e.g. \"https://staging.kpow.z-corp.com/\"\n[**SAML SSO**](https://docs.kpow.io/user-authentication/saml) | Authenticate users with SAML (custom, Azure AD, AWS SSO, Okta, etc)\n`auth_provider_type`                | \"saml\"\n`saml_relying_party_identifier`     | e.g. \"kpow.io\"\n`saml_asc_url`                      | e.g. \"https://kpow.corp.com/saml\"\n`saml_metadata_file`                | e.g. \"/opt/kpow/aws-metadata.xml\" (optional)\n`saml_cert`                         | e.g. \"/var/certs/saml-cert.cer\"\n\n----\n\n## Providing Configuration Files to kPow\n\nSome of the configuration above is provided as files (e.g. YAML or Keystores). You will need to provide these iles in the form of secrets or configmaps.\n\nFor Example the specification ```https_keystore_location``` needs a keystore file in location *\"/ssl/https.keystore.jks\"*.\n\nThe Operator expectes the file in the form of a secret to be available in the namespace where its installed. \n\nPlease follow below instructions to know how to create the Secret or configmap for such CR Specifications listed above:\n\n**Command to Create ConfigMaps from a file:**\n\n```\n  oc create configmap <name of configmap> --from-file <file name> -n <project-name>\n```\n\n**Command to create secrets from a file:**\n\n```\n  oc create secret generic <name of secret> --from-file <file name> -n <project-name>\n```\n\n  **NOTE**:The configmaps and secrets must be created prior to running the operator and must adhere to the names mentioned below.\t\n  \n----    \nCR Spec          \t  \t\t\t\t\t\t\t    \t|       CR Value\t\t\t|  Creation command\n--------------------------------------------------------|---------------------------|-----------------------------\n`rbac_configuration.rbac_configuration_file`\t\t\t\t|/opt/kpow/rbac-config.yaml\t|oc create rbac-configuration --from-file rbac-config.yaml -n <project-name\\>\n`data_policy_configuration. data_policy_configuration_file`|/opt/kpow/data-config.yml\t|oc create data-policy-configuration --from-file data-config.yml -n <project-name\\>\n`kafka_cluster_configuration.ssl_keystore_location`\t\t|/ssl/kafka.keystore.jks\t|oc create secret generic kafka-keystore --from-file kafka.keystore.jks -n <project-name\\>\n`kafka_cluster_configuration.ssl_truststore_location`\t\t|/ssl/kafka.truststore.jks\t|oc create secret generic kafka-truststore --from-file kafka.truststore.jks -n <project-name\\>\n`https_configuration.https_keystore`\t\t\t\t\t\t|/ssl/https.keystore.jks\t|oc create secret generic https-keystore --from-file https.keystore.jks -n <project-name\\>\n`https_configuration.https_truststore`\t\t\t\t\t|/ssl/https.truststore.jks\t|oc create secret generic https-truststore --from-file https.truststore.jks -n   <project-name\\>\n`saml_sso_options.saml_metadata_file`\t\t\t\t\t\t|/opt/kpow/aws-metadata.xml\t|oc create saml-metadata --from-file aws-metadata.xml -n <project-name\\>\n`saml_sso_options.saml_cert`\t\t\t\t\t\t\t\t|/var/certs/saml-cert.cer\t|oc create secret generic saml-cert --from-file saml-cert.cer -n <project-name\\>\n",
      "csv_display_name": "kPow for Apache Kafka",
      "csv_metadata_description": "kPow is a toolkit for engineers who build, manage, and maintain systems powered by Apache Kafka",
      "csv_name": "kpow-io.v1.0.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:12:58.065000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "kpow-io-certified",
      "provided_apis": [
        {
          "group": "kpow.io",
          "kind": "Kpow",
          "version": "v1beta1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "image": "registry.connect.redhat.com/operatr-io/kpow@sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "name": "kpow-9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185-annotation"
        },
        {
          "digest": "sha256:d1b17c96188a5f2748755f4de355a0f103e93b488108dbefb4c4b5714759f510",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:d1b17c96188a5f2748755f4de355a0f103e93b488108dbefb4c4b5714759f510",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:84fc5cafcf88e5334fbdd540da6d340ca401a69360560c7dde448c03c21a724f",
          "image": "registry.connect.redhat.com/operatr-io/kpow-operator@sha256:84fc5cafcf88e5334fbdd540da6d340ca401a69360560c7dde448c03c21a724f",
          "name": "manager"
        },
        {
          "digest": "sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "image": "registry.connect.redhat.com/operatr-io/kpow@sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "name": "kpow"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "1.0.4",
      "version_original": "1.0.4"
    },
    {
      "_id": "613f630dc9bc35f21dc4a41d",
      "alm_examples": [
        {
          "kind": "Kpow",
          "metadata": {
            "name": "kpow-sample"
          },
          "spec": {
            "bootstrap": "",
            "data_policy_configuration": {
              "data_policy_configuration_file": ""
            },
            "environment_name": "change me",
            "github_sso_options": {
              "auth_provider_type": "",
              "openid_api_uri": "",
              "openid_auth_uri": "",
              "openid_client_id": "",
              "openid_client_secret": "",
              "openid_landing_uri": "",
              "openid_token_uri": ""
            },
            "global_access_configuration": {
              "allow_broker_edit": "",
              "allow_connect_create": "",
              "allow_connect_edit": "",
              "allow_group_edit": "",
              "allow_schema_create": "",
              "allow_schema_edit": "",
              "allow_topic_create": "",
              "allow_topic_delete": "",
              "allow_topic_edit": "",
              "allow_topic_inspect": "",
              "allow_topic_produce": ""
            },
            "https_configuration": {
              "enable_https": false,
              "https_keystore": "",
              "https_keystore_password": "",
              "https_keystore_type": "",
              "https_truststore": "",
              "https_truststore_password": "",
              "https_truststore_type": ""
            },
            "kafka_cluster_configuration": {
              "sasl_jaas_config": "",
              "sasl_login_callback_handler_class": "",
              "sasl_mechanism": "",
              "security_protocol": "",
              "ssl_cipher_suites": "",
              "ssl_enabled_protocols": "",
              "ssl_endpoint_identification_algorithm": "",
              "ssl_key_password": "",
              "ssl_keymanager_algorithm": "",
              "ssl_keystore_location": "",
              "ssl_keystore_password": "",
              "ssl_keystore_type": "",
              "ssl_protocol": "",
              "ssl_provider": "",
              "ssl_secure_random_implementation": "",
              "ssl_trustmanager_algorithm": "",
              "ssl_truststore_location": "",
              "ssl_truststore_password": "",
              "ssl_truststore_type": ""
            },
            "kafka_connect_configuration": {
              "connect_auth": "",
              "connect_basic_auth_pass": "",
              "connect_basic_auth_user": "",
              "connect_group_id": "",
              "connect_offset_storage_topic": "",
              "connect_rest_url": ""
            },
            "license_configuration": {
              "license_code": "your-licence-code",
              "license_expiry": "your-license-expiry",
              "license_id": "your-license-id",
              "license_signature": "e.g. aa8ac6df95...",
              "licensee": "your-org"
            },
            "live_mode_configuration": {
              "live_mode_enabled": "",
              "live_mode_interval_ms": "",
              "live_mode_max_concurrent_users": "",
              "live_mode_period_ms": ""
            },
            "okta_sso_options": {
              "auth_provider_type": "",
              "okta_organisation": "",
              "openid_client_id": "",
              "openid_client_secret": "",
              "openid_landing_uri": ""
            },
            "prometheus_egress": false,
            "rbac_configuration": {
              "rbac_configuration_file": ""
            },
            "saml_sso_options": {
              "auth_provider_type": "",
              "saml_acs_url": "",
              "saml_cert": "",
              "saml_metadata_file": "",
              "saml_relying_party_identifier": ""
            },
            "schema_registry_configuration": {
              "schema_registry_auth": "",
              "schema_registry_name": "",
              "schema_registry_password": "",
              "schema_registry_url": "",
              "schema_registry_user": ""
            },
            "serdis_configuration": {
              "available_key_serdes": "",
              "available_value_serdes": "",
              "custom_serdes": "",
              "default_key_serdes": "",
              "default_value_serdes": ""
            },
            "slack_integration": {
              "slack_webhook_url": ""
            },
            "system_configuration": {
              "http_port": "",
              "replication_factor": "",
              "show_splash": "",
              "snapshot_parallelism": ""
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/operatr-io/kpow-operator-bundle@sha256:eb62d828a556a1bfb3381ae46355332e52ca63143a66376002c1c15d52646543",
      "bundle_path_digest": "sha256:eb62d828a556a1bfb3381ae46355332e52ca63143a66376002c1c15d52646543",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T14:41:17.070000+00:00",
      "csv_description": "\nUse kPow to monitor and manage your Kafka Clusters, Schema Registries, and Connect Clusters.\n\nThe minimum required configuration to run kPow is **license details and a bootstrap URL**.\n\nIf you have a Red Hat Marketplace subscription to kPow you will receive a license directly.\n\nSign-up online to receive a [free 30-day trial license](https://kpow.io/try) in minutes.\n\nkPow is compatible with Red Hat AMQ Streams.\n\n----\n\n##kPow CR Spec Details\n\nAfter you have installed the KPow operator, create a CR to get an instance of KPow. You can setup a Kpow instance with custom configuration using the specifications given in the CR. Refer below table to understand the available specifications for custom configuration of KPow. Set the required value for specifications while creating the CR.\n\n-----\n\nRequired Configuration            |            Details / Example\n----------------------------------|-----------------------------\n**License Details**               | If you need a license, start a [free 30-day trial today](https://kpow.io/try).\n`license_id`                      | a620baa9-956e-43bf-9bc5-adf635533c4a\n`license_code`                      | RH_MARKETPLACE\n`licensee`                          | Your Organisation\n`license_expiry`                    | \"2022-28-12\" - quoted to ensure expiry field interpreted as a string, not a date.\n`license_signature`                 | 8094DEC9...\n**Minimum Configuration**         | kPow will start with only license details and a Kafka bootstrap URL.\n`bootstrap`                        | The Bootstrap URL of your Kafka Cluster\n\nOptional Configuration            |            Details / Example\n----------------------------------|-----------------------------\n**System Options**                | General system configuration\n`environment_name`                  | A meaningful name for this installation, e.g. \"Trade Book (Staging)\"\n`http_port`                         | The port that kPow will serve its UI (default: 3000)\n`show_splash`                       | Show the kPow splash screen for each new user session (default: true)\n[**System HTTPS Options**](https://docs.kpow.io/features/https-connections) | the kPow UI can be served over HTTPS\n`enable_https`                      | default: false \n`https_keystore_location`           | e.g. \"/ssl/https.keystore.jks\"\n`https_keystore_type`               | e.g. JKS\n`https_keystore_password`           | e.g. \"ssl-key-pass\"\n`https_truststore_location`         | e.g. \"/ssl/https.truststore.jks\"\n`https_truststore_type`             | e.g. JKS\n`https_truststore_password`         | e.g. \"ssl-trust-pass\"\n[**Kafka Cluster Options**](https://kafka.apache.org/documentation/#adminclientconfigs) | kPow uses the same Kafka Cluster connection options as a standard consumer, producer, or admin client.\n`security_protocol`                 | e.g. SASL_SSL\n`sasl_mechanism`                    | e.g. PLAIN\n`sasl_jaas_config`                  | e.g. \"org.apache.kafka.common.security.plain.Plain..\"\n`sasl_login_callback_handler_class` | e.g. \"com.corp.kafka.security.sasl.oauth.KafkaBrokerTokenCreator\"\n`ssl_keystore_location`             | e.g. \"/ssl/kafka.keystore.jks\"\n`ssl_keystore_password`             | e.g. \"keystore-pass-123\"\n`ssl_key_password`                  | e.g. \"key-pass-123\"\n`ssl_keystore_type`                 | e.g. JKS\n`ssl_keymanager_algorithm`          | e.g. SunX509\n`ssl_truststore_location`           | e.g. \"/ssl/kafka.truststore.jks\"\n`ssl_truststore_password`           | e.g. \"trust-pass-123\"\n`ssl_truststore_type`               | e.g. JKS\n`ssl_trustmanager_algorithm`        | e.g. PKIX\n`ssl_endpoint_identification_algorithm` | e.g. https\n`ssl_provider`                      | e.g. \"default\"\n`ssl_cipher_suites`                 | e.g. \"default: \n`ssl_protocol`                      | e.g. TLS\n`ssl_enabled_protocols`             | e.g. \"TLSv.12,TLSv1.1,TLSv1\"\n`ssl_secure_random_implementation`  | e.g. SHA1PRNG\n[**Schema Registry**](https://docs.kpow.io/config/schema-registry) | Monitor and manage a Schema Registry connected to your Kafka Cluster\n`schema_registry_url`               | e.g. \"https://registry-host\"\n`schema_registry_auth`              | e.g. USER_INFO\n`schema_registry_user`              | e.g. registry-user\n`schema_registry_password`          | e.g. registry-pass\n[**Kafka Connect**](https://docs.kpow.io/config/kafka-connect) | Monitor and manage a Kafka Connect Cluster connected to your Kafka Cluster\n`connect_rest_url`                  | e.g. http://localhost:8083\n`connect_auth`                      | e.g. BASIC\n`connect_basic_auth_user`           | e.g. connect-user\n`connect_basic_auth_pass`           | e.g. connect-pass\n`connect_group_id`                  | e.g. connect-group-id\n`connect_offset_storage_topic`      | e.g. connect-topic\n[**Live Mode**](https://docs.kpow.io/features/live-mode) | View and monitor your Kafka resources in near-realtime\n`live_mode_enabled`                 | default: true\n`live_mode_period_ms`               | The duration of a live mode session, default: 60000ms / one minute\n`live_mode_interval`                | The duration between live mode snapshot gathering, default: 5000ms / five seconds\n`live_mode_max_concurrent_users`    | The number of concurrent live mode sessions that may be active, default: 2\n**Data Inspect**                  | Provide custom serdes, set default serdes, or restrict serdes available to users\n[custom_serdes](https://docs.kpow.io/features/data-inspect/serdes#custom-serdes) | e.g. \"io.kpow.SerdeOne,io.kpow.SerdeTwo\"\n`default_key_serdes`                | e.g. \"JSON\"\n`default_value_serdes`              | e.g. \"AVRO\"\n`available_key_serdes`              | e.g. \"JSON,String,Transit / JSON\"\n`available_value_serdes`            | e.g. \"JSON,String,io.kpow.SerdeOne\"\n[**Prometheus Endpoints**](https://docs.kpow.io/features/prometheus) | kPow provides metrics and offsets via Prometheus Endpoints for scraping into your enterprise monitoring system of choice.\n`prometheus_egress`                 | default: false\n[**Slack Integration**](https://docs.kpow.io/features/slack-integration) | kPow can send audit log records to a slack channel of your choosing\n`slack_webhook_url`                 | e.g \"https://hooks.slack.com/services/...\"\n[**Simple Access Control**](https://docs.kpow.io/user-authorization/simple-access-control) | kPow supports simple control of users actions via global access controls\n`allow_topic_create`                | default: false - Users may create new topics\n`allow_topic_delete`                | default: false - Users may delete existing topics\n`allow_topic_inspect`               | default: false - Users may view topic key and value content\n`allow_topic_produce`              | default: false - Users may produce new messages to topics\n`allow_topic_edit`                  | default: false - Users may edit topic configuration\n`allow_broker_edit`                 | default: false - Users may edit broker configuration\n`allow_group_edit`                  | default: false - Users may edit and delete consumer groups and group offsets\n`allow_schema_edit`                 | default: false - Users may edit schemas and subjects\n`allow_schema_create`               | default: false - Users may create new schemas and subjects\n`allow_connect_create`              | default: false - Users may create new connectors and tasks\n`allow_connect_edit`                | default: false - Users may edit, pause, delete, and restart connectors and tasks\n[**Role Based Access Control**](https://docs.kpow.io/user-authorization/role-based-access-control) | kPow supports Role Based Access Control when used in conjunction with User Authentication\n`rbac_configuration_file`           | e.g. \"/path/to/rbac-config.yaml\"\n[**Data Policies**](https://docs.kpow.io/features/data-policies) | kPow supports masking and redaction of sensitive data in Data Inspect and kREPL results\n`data_policy_configuration_file`       | e.g. \"/path/to/masking/config.yaml\"\n[**User Authentication**](https://docs.kpow.io/user-authentication/overview) | kPow supports a number of methods of User Authentication including SAML, Okta, and OpenID\n[**Okta SSO**](https://docs.kpow.io/user-authentication/openid/okta) | Authenticate users with Okta \n`auth_provider_type`                | \"okta\"\n`okta_organisation`                 | e.g. \"your-organisation\"\n`openid_client_id`                  | The 'Client ID' found in the \"Client Credentials\" section of your Okta integration\n`openid_client_secret`              | The 'Client Secret' found in the \"Client Credentials\" section of your Okta integration\n`openid_landing_uri`                | e.g. \"https://staging.kpow.z-corp.com\"\n[**Github SSO**](https://docs.kpow.io/user-authentication/openid/github) | Authenticate users with Github (including Github Enterprise)\n`auth_provider_type`                | \"github\"\n`openid_token_id`                   | e.g. \"https://github.com/login/oauth/access_token\" or \"[GHE Server URL]/login/oauth/access_token\"\n`openid_auth_uri`                   | e.g. \"https://github.com/login/oauth/authorize\" or \"[GHE Server URL]/login/oauth/authorize\"\n`openid_api_uri`                    | e.g. \"https://api.github.com/user\" or, \"[GHE Server URL]/api/v3/user\"\n`openid_client_id`                  | e.g. The 'Client ID' found in your configured Github Oath App\n`openid_client_secret`              | The 'Client Secret' found in your configured Github Oath App\n`openid_landing_uri`                | e.g. \"https://staging.kpow.z-corp.com/\"\n[**SAML SSO**](https://docs.kpow.io/user-authentication/saml) | Authenticate users with SAML (custom, Azure AD, AWS SSO, Okta, etc)\n`auth_provider_type`                | \"saml\"\n`saml_relying_party_identifier`     | e.g. \"kpow.io\"\n`saml_asc_url`                      | e.g. \"https://kpow.corp.com/saml\"\n`saml_metadata_file`                | e.g. \"/opt/kpow/aws-metadata.xml\" (optional)\n`saml_cert`                         | e.g. \"/var/certs/saml-cert.cer\"\n\n----\n\n## Providing Configuration Files to kPow\n\nSome of the configuration above is provided as files (e.g. YAML or Keystores). You will need to provide these iles in the form of secrets or configmaps.\n\nFor Example the specification ```https_keystore_location``` needs a keystore file in location *\"/ssl/https.keystore.jks\"*.\n\nThe Operator expectes the file in the form of a secret to be available in the namespace where its installed. \n\nPlease follow below instructions to know how to create the Secret or configmap for such CR Specifications listed above:\n\n**Command to Create ConfigMaps from a file:**\n\n```\n  oc create configmap <name of configmap> --from-file <file name> -n <project-name>\n```\n\n**Command to create secrets from a file:**\n\n```\n  oc create secret generic <name of secret> --from-file <file name> -n <project-name>\n```\n\n  **NOTE**:The configmaps and secrets must be created prior to running the operator and must adhere to the names mentioned below.\t\n  \n----    \nCR Spec          \t  \t\t\t\t\t\t\t    \t|       CR Value\t\t\t|  Creation command\n--------------------------------------------------------|---------------------------|-----------------------------\n`rbac_configuration.rbac_configuration_file`\t\t\t\t|/opt/kpow/rbac-config.yaml\t|oc create rbac-configuration --from-file rbac-config.yaml -n <project-name\\>\n`data_policy_configuration. data_policy_configuration_file`|/opt/kpow/data-config.yml\t|oc create data-policy-configuration --from-file data-config.yml -n <project-name\\>\n`kafka_cluster_configuration.ssl_keystore_location`\t\t|/ssl/kafka.keystore.jks\t|oc create secret generic kafka-keystore --from-file kafka.keystore.jks -n <project-name\\>\n`kafka_cluster_configuration.ssl_truststore_location`\t\t|/ssl/kafka.truststore.jks\t|oc create secret generic kafka-truststore --from-file kafka.truststore.jks -n <project-name\\>\n`https_configuration.https_keystore`\t\t\t\t\t\t|/ssl/https.keystore.jks\t|oc create secret generic https-keystore --from-file https.keystore.jks -n <project-name\\>\n`https_configuration.https_truststore`\t\t\t\t\t|/ssl/https.truststore.jks\t|oc create secret generic https-truststore --from-file https.truststore.jks -n   <project-name\\>\n`saml_sso_options.saml_metadata_file`\t\t\t\t\t\t|/opt/kpow/aws-metadata.xml\t|oc create saml-metadata --from-file aws-metadata.xml -n <project-name\\>\n`saml_sso_options.saml_cert`\t\t\t\t\t\t\t\t|/var/certs/saml-cert.cer\t|oc create secret generic saml-cert --from-file saml-cert.cer -n <project-name\\>\n",
      "csv_display_name": "kPow for Apache Kafka",
      "csv_metadata_description": "kPow is a toolkit for engineers who build, manage, and maintain systems powered by Apache Kafka",
      "csv_name": "kpow-io.v1.0.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:13:01.618000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "kpow-io-certified",
      "provided_apis": [
        {
          "group": "kpow.io",
          "kind": "Kpow",
          "version": "v1beta1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "image": "registry.connect.redhat.com/operatr-io/kpow@sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "name": "kpow-9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185-annotation"
        },
        {
          "digest": "sha256:d1b17c96188a5f2748755f4de355a0f103e93b488108dbefb4c4b5714759f510",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:d1b17c96188a5f2748755f4de355a0f103e93b488108dbefb4c4b5714759f510",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:84fc5cafcf88e5334fbdd540da6d340ca401a69360560c7dde448c03c21a724f",
          "image": "registry.connect.redhat.com/operatr-io/kpow-operator@sha256:84fc5cafcf88e5334fbdd540da6d340ca401a69360560c7dde448c03c21a724f",
          "name": "manager"
        },
        {
          "digest": "sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "image": "registry.connect.redhat.com/operatr-io/kpow@sha256:9c99ef45515fde9064f7af80b53a6e84df817305727b2afae42b4b4452f71185",
          "name": "kpow"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.0.4",
      "version_original": "1.0.4"
    },
    {
      "_id": "613f64a8a5ebcd070d164045",
      "alm_examples": [
        {
          "kind": "Adam",
          "metadata": {
            "name": "example-adam",
            "namespace": "appdynamics"
          },
          "spec": {
            "controllerUrl": "https://saas.appdynamics.com"
          }
        },
        {
          "kind": "Clusteragent",
          "metadata": {
            "name": "k8s-cluster-agent",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "appName": "Cluster1",
            "controllerUrl": "https://saas.appdynamics.com",
            "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
            "serviceAccountName": "appdynamics-cluster-agent",
            "stdoutLogging": "true"
          }
        },
        {
          "kind": "InfraViz",
          "metadata": {
            "name": "appd-infraviz",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "controllerUrl": "https://saas.appdynamics.com",
            "enableDockerViz": "false",
            "enableMasters": true,
            "globalAccount": "customer1_12345",
            "netVizPort": 3892,
            "stdoutLogging": true
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/appdynamics/cluster-agent-operator-bundle@sha256:344e06684771a54b6a74d6d05713160381ffed8a472f0e1dcc4321ac21015c21",
      "bundle_path_digest": "sha256:344e06684771a54b6a74d6d05713160381ffed8a472f0e1dcc4321ac21015c21",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T14:48:08.958000+00:00",
      "csv_description": "AppDynamics Operator simplifies the configuration and lifecycle management of the AppDynamics ClusterAgent and the AppDynamics Machine Agent on different Kubernetes distributions and OpenShift. \nThe Operator encapsulates key operational knowledge on how to configure and upgrade the ClusterAgent and the Machine Agent. \nIt knows, for example, which configuration changes are benign and do not require restart of the ClusterAgent, \nwhich minimizes unnecesary load on the cluster API server.\n\n\n## Operator deployment\n\n* Create namespace for AppDynamics\n```\n  kubectl create namespace appdynamics\n```\n\n* Create Secret `cluster-agent-secret`. Set \"controller-key\" \nto the access key to the AppDynamics controller.  \n\n```\nkubectl -n appdynamics create secret generic cluster-agent-secret --from-literal=controller-key=\"<controller-access-key>\"\n```\n\n* Update the image reference in the Operator deployment spec, if necessary.  \n\nFor additional details refer to [this documentation](https://github.com/Appdynamics/appdynamics-operator)\n",
      "csv_display_name": "Appdynamics Operator",
      "csv_metadata_description": "End to end monitoring of applications on Kubernetes and OpenShift clusters with AppDynamics.",
      "csv_name": "appdynamics-operator.v0.6.11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:19:14.419000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "appdynamics-operator",
      "provided_apis": [
        {
          "group": "appdynamics.com",
          "kind": "InfraViz",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Adam",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clusteragent",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clustercollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Hostcollector",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "cluster-agent-operator-9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950-annotation"
        },
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "appdynamics-operator"
        },
        {
          "digest": "sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "name": "cluster-agent-5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.6.11",
      "version_original": "0.6.11"
    },
    {
      "_id": "613f690da5ebcd070d16404b",
      "alm_examples": [
        {
          "kind": "Adam",
          "metadata": {
            "name": "example-adam",
            "namespace": "appdynamics"
          },
          "spec": {
            "controllerUrl": "https://saas.appdynamics.com"
          }
        },
        {
          "kind": "Clusteragent",
          "metadata": {
            "name": "k8s-cluster-agent",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "appName": "Cluster1",
            "controllerUrl": "https://saas.appdynamics.com",
            "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
            "serviceAccountName": "appdynamics-cluster-agent",
            "stdoutLogging": "true"
          }
        },
        {
          "kind": "InfraViz",
          "metadata": {
            "name": "appd-infraviz",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "controllerUrl": "https://saas.appdynamics.com",
            "enableDockerViz": "false",
            "enableMasters": true,
            "globalAccount": "customer1_12345",
            "image": "registry.connect.redhat.com/appdynamics/machine-agent-analytics@sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
            "netVizPort": 3892,
            "stdoutLogging": true
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/appdynamics/cluster-agent-operator-bundle@sha256:8334d8638984099f3ef40793e62f1e68e0691c05c86326ae40a752e39bf3f316",
      "bundle_path_digest": "sha256:8334d8638984099f3ef40793e62f1e68e0691c05c86326ae40a752e39bf3f316",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T15:06:53.534000+00:00",
      "csv_description": "AppDynamics Operator simplifies the configuration and lifecycle management of the AppDynamics ClusterAgent and the AppDynamics Machine Agent on different Kubernetes distributions and OpenShift. \nThe Operator encapsulates key operational knowledge on how to configure and upgrade the ClusterAgent and the Machine Agent. \nIt knows, for example, which configuration changes are benign and do not require restart of the ClusterAgent, \nwhich minimizes unnecesary load on the cluster API server.\n\n\n## Operator deployment\n\n* Create namespace for AppDynamics\n```\n  kubectl create namespace appdynamics\n```\n\n* Create Secret `cluster-agent-secret`. Set \"controller-key\" \nto the access key to the AppDynamics controller.  \n\n```\nkubectl -n appdynamics create secret generic cluster-agent-secret --from-literal=controller-key=\"<controller-access-key>\"\n```\n\n* Update the image reference in the Operator deployment spec, if necessary.  \n\nFor additional details refer to [this documentation](https://github.com/Appdynamics/appdynamics-operator)\n",
      "csv_display_name": "Appdynamics Operator",
      "csv_metadata_description": "End to end monitoring of applications on Kubernetes and OpenShift clusters with AppDynamics.",
      "csv_name": "appdynamics-operator.v0.6.12",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T15:10:06.707000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "appdynamics-operator",
      "provided_apis": [
        {
          "group": "appdynamics.com",
          "kind": "Hostcollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "InfraViz",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Adam",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clusteragent",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clustercollector",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "cluster-agent-operator-9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950-annotation"
        },
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "appdynamics-operator"
        },
        {
          "digest": "sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
          "image": "registry.connect.redhat.com/appdynamics/machine-agent-analytics@sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
          "name": "machine-agent-analytics-b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65-annotation"
        },
        {
          "digest": "sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "name": "cluster-agent-5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.6.12",
      "version_original": "0.6.12"
    },
    {
      "_id": "613f6a8cd63f43cf6ee3769b",
      "alm_examples": [
        {
          "kind": "Adam",
          "metadata": {
            "name": "example-adam",
            "namespace": "appdynamics"
          },
          "spec": {
            "controllerUrl": "https://saas.appdynamics.com"
          }
        },
        {
          "kind": "Clusteragent",
          "metadata": {
            "name": "k8s-cluster-agent",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "appName": "Cluster1",
            "controllerUrl": "https://saas.appdynamics.com",
            "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
            "serviceAccountName": "appdynamics-cluster-agent",
            "stdoutLogging": "true"
          }
        },
        {
          "kind": "InfraViz",
          "metadata": {
            "name": "appd-infraviz",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "controllerUrl": "https://saas.appdynamics.com",
            "enableDockerViz": "false",
            "enableMasters": true,
            "globalAccount": "customer1_12345",
            "image": "registry.connect.redhat.com/appdynamics/machine-agent-analytics@sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
            "netVizPort": 3892,
            "stdoutLogging": true
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/appdynamics/cluster-agent-operator-bundle@sha256:8334d8638984099f3ef40793e62f1e68e0691c05c86326ae40a752e39bf3f316",
      "bundle_path_digest": "sha256:8334d8638984099f3ef40793e62f1e68e0691c05c86326ae40a752e39bf3f316",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T15:13:16.799000+00:00",
      "csv_description": "AppDynamics Operator simplifies the configuration and lifecycle management of the AppDynamics ClusterAgent and the AppDynamics Machine Agent on different Kubernetes distributions and OpenShift. \nThe Operator encapsulates key operational knowledge on how to configure and upgrade the ClusterAgent and the Machine Agent. \nIt knows, for example, which configuration changes are benign and do not require restart of the ClusterAgent, \nwhich minimizes unnecesary load on the cluster API server.\n\n\n## Operator deployment\n\n* Create namespace for AppDynamics\n```\n  kubectl create namespace appdynamics\n```\n\n* Create Secret `cluster-agent-secret`. Set \"controller-key\" \nto the access key to the AppDynamics controller.  \n\n```\nkubectl -n appdynamics create secret generic cluster-agent-secret --from-literal=controller-key=\"<controller-access-key>\"\n```\n\n* Update the image reference in the Operator deployment spec, if necessary.  \n\nFor additional details refer to [this documentation](https://github.com/Appdynamics/appdynamics-operator)\n",
      "csv_display_name": "Appdynamics Operator",
      "csv_metadata_description": "End to end monitoring of applications on Kubernetes and OpenShift clusters with AppDynamics.",
      "csv_name": "appdynamics-operator.v0.6.12",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:20:47.309000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "appdynamics-operator",
      "provided_apis": [
        {
          "group": "appdynamics.com",
          "kind": "Clustercollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Hostcollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "InfraViz",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Adam",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clusteragent",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "cluster-agent-operator-9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950-annotation"
        },
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "appdynamics-operator"
        },
        {
          "digest": "sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
          "image": "registry.connect.redhat.com/appdynamics/machine-agent-analytics@sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
          "name": "machine-agent-analytics-b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65-annotation"
        },
        {
          "digest": "sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "name": "cluster-agent-5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.6.12",
      "version_original": "0.6.12"
    },
    {
      "_id": "613f6b771904331703806a7a",
      "alm_examples": [
        {
          "kind": "Adam",
          "metadata": {
            "name": "example-adam",
            "namespace": "appdynamics"
          },
          "spec": {
            "controllerUrl": "https://saas.appdynamics.com"
          }
        },
        {
          "kind": "Clusteragent",
          "metadata": {
            "name": "k8s-cluster-agent",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "appName": "Cluster1",
            "controllerUrl": "https://saas.appdynamics.com",
            "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
            "serviceAccountName": "appdynamics-cluster-agent",
            "stdoutLogging": "true"
          }
        },
        {
          "kind": "InfraViz",
          "metadata": {
            "name": "appd-infraviz",
            "namespace": "appdynamics"
          },
          "spec": {
            "account": "customer1",
            "controllerUrl": "https://saas.appdynamics.com",
            "enableDockerViz": "false",
            "enableMasters": true,
            "globalAccount": "customer1_12345",
            "image": "registry.connect.redhat.com/appdynamics/machine-agent-analytics@sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
            "netVizPort": 3892,
            "stdoutLogging": true
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/appdynamics/cluster-agent-operator-bundle@sha256:8334d8638984099f3ef40793e62f1e68e0691c05c86326ae40a752e39bf3f316",
      "bundle_path_digest": "sha256:8334d8638984099f3ef40793e62f1e68e0691c05c86326ae40a752e39bf3f316",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2021-09-13T15:17:11.304000+00:00",
      "csv_description": "AppDynamics Operator simplifies the configuration and lifecycle management of the AppDynamics ClusterAgent and the AppDynamics Machine Agent on different Kubernetes distributions and OpenShift. \nThe Operator encapsulates key operational knowledge on how to configure and upgrade the ClusterAgent and the Machine Agent. \nIt knows, for example, which configuration changes are benign and do not require restart of the ClusterAgent, \nwhich minimizes unnecesary load on the cluster API server.\n\n\n## Operator deployment\n\n* Create namespace for AppDynamics\n```\n  kubectl create namespace appdynamics\n```\n\n* Create Secret `cluster-agent-secret`. Set \"controller-key\" \nto the access key to the AppDynamics controller.  \n\n```\nkubectl -n appdynamics create secret generic cluster-agent-secret --from-literal=controller-key=\"<controller-access-key>\"\n```\n\n* Update the image reference in the Operator deployment spec, if necessary.  \n\nFor additional details refer to [this documentation](https://github.com/Appdynamics/appdynamics-operator)\n",
      "csv_display_name": "Appdynamics Operator",
      "csv_metadata_description": "End to end monitoring of applications on Kubernetes and OpenShift clusters with AppDynamics.",
      "csv_name": "appdynamics-operator.v0.6.12",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:15:36.102000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "appdynamics-operator",
      "provided_apis": [
        {
          "group": "appdynamics.com",
          "kind": "Clusteragent",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Clustercollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Hostcollector",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "InfraViz",
          "version": "v1alpha1"
        },
        {
          "group": "appdynamics.com",
          "kind": "Adam",
          "version": "v1alpha1"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "cluster-agent-operator-9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950-annotation"
        },
        {
          "digest": "sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent-operator@sha256:9e992ba1274f818c0ca78d2654ac797e7139584a8e99919f3cf4b0676ac23950",
          "name": "appdynamics-operator"
        },
        {
          "digest": "sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
          "image": "registry.connect.redhat.com/appdynamics/machine-agent-analytics@sha256:b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65",
          "name": "machine-agent-analytics-b7ac0d5d6ed5028b97af3e49f79de95e1a78b3ce03ba93f7c4e648eb4b440f65-annotation"
        },
        {
          "digest": "sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "image": "registry.connect.redhat.com/appdynamics/cluster-agent@sha256:5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f",
          "name": "cluster-agent-5df48126b5e6c22cd73f8b3f3bdb8cc2fc6fae23085e922de32a586b754b6e7f-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.6.12",
      "version_original": "0.6.12"
    },
    {
      "_id": "61401834c9bc35f21dc4a44f",
      "alm_examples": [
        {
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoCompaction": {
                "databaseFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                },
                "parallelCompaction": false,
                "timeWindow": {
                  "abortCompactionOutsideWindow": true,
                  "end": "06:00",
                  "start": "02:00"
                },
                "tombstonePurgeInterval": "72h",
                "viewFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                }
              },
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:df774ff23ab5a517a99d2d6565144574a29566ea8120d27803767aaf7f1c3b7c",
      "bundle_path_digest": "sha256:df774ff23ab5a517a99d2d6565144574a29566ea8120d27803767aaf7f1c3b7c",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-14T03:34:12.019000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.2.1-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-04T14:50:15.349000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "name": "operator-81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f-annotation"
        },
        {
          "digest": "sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "exporter-b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "name": "server-c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "2.2.1-1",
      "version_original": "2.2.1-1"
    },
    {
      "_id": "614018a2a5ebcd070d164099",
      "alm_examples": [
        {
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoCompaction": {
                "databaseFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                },
                "parallelCompaction": false,
                "timeWindow": {
                  "abortCompactionOutsideWindow": true,
                  "end": "06:00",
                  "start": "02:00"
                },
                "tombstonePurgeInterval": "72h",
                "viewFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                }
              },
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:df774ff23ab5a517a99d2d6565144574a29566ea8120d27803767aaf7f1c3b7c",
      "bundle_path_digest": "sha256:df774ff23ab5a517a99d2d6565144574a29566ea8120d27803767aaf7f1c3b7c",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-14T03:36:02.946000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.2.1-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:19:55.064000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "name": "operator-81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f-annotation"
        },
        {
          "digest": "sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "exporter-b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "name": "server-c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "2.2.1-1",
      "version_original": "2.2.1-1"
    },
    {
      "_id": "61401f7aa5ebcd070d1640a7",
      "alm_examples": [
        {
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoCompaction": {
                "databaseFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                },
                "parallelCompaction": false,
                "timeWindow": {
                  "abortCompactionOutsideWindow": true,
                  "end": "06:00",
                  "start": "02:00"
                },
                "tombstonePurgeInterval": "72h",
                "viewFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                }
              },
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:df774ff23ab5a517a99d2d6565144574a29566ea8120d27803767aaf7f1c3b7c",
      "bundle_path_digest": "sha256:df774ff23ab5a517a99d2d6565144574a29566ea8120d27803767aaf7f1c3b7c",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-14T04:05:14.848000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.2.1-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T10:56:08.927000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "name": "operator-81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f-annotation"
        },
        {
          "digest": "sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "exporter-b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "name": "server-c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.2.1-1",
      "version_original": "2.2.1-1"
    },
    {
      "_id": "61402200a5ebcd070d1640a8",
      "alm_examples": [
        {
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoCompaction": {
                "databaseFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                },
                "parallelCompaction": false,
                "timeWindow": {
                  "abortCompactionOutsideWindow": true,
                  "end": "06:00",
                  "start": "02:00"
                },
                "tombstonePurgeInterval": "72h",
                "viewFragmentationThreshold": {
                  "percent": 30,
                  "size": "1Gi"
                }
              },
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:df774ff23ab5a517a99d2d6565144574a29566ea8120d27803767aaf7f1c3b7c",
      "bundle_path_digest": "sha256:df774ff23ab5a517a99d2d6565144574a29566ea8120d27803767aaf7f1c3b7c",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2021-09-14T04:16:00.585000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.2.1-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-04-05T11:22:00.282000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        }
      ],
      "related_images": [
        {
          "digest": "sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "name": "operator-81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f-annotation"
        },
        {
          "digest": "sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:81e281289f56a2634d337e7ed6ca75ee914748a8f00a1baac70c50694b39e11f",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:187046a848f32233e7e92705c57fa864b1d373c2078a92b51c9706bec6e372e5",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784",
          "name": "exporter-b9ff3aec88f42f8e6164d61a1c5f845b4c3dd3f606ac552170d5c61311ce5784-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15",
          "name": "server-c94326da3435265666a4e332c6c5e78ddf74777cb64e1e8728d237c6b5008c15-annotation"
        }
      ],
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.2.1-1",
      "version_original": "2.2.1-1"
    }
  ],
  "page": 15,
  "page_size": 100,
  "total": 3370
}
