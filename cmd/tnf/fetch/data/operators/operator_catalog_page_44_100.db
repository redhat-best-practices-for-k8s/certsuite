{
  "data": [
    {
      "_id": "62fa4c8230a80ba5a208d0ef",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "redislabs/redis",
              "versionTag": "6.0.6-39.rhel7-openshift"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:b0edae8a793cf9e47a9d33a6341e2d2dfd3656ce8f2a4fbaa7d22a7407402b7f",
      "bundle_path_digest": "sha256:b0edae8a793cf9e47a9d33a6341e2d2dfd3656ce8f2a4fbaa7d22a7407402b7f",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.8",
      "creation_date": "2022-08-15T13:39:14.431000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis Labs. Redis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data tructures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-inclass exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n* For service broker - a k8s distribution that supports service catalog (see also: service-catalog)\n\n#### Recommendations/known issues\n\nChanging the name of the Redis Enterprise Cluster in the spec from the\ndefault requires running a command to grant permissions to the pods:\n\n\n> *oc adm policy add-scc-to-group redis-enterprise-scc\nsystem:serviceaccounts:my-project*\n\n\nReplace my-project withh the actual name of the project/namespace.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.0.6-24",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-08-15T13:39:14.431000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis Labs",
      "related_images": [
        {
          "digest": "sha256:998d6be639f0ce0cb0c51f048845c4d7aaacdb820c2c0e0450392c966744507b",
          "image": "redislabs/operator@sha256:998d6be639f0ce0cb0c51f048845c4d7aaacdb820c2c0e0450392c966744507b",
          "name": "operator-998d6be639f0ce0cb0c51f048845c4d7aaacdb820c2c0e0450392c966744507b-annotation"
        },
        {
          "digest": "sha256:998d6be639f0ce0cb0c51f048845c4d7aaacdb820c2c0e0450392c966744507b",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:998d6be639f0ce0cb0c51f048845c4d7aaacdb820c2c0e0450392c966744507b",
          "name": "redis-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "6.0.6-24",
      "version_original": "6.0.6-24"
    },
    {
      "_id": "62fa4cdb30a80ba5a208d218",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "bootstrapperImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise-operator"
            },
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "redisEnterpriseServicesRiggerImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/services-manager"
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:3e0fde49a8e8c5f780a76087bbc42507a42fea0e1fea572ae05ba66d27338365",
      "bundle_path_digest": "sha256:3e0fde49a8e8c5f780a76087bbc42507a42fea0e1fea572ae05ba66d27338365",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.10",
      "creation_date": "2022-08-15T13:40:43.479000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis.\nRedis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n*  Changing the name of the Redis Enterprise Cluster in the spec from the\n  default requires running commands to grant permissions to the pods:\n\n  Operator service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:redis-enterprise-operator*<br>\n\n    Redis Enterprise Cluster service account:\n    > *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:<name of Redis Enterprise Cluster>*\n\n    Replace my-project with the actual name of the project/namespace. Replace <name of Redis Enterprise Cluster> with the name of the Redis Enterprise Cluster.\n\n* When creating a Redis Enterprise Cluster without specifying in its spec a `persistentSpec` with explicit `storageClassName`,\nthe default Storage Class of the Kubernetes cluster will be used to dynamically provision storage for the Redis Enterprise Cluster's nodes.\nHowever, if the Kubernetes cluster doesn't have a default Storage Class configured\nand no specific Storage Class was specified in the Redis Enterprise Cluster Spec, the installation of the cluster could fail.\nTo avoid such scenario in cases where a specific Storage Class isn't specified, make sure the Kubernetes Cluster has a default Storage Cluster configured.\nThis can be done by running a `kubectl get storageclass` command and see if one of the Storage Classes' names contains a `(default)` mark.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.10-45",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-08-15T13:40:43.479000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis",
      "related_images": [
        {
          "digest": "sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "name": "admission-image"
        },
        {
          "digest": "sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "name": "bootstrapper-image"
        },
        {
          "digest": "sha256:8b46f2aacfb3e1b243c0b64975accc55fd80b4de9c9ac4769c3c67a5676addc5",
          "image": "registry.connect.redhat.com/redislabs/services-manager@sha256:8b46f2aacfb3e1b243c0b64975accc55fd80b4de9c9ac4769c3c67a5676addc5",
          "name": "k8s-controller-image"
        },
        {
          "digest": "sha256:ffdf10a1b08115fd4de06f1cd751d36644a9bc9a8c11034cd432e9b66a5d2f73",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise@sha256:ffdf10a1b08115fd4de06f1cd751d36644a9bc9a8c11034cd432e9b66a5d2f73",
          "name": "rs-image"
        },
        {
          "digest": "sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "name": "redis-enterprise-operator-b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6-annotation"
        },
        {
          "digest": "sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:b771ef87bf211c17c37df028c202aac97170fb6d7d5d49b3ccb3410deb8212f6",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "6.2.10-45",
      "version_original": "6.2.10-45"
    },
    {
      "_id": "62fa4cf0e07d71e4beb6b627",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "redislabs/redis",
              "versionTag": "6.0.6-39.rhel7-openshift"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:a16342d44039d3619f08f3fdd411e0074f7809442fbb486939b69f8c15451ecf",
      "bundle_path_digest": "sha256:a16342d44039d3619f08f3fdd411e0074f7809442fbb486939b69f8c15451ecf",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.10",
      "creation_date": "2022-08-15T13:41:04.466000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis Labs. Redis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data tructures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-inclass exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n* For service broker - a k8s distribution that supports service catalog (see also: service-catalog)\n\n#### Recommendations/known issues\n\nChanging the name of the Redis Enterprise Cluster in the spec from the\ndefault requires running a command to grant permissions to the pods:\n\n\n> *oc adm policy add-scc-to-group redis-enterprise-scc\nsystem:serviceaccounts:my-project*\n\n\nReplace my-project withh the actual name of the project/namespace.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.0.6-11",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-08-15T13:41:04.466000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis Labs",
      "related_images": [
        {
          "digest": "sha256:fcdcfbf38860a41a5c4a635cf74d8f421fe48b4180b51c511e5eb3f309c7f6e2",
          "image": "redislabs/operator@sha256:fcdcfbf38860a41a5c4a635cf74d8f421fe48b4180b51c511e5eb3f309c7f6e2",
          "name": "operator-fcdcfbf38860a41a5c4a635cf74d8f421fe48b4180b51c511e5eb3f309c7f6e2-annotation"
        },
        {
          "digest": "sha256:fcdcfbf38860a41a5c4a635cf74d8f421fe48b4180b51c511e5eb3f309c7f6e2",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:fcdcfbf38860a41a5c4a635cf74d8f421fe48b4180b51c511e5eb3f309c7f6e2",
          "name": "redis-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "6.0.6-11",
      "version_original": "6.0.6-11"
    },
    {
      "_id": "62fa4cf9e07d71e4beb6b636",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "redislabs/redis",
              "versionTag": "5.4.14-31.rhel7-openshift"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "size": 3,
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:16da0c69f0e2585be8e44abe6e269dd35176771f5a1ecda3111c8a2f52c790a0",
      "bundle_path_digest": "sha256:16da0c69f0e2585be8e44abe6e269dd35176771f5a1ecda3111c8a2f52c790a0",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.10",
      "creation_date": "2022-08-15T13:41:13.116000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis Labs. Redis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data tructures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-inclass exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n* For service broker - a k8s distribution that supports service catalog (see also: service-catalog)\n\n#### Recommendations/known issues\n\nChanging the name of the Redis Enterprise Cluster in the spec from the\ndefault requires running a command to grant permissions to the pods:\n\n\n> *oc adm policy add-scc-to-group redis-enterprise-scc\nsystem:serviceaccounts:my-project*\n\n\nReplace my-project withh the actual name of the project/namespace.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v5.4.14-7",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-08-15T13:41:13.116000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis Labs",
      "related_images": [
        {
          "digest": "sha256:062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217",
          "image": "redislabs/operator@sha256:062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217",
          "name": "operator-062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217-annotation"
        },
        {
          "digest": "sha256:062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217",
          "name": "redis-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "5.4.14-7",
      "version_original": "5.4.14-7"
    },
    {
      "_id": "62fa4d25bb3b9bafa134b10b",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "redislabs/redis",
              "versionTag": "6.2.4-55.rhel7-openshift"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:c50c8d2b5c62f60b96314c3fe64cf073727ab699b85992443f96b8743269b8a4",
      "bundle_path_digest": "sha256:c50c8d2b5c62f60b96314c3fe64cf073727ab699b85992443f96b8743269b8a4",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.4",
      "creation_date": "2022-08-15T13:41:57.334000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis Labs. Redis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n\nChanging the name of the Redis Enterprise Cluster in the spec from the\ndefault requires running a command to grant permissions to the pods:\n\n\n> *oc adm policy add-scc-to-group redis-enterprise-scc\nsystem:serviceaccounts:my-project*\n\n\nReplace my-project with the actual name of the project/namespace.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.4-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-08-15T13:41:57.334000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis Labs",
      "related_images": [
        {
          "digest": "sha256:8e536825e5062bdda268ed1f0a88824b863ff7945a2f97b7ffc29422569c4bb9",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:8e536825e5062bdda268ed1f0a88824b863ff7945a2f97b7ffc29422569c4bb9",
          "name": "redis-enterprise-operator-8e536825e5062bdda268ed1f0a88824b863ff7945a2f97b7ffc29422569c4bb9-annotation"
        },
        {
          "digest": "sha256:8e536825e5062bdda268ed1f0a88824b863ff7945a2f97b7ffc29422569c4bb9",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:8e536825e5062bdda268ed1f0a88824b863ff7945a2f97b7ffc29422569c4bb9",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:8e536825e5062bdda268ed1f0a88824b863ff7945a2f97b7ffc29422569c4bb9",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:8e536825e5062bdda268ed1f0a88824b863ff7945a2f97b7ffc29422569c4bb9",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "6.2.4-1",
      "version_original": "6.2.4-1"
    },
    {
      "_id": "62fa4d2730a80ba5a208d2bf",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "redislabs/redis",
              "versionTag": "5.4.14-31.rhel7-openshift"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "size": 3,
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:16da0c69f0e2585be8e44abe6e269dd35176771f5a1ecda3111c8a2f52c790a0",
      "bundle_path_digest": "sha256:16da0c69f0e2585be8e44abe6e269dd35176771f5a1ecda3111c8a2f52c790a0",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.8",
      "creation_date": "2022-08-15T13:41:59.161000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis Labs. Redis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data tructures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-inclass exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n* For service broker - a k8s distribution that supports service catalog (see also: service-catalog)\n\n#### Recommendations/known issues\n\nChanging the name of the Redis Enterprise Cluster in the spec from the\ndefault requires running a command to grant permissions to the pods:\n\n\n> *oc adm policy add-scc-to-group redis-enterprise-scc\nsystem:serviceaccounts:my-project*\n\n\nReplace my-project withh the actual name of the project/namespace.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v5.4.14-7",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-08-15T13:41:59.161000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis Labs",
      "related_images": [
        {
          "digest": "sha256:062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217",
          "image": "redislabs/operator@sha256:062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217",
          "name": "operator-062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217-annotation"
        },
        {
          "digest": "sha256:062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:062b478e4d49e263a99a87140a56919d204a00583113fcc2e9da00fe4dfb8217",
          "name": "redis-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "5.4.14-7",
      "version_original": "5.4.14-7"
    },
    {
      "_id": "62fa4d404eed1a4804130587",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "bootstrapperImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise-operator"
            },
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "registry.connect.redhat.com/redislabs/redis-enterprise",
              "versionTag": "6.2.10-90.rhel7-openshift"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "redisEnterpriseServicesRiggerImageSpec": {
              "repository": "registry.connect.redhat.com/redislabs/services-manager"
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        },
        {
          "api_version": "app.redislabs.com/v1alpha1",
          "kind": "RedisEnterpriseDatabase",
          "metadata": {
            "name": "redb"
          },
          "spec": {
            "redisEnterpriseCluster": {
              "name": "rec"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:95644b2ccc560118c16350bcd4380db7c08d97258bf1034387e96a9b1814fef5",
      "bundle_path_digest": "sha256:95644b2ccc560118c16350bcd4380db7c08d97258bf1034387e96a9b1814fef5",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.10",
      "creation_date": "2022-08-15T13:42:24.114000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis.\nRedis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data structures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-in-class exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\nChanging the name of the Redis Enterprise Cluster in the spec from the\ndefault requires running commands to grant permissions to the pods:\n\nOperator service account:\n> *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:redis-enterprise-operator*<br>\n\nRedis Enterprise Cluster service account:\n> *oc adm policy add-scc-to-user redis-enterprise-scc system:serviceaccount:my-project:<name of Redis Enterprise Cluster>*\n\nReplace my-project with the actual name of the project/namespace. Replace <name of Redis Enterprise Cluster> with the name of the Redis Enterprise Cluster.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.2.10-4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-08-15T13:42:24.114000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseDatabase",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis",
      "related_images": [
        {
          "digest": "sha256:97ffbde86f27810b1a5e6fee7ec53683f49b650cc33c327696be66d04e10bf31",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:97ffbde86f27810b1a5e6fee7ec53683f49b650cc33c327696be66d04e10bf31",
          "name": "redis-enterprise-operator-97ffbde86f27810b1a5e6fee7ec53683f49b650cc33c327696be66d04e10bf31-annotation"
        },
        {
          "digest": "sha256:97ffbde86f27810b1a5e6fee7ec53683f49b650cc33c327696be66d04e10bf31",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:97ffbde86f27810b1a5e6fee7ec53683f49b650cc33c327696be66d04e10bf31",
          "name": "redis-enterprise-operator"
        },
        {
          "digest": "sha256:97ffbde86f27810b1a5e6fee7ec53683f49b650cc33c327696be66d04e10bf31",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:97ffbde86f27810b1a5e6fee7ec53683f49b650cc33c327696be66d04e10bf31",
          "name": "admission"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "6.2.10-4",
      "version_original": "6.2.10-4"
    },
    {
      "_id": "62fa4d56e07d71e4beb6b70f",
      "alm_examples": [
        {
          "api_version": "app.redislabs.com/v1",
          "kind": "RedisEnterpriseCluster",
          "metadata": {
            "name": "rec"
          },
          "spec": {
            "nodes": 3,
            "persistentSpec": {
              "enabled": true
            },
            "redisEnterpriseImageSpec": {
              "imagePullPolicy": "IfNotPresent",
              "repository": "redislabs/redis",
              "versionTag": "6.0.8-30.rhel7-openshift"
            },
            "redisEnterpriseNodeResources": {
              "limits": {
                "cpu": "4000m",
                "memory": "4Gi"
              },
              "requests": {
                "cpu": "4000m",
                "memory": "4Gi"
              }
            },
            "uiServiceType": "ClusterIP",
            "username": "demo@redislabs.com"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/redislabs/redis-enterprise-operator-bundle@sha256:889042a1600da27e338f688044802838bb1b8126e9fd900ff0fbccfdf29b75ec",
      "bundle_path_digest": "sha256:889042a1600da27e338f688044802838bb1b8126e9fd900ff0fbccfdf29b75ec",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "6.2.8",
      "creation_date": "2022-08-15T13:42:46.524000+00:00",
      "csv_description": "Redis Enterprise Software is enterprise grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis by Redis Labs. Redis Enterprise Software extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability, with significant operational savings.\nRedis Enterprise provides a flexible and cost-effective data platform so developers can focus on rapid, high-quality development for sophisticated use cases that meet the needs of a modern digital enterprise. With Redis Enterprise, you can:\n* Enjoy high performance and record-setting low latencies with minimal operational overhead\n* Develop highly scalable microservices-based and Kubernetes-orchestrated applications\n* Use versatile data structures for a variety of use cases, such as high-speed transactions, user session management, real-time analytics, caching and many more\n* Leverage enterprise-grade operational controls for high availability, security and seamless scale\n* Automate operational best practices for deploying and managing the Redis Enterprise data platform with built-in Kubernetes Operator support\n* Deploy, manage and move applications to and from any cloud for seamless portability\n\n### Core capabilities\n* **Speed application development and time to market** - Unburden developers from the complexity of infrastructure operations and empower them to manage data with maximum flexibility. This helps them build modern applications quickly, make changes rapidly and support a variety of different data tructures, models, relationships and use cases.\n* **Ensure business continuity with always-on data service** - Maintain service uptime and ensure failsafe high availability, instant failover and automatic recovery to protect your applications against unplanned downtime, outages and data loss.\n* **Design for performance and unmatched user experience** - Deliver the highest level of responsiveness and interactive customer experiences with best-inclass exceptional linear scaling high performance through a shared-nothing architecture and in-memory processing.\n* **Simplify operations with automated lifecycle management and layered orchestration** - Automate database provisioning, management and maintenance, and abstract away the complexities of high availability, seamless scale and zero-downtime upgrades with built-in support for Kubernetes and Operators.\n* **Enjoy multi-level robust security** - Provide granular control to meet self-imposed and regulatory compliance regulations and standards with built-in finegrained security controls and data encryption.\n* **Gain platform independence with flexible deployment options** - Ensure portability with multiple deployment options across any cloud of choice for multicloud and hybrid deployments and on-premises.\n* **Ensure success** - Leverage 24x7 enterprise-grade support backed by expertise in managing and scaling hundreds of thousands of Redis databases for thousands of enterprise customers worldwide.\n* **Future proof investments** - Use the most popular database amongst developers, built on open source innovation and entrenched firmly in the application landscape with over 1B downloads on Docker. Redis preserves your investment for years with easy extensibility and versatility through its Modules and support for over 50 different programming languages\n\n#### Prerequisites\nA minimum of 3 nodes which support the following requirements:\n* RAM - At least 3GB with 4GB recommended\n* Persistent Storage - At least 10GB of free space with 20GB recommended.\n* A kubernetes version of 1.9 or higher\n\n#### Recommendations/known issues\n\nChanging the name of the Redis Enterprise Cluster in the spec from the\ndefault requires running a command to grant permissions to the pods:\n\n\n> *oc adm policy add-scc-to-group redis-enterprise-scc\nsystem:serviceaccounts:my-project*\n\n\nReplace my-project withh the actual name of the project/namespace.",
      "csv_display_name": "Redis Enterprise Operator",
      "csv_metadata_description": "",
      "csv_name": "redis-enterprise-operator.v6.0.8-20",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-08-15T13:42:46.524000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "redis-enterprise-operator-cert",
      "provided_apis": [
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1"
        },
        {
          "group": "app.redislabs.com",
          "kind": "RedisEnterpriseCluster",
          "version": "v1alpha1"
        }
      ],
      "provider": "Redis Labs",
      "related_images": [
        {
          "digest": "sha256:0b131c1b337c2d0a8d25cb137b0a039fb9c1f30806e90bd83441688aec2998b1",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:0b131c1b337c2d0a8d25cb137b0a039fb9c1f30806e90bd83441688aec2998b1",
          "name": "redis-enterprise-operator-0b131c1b337c2d0a8d25cb137b0a039fb9c1f30806e90bd83441688aec2998b1-annotation"
        },
        {
          "digest": "sha256:0b131c1b337c2d0a8d25cb137b0a039fb9c1f30806e90bd83441688aec2998b1",
          "image": "registry.connect.redhat.com/redislabs/redis-enterprise-operator@sha256:0b131c1b337c2d0a8d25cb137b0a039fb9c1f30806e90bd83441688aec2998b1",
          "name": "redis-enterprise-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "6.0.8-20",
      "version_original": "6.0.8-20"
    },
    {
      "_id": "62fa8591bb3b9bafa134ec77",
      "alm_examples": [
        {
          "api_version": "containers.kove.net/v1",
          "kind": "KoveMemoryConfig",
          "metadata": {
            "labels": {
              "KoveMemoryConfig": ""
            },
            "name": "example-memory-config"
          },
          "spec": {
            "container": "",
            "namespace": "",
            "size": ""
          }
        },
        {
          "api_version": "containers.kove.net/v1",
          "kind": "KoveSystem",
          "metadata": {
            "labels": {
              "KoveSystem": ""
            },
            "name": "kove-system"
          },
          "spec": {
            "autogrowInitialIncrement": "1 GiB",
            "autogrowMaxIncrement": "1 TiB",
            "l4MaxMemoryPercent": 90,
            "managementConsoleAddress": "",
            "memoryPlusLocalMemoryMaxSize": "0 MiB"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/kove/kove-operator@sha256:20848f4ec2476dce1bf133805fd6d2f14c605225f6f55ca8a688e753f90434c7",
      "bundle_path_digest": "sha256:20848f4ec2476dce1bf133805fd6d2f14c605225f6f55ca8a688e753f90434c7",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-15T17:42:41.572000+00:00",
      "csv_description": "## About the Operator\nThe Kove External Memory operator integrates Kove's software-defined memory\nsolution with Kubernetes and OpenShift clusters. The operator enables\ncontainers to transparently provision any amount of memory, enabling total\ncontainer memory capacity to exceed physical hardware limitations.\n\nUse the Kove External Memory operator to:\n\n* Create custom resources to automatically allocate memory to specific\n  containers\n* Increase container flexibility with dynamic memory capacity that\n  auto-grows as needed\n* Scale up container density by removing memory as an orchestration\n  bottleneck\n\n## Using the Operator\nUsing the operator is as simple as creating a custom resource which\nspecifies an initial Kove memory allocation size, a namespace, and a\ncontainer name. Containers in the specified namespace with a matching\ncontainer name will automatically be provided with a Kove memory allocation\nwhich will auto-grow as needed.\n\n## About Kove\nKove makes memory a provisionable resource, just like storage, CPUs, and\nnetworking. Kove External Memory provides a transparent, linearly scalable\nmemory resource accessible to any application without code changes, from\nbare-metal apps to virtual machines and containers.\n",
      "csv_display_name": "Kove External Memory Operator",
      "csv_metadata_description": "Configure containers with Kove memory",
      "csv_name": "kove-operator.v1.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-22T03:09:54.971000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "kove-operator",
      "provided_apis": [
        {
          "group": "containers.kove.net",
          "kind": "KoveMemoryConfig",
          "plural": "kovememoryconfigs",
          "version": "v1"
        },
        {
          "group": "containers.kove.net",
          "kind": "KoveSystem",
          "plural": "kovesystems",
          "version": "v1"
        }
      ],
      "provider": "Kove",
      "related_images": [
        {
          "digest": "sha256:5ecc7d5b87a8e26725f09994b875ddb7409bb8a0a2f521f5f53ca45b9ead3a8a",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:5ecc7d5b87a8e26725f09994b875ddb7409bb8a0a2f521f5f53ca45b9ead3a8a",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "image": "quay.io/kove/kove-operator@sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "name": "manager"
        },
        {
          "digest": "sha256:9a7b0fab9639bb447923314a708ea43689455481a478aea9f2ff4c66529f02bc",
          "image": "quay.io/kove/kove-agent@sha256:9a7b0fab9639bb447923314a708ea43689455481a478aea9f2ff4c66529f02bc",
          "name": "kove_agent"
        },
        {
          "digest": "sha256:e43602c50bb3199718bf07e69c223ef76c2b7b5a0f220d240b7fe6d3402aef69",
          "image": "quay.io/kove/kove-client-drivers@sha256:e43602c50bb3199718bf07e69c223ef76c2b7b5a0f220d240b7fe6d3402aef69",
          "name": "kove_client_drivers_8_3"
        },
        {
          "digest": "sha256:8b4799e78ff5fb320b8b3ce47a0dfb39c916a2b515ebd01965dbbee116f755be",
          "image": "quay.io/kove/kove-client-drivers@sha256:8b4799e78ff5fb320b8b3ce47a0dfb39c916a2b515ebd01965dbbee116f755be",
          "name": "kove_client_drivers_8_4"
        },
        {
          "digest": "sha256:539c427e7f899a212313516c15e23794e850e98e47ce7b63b8aceaf7948bd30b",
          "image": "quay.io/kove/kove-container-hook@sha256:539c427e7f899a212313516c15e23794e850e98e47ce7b63b8aceaf7948bd30b",
          "name": "kove_container_hook"
        },
        {
          "digest": "sha256:06b9e43db37014ecda17a8bfe23918d5af031fe7265e8e49136ec538e2f98c6b",
          "image": "quay.io/kove/kove-xmem@sha256:06b9e43db37014ecda17a8bfe23918d5af031fe7265e8e49136ec538e2f98c6b",
          "name": "kove_xmem"
        },
        {
          "digest": "sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "image": "quay.io/kove/kove-operator@sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "name": "kove-operator-a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.2.0",
      "version_original": "1.2.0"
    },
    {
      "_id": "62fa861bbb3b9bafa134ec7f",
      "alm_examples": [
        {
          "api_version": "containers.kove.net/v1",
          "kind": "KoveMemoryConfig",
          "metadata": {
            "labels": {
              "KoveMemoryConfig": ""
            },
            "name": "example-memory-config"
          },
          "spec": {
            "container": "",
            "namespace": "",
            "size": ""
          }
        },
        {
          "api_version": "containers.kove.net/v1",
          "kind": "KoveSystem",
          "metadata": {
            "labels": {
              "KoveSystem": ""
            },
            "name": "kove-system"
          },
          "spec": {
            "autogrowInitialIncrement": "1 GiB",
            "autogrowMaxIncrement": "1 TiB",
            "l4MaxMemoryPercent": 90,
            "managementConsoleAddress": "",
            "memoryPlusLocalMemoryMaxSize": "0 MiB"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/kove/kove-operator@sha256:20848f4ec2476dce1bf133805fd6d2f14c605225f6f55ca8a688e753f90434c7",
      "bundle_path_digest": "sha256:20848f4ec2476dce1bf133805fd6d2f14c605225f6f55ca8a688e753f90434c7",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-15T17:44:59.283000+00:00",
      "csv_description": "## About the Operator\nThe Kove External Memory operator integrates Kove's software-defined memory\nsolution with Kubernetes and OpenShift clusters. The operator enables\ncontainers to transparently provision any amount of memory, enabling total\ncontainer memory capacity to exceed physical hardware limitations.\n\nUse the Kove External Memory operator to:\n\n* Create custom resources to automatically allocate memory to specific\n  containers\n* Increase container flexibility with dynamic memory capacity that\n  auto-grows as needed\n* Scale up container density by removing memory as an orchestration\n  bottleneck\n\n## Using the Operator\nUsing the operator is as simple as creating a custom resource which\nspecifies an initial Kove memory allocation size, a namespace, and a\ncontainer name. Containers in the specified namespace with a matching\ncontainer name will automatically be provided with a Kove memory allocation\nwhich will auto-grow as needed.\n\n## About Kove\nKove makes memory a provisionable resource, just like storage, CPUs, and\nnetworking. Kove External Memory provides a transparent, linearly scalable\nmemory resource accessible to any application without code changes, from\nbare-metal apps to virtual machines and containers.\n",
      "csv_display_name": "Kove External Memory Operator",
      "csv_metadata_description": "Configure containers with Kove memory",
      "csv_name": "kove-operator.v1.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-22T03:09:12.357000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "kove-operator",
      "provided_apis": [
        {
          "group": "containers.kove.net",
          "kind": "KoveMemoryConfig",
          "plural": "kovememoryconfigs",
          "version": "v1"
        },
        {
          "group": "containers.kove.net",
          "kind": "KoveSystem",
          "plural": "kovesystems",
          "version": "v1"
        }
      ],
      "provider": "Kove",
      "related_images": [
        {
          "digest": "sha256:5ecc7d5b87a8e26725f09994b875ddb7409bb8a0a2f521f5f53ca45b9ead3a8a",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:5ecc7d5b87a8e26725f09994b875ddb7409bb8a0a2f521f5f53ca45b9ead3a8a",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "image": "quay.io/kove/kove-operator@sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "name": "manager"
        },
        {
          "digest": "sha256:9a7b0fab9639bb447923314a708ea43689455481a478aea9f2ff4c66529f02bc",
          "image": "quay.io/kove/kove-agent@sha256:9a7b0fab9639bb447923314a708ea43689455481a478aea9f2ff4c66529f02bc",
          "name": "kove_agent"
        },
        {
          "digest": "sha256:e43602c50bb3199718bf07e69c223ef76c2b7b5a0f220d240b7fe6d3402aef69",
          "image": "quay.io/kove/kove-client-drivers@sha256:e43602c50bb3199718bf07e69c223ef76c2b7b5a0f220d240b7fe6d3402aef69",
          "name": "kove_client_drivers_8_3"
        },
        {
          "digest": "sha256:8b4799e78ff5fb320b8b3ce47a0dfb39c916a2b515ebd01965dbbee116f755be",
          "image": "quay.io/kove/kove-client-drivers@sha256:8b4799e78ff5fb320b8b3ce47a0dfb39c916a2b515ebd01965dbbee116f755be",
          "name": "kove_client_drivers_8_4"
        },
        {
          "digest": "sha256:539c427e7f899a212313516c15e23794e850e98e47ce7b63b8aceaf7948bd30b",
          "image": "quay.io/kove/kove-container-hook@sha256:539c427e7f899a212313516c15e23794e850e98e47ce7b63b8aceaf7948bd30b",
          "name": "kove_container_hook"
        },
        {
          "digest": "sha256:06b9e43db37014ecda17a8bfe23918d5af031fe7265e8e49136ec538e2f98c6b",
          "image": "quay.io/kove/kove-xmem@sha256:06b9e43db37014ecda17a8bfe23918d5af031fe7265e8e49136ec538e2f98c6b",
          "name": "kove_xmem"
        },
        {
          "digest": "sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "image": "quay.io/kove/kove-operator@sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "name": "kove-operator-a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.2.0",
      "version_original": "1.2.0"
    },
    {
      "_id": "62fa899a30a80ba5a2090e2e",
      "alm_examples": [
        {
          "api_version": "containers.kove.net/v1",
          "kind": "KoveMemoryConfig",
          "metadata": {
            "labels": {
              "KoveMemoryConfig": ""
            },
            "name": "example-memory-config"
          },
          "spec": {
            "container": "",
            "namespace": "",
            "size": ""
          }
        },
        {
          "api_version": "containers.kove.net/v1",
          "kind": "KoveSystem",
          "metadata": {
            "labels": {
              "KoveSystem": ""
            },
            "name": "kove-system"
          },
          "spec": {
            "autogrowInitialIncrement": "1 GiB",
            "autogrowMaxIncrement": "1 TiB",
            "l4MaxMemoryPercent": 90,
            "managementConsoleAddress": "",
            "memoryPlusLocalMemoryMaxSize": "0 MiB"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/kove/kove-operator@sha256:20848f4ec2476dce1bf133805fd6d2f14c605225f6f55ca8a688e753f90434c7",
      "bundle_path_digest": "sha256:20848f4ec2476dce1bf133805fd6d2f14c605225f6f55ca8a688e753f90434c7",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-15T17:59:54.728000+00:00",
      "csv_description": "## About the Operator\nThe Kove External Memory operator integrates Kove's software-defined memory\nsolution with Kubernetes and OpenShift clusters. The operator enables\ncontainers to transparently provision any amount of memory, enabling total\ncontainer memory capacity to exceed physical hardware limitations.\n\nUse the Kove External Memory operator to:\n\n* Create custom resources to automatically allocate memory to specific\n  containers\n* Increase container flexibility with dynamic memory capacity that\n  auto-grows as needed\n* Scale up container density by removing memory as an orchestration\n  bottleneck\n\n## Using the Operator\nUsing the operator is as simple as creating a custom resource which\nspecifies an initial Kove memory allocation size, a namespace, and a\ncontainer name. Containers in the specified namespace with a matching\ncontainer name will automatically be provided with a Kove memory allocation\nwhich will auto-grow as needed.\n\n## About Kove\nKove makes memory a provisionable resource, just like storage, CPUs, and\nnetworking. Kove External Memory provides a transparent, linearly scalable\nmemory resource accessible to any application without code changes, from\nbare-metal apps to virtual machines and containers.\n",
      "csv_display_name": "Kove External Memory Operator",
      "csv_metadata_description": "Configure containers with Kove memory",
      "csv_name": "kove-operator.v1.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-22T03:13:23.329000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "kove-operator",
      "provided_apis": [
        {
          "group": "containers.kove.net",
          "kind": "KoveMemoryConfig",
          "plural": "kovememoryconfigs",
          "version": "v1"
        },
        {
          "group": "containers.kove.net",
          "kind": "KoveSystem",
          "plural": "kovesystems",
          "version": "v1"
        }
      ],
      "provider": "Kove",
      "related_images": [
        {
          "digest": "sha256:5ecc7d5b87a8e26725f09994b875ddb7409bb8a0a2f521f5f53ca45b9ead3a8a",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:5ecc7d5b87a8e26725f09994b875ddb7409bb8a0a2f521f5f53ca45b9ead3a8a",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "image": "quay.io/kove/kove-operator@sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "name": "manager"
        },
        {
          "digest": "sha256:9a7b0fab9639bb447923314a708ea43689455481a478aea9f2ff4c66529f02bc",
          "image": "quay.io/kove/kove-agent@sha256:9a7b0fab9639bb447923314a708ea43689455481a478aea9f2ff4c66529f02bc",
          "name": "kove_agent"
        },
        {
          "digest": "sha256:e43602c50bb3199718bf07e69c223ef76c2b7b5a0f220d240b7fe6d3402aef69",
          "image": "quay.io/kove/kove-client-drivers@sha256:e43602c50bb3199718bf07e69c223ef76c2b7b5a0f220d240b7fe6d3402aef69",
          "name": "kove_client_drivers_8_3"
        },
        {
          "digest": "sha256:8b4799e78ff5fb320b8b3ce47a0dfb39c916a2b515ebd01965dbbee116f755be",
          "image": "quay.io/kove/kove-client-drivers@sha256:8b4799e78ff5fb320b8b3ce47a0dfb39c916a2b515ebd01965dbbee116f755be",
          "name": "kove_client_drivers_8_4"
        },
        {
          "digest": "sha256:539c427e7f899a212313516c15e23794e850e98e47ce7b63b8aceaf7948bd30b",
          "image": "quay.io/kove/kove-container-hook@sha256:539c427e7f899a212313516c15e23794e850e98e47ce7b63b8aceaf7948bd30b",
          "name": "kove_container_hook"
        },
        {
          "digest": "sha256:06b9e43db37014ecda17a8bfe23918d5af031fe7265e8e49136ec538e2f98c6b",
          "image": "quay.io/kove/kove-xmem@sha256:06b9e43db37014ecda17a8bfe23918d5af031fe7265e8e49136ec538e2f98c6b",
          "name": "kove_xmem"
        },
        {
          "digest": "sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "image": "quay.io/kove/kove-operator@sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "name": "kove-operator-a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.2.0",
      "version_original": "1.2.0"
    },
    {
      "_id": "62fa8ac64eed1a48041340ed",
      "alm_examples": [
        {
          "api_version": "containers.kove.net/v1",
          "kind": "KoveMemoryConfig",
          "metadata": {
            "labels": {
              "KoveMemoryConfig": ""
            },
            "name": "example-memory-config"
          },
          "spec": {
            "container": "",
            "namespace": "",
            "size": ""
          }
        },
        {
          "api_version": "containers.kove.net/v1",
          "kind": "KoveSystem",
          "metadata": {
            "labels": {
              "KoveSystem": ""
            },
            "name": "kove-system"
          },
          "spec": {
            "autogrowInitialIncrement": "1 GiB",
            "autogrowMaxIncrement": "1 TiB",
            "l4MaxMemoryPercent": 90,
            "managementConsoleAddress": "",
            "memoryPlusLocalMemoryMaxSize": "0 MiB"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [
        "amd64"
      ],
      "bundle_path": "registry.connect.redhat.com/kove/kove-operator@sha256:20848f4ec2476dce1bf133805fd6d2f14c605225f6f55ca8a688e753f90434c7",
      "bundle_path_digest": "sha256:20848f4ec2476dce1bf133805fd6d2f14c605225f6f55ca8a688e753f90434c7",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-15T18:04:54.239000+00:00",
      "csv_description": "## About the Operator\nThe Kove External Memory operator integrates Kove's software-defined memory\nsolution with Kubernetes and OpenShift clusters. The operator enables\ncontainers to transparently provision any amount of memory, enabling total\ncontainer memory capacity to exceed physical hardware limitations.\n\nUse the Kove External Memory operator to:\n\n* Create custom resources to automatically allocate memory to specific\n  containers\n* Increase container flexibility with dynamic memory capacity that\n  auto-grows as needed\n* Scale up container density by removing memory as an orchestration\n  bottleneck\n\n## Using the Operator\nUsing the operator is as simple as creating a custom resource which\nspecifies an initial Kove memory allocation size, a namespace, and a\ncontainer name. Containers in the specified namespace with a matching\ncontainer name will automatically be provided with a Kove memory allocation\nwhich will auto-grow as needed.\n\n## About Kove\nKove makes memory a provisionable resource, just like storage, CPUs, and\nnetworking. Kove External Memory provides a transparent, linearly scalable\nmemory resource accessible to any application without code changes, from\nbare-metal apps to virtual machines and containers.\n",
      "csv_display_name": "Kove External Memory Operator",
      "csv_metadata_description": "Configure containers with Kove memory",
      "csv_name": "kove-operator.v1.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-22T03:16:43.698000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "kove-operator",
      "provided_apis": [
        {
          "group": "containers.kove.net",
          "kind": "KoveMemoryConfig",
          "plural": "kovememoryconfigs",
          "version": "v1"
        },
        {
          "group": "containers.kove.net",
          "kind": "KoveSystem",
          "plural": "kovesystems",
          "version": "v1"
        }
      ],
      "provider": "Kove",
      "related_images": [
        {
          "digest": "sha256:5ecc7d5b87a8e26725f09994b875ddb7409bb8a0a2f521f5f53ca45b9ead3a8a",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:5ecc7d5b87a8e26725f09994b875ddb7409bb8a0a2f521f5f53ca45b9ead3a8a",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "image": "quay.io/kove/kove-operator@sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "name": "manager"
        },
        {
          "digest": "sha256:9a7b0fab9639bb447923314a708ea43689455481a478aea9f2ff4c66529f02bc",
          "image": "quay.io/kove/kove-agent@sha256:9a7b0fab9639bb447923314a708ea43689455481a478aea9f2ff4c66529f02bc",
          "name": "kove_agent"
        },
        {
          "digest": "sha256:e43602c50bb3199718bf07e69c223ef76c2b7b5a0f220d240b7fe6d3402aef69",
          "image": "quay.io/kove/kove-client-drivers@sha256:e43602c50bb3199718bf07e69c223ef76c2b7b5a0f220d240b7fe6d3402aef69",
          "name": "kove_client_drivers_8_3"
        },
        {
          "digest": "sha256:8b4799e78ff5fb320b8b3ce47a0dfb39c916a2b515ebd01965dbbee116f755be",
          "image": "quay.io/kove/kove-client-drivers@sha256:8b4799e78ff5fb320b8b3ce47a0dfb39c916a2b515ebd01965dbbee116f755be",
          "name": "kove_client_drivers_8_4"
        },
        {
          "digest": "sha256:539c427e7f899a212313516c15e23794e850e98e47ce7b63b8aceaf7948bd30b",
          "image": "quay.io/kove/kove-container-hook@sha256:539c427e7f899a212313516c15e23794e850e98e47ce7b63b8aceaf7948bd30b",
          "name": "kove_container_hook"
        },
        {
          "digest": "sha256:06b9e43db37014ecda17a8bfe23918d5af031fe7265e8e49136ec538e2f98c6b",
          "image": "quay.io/kove/kove-xmem@sha256:06b9e43db37014ecda17a8bfe23918d5af031fe7265e8e49136ec538e2f98c6b",
          "name": "kove_xmem"
        },
        {
          "digest": "sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "image": "quay.io/kove/kove-operator@sha256:a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a",
          "name": "kove-operator-a3dde7208e05673dcd766dd9a0beeca1b5ebf7ec09822bf5e18ce9c6e778550a-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.2.0",
      "version_original": "1.2.0"
    },
    {
      "_id": "62fb9482b289bcc7af678910",
      "alm_examples": [
        {
          "api_version": "pg.percona.com/v1",
          "kind": "PerconaPGCluster",
          "metadata": {
            "labels": {
              "pgo-version": "1.3.0"
            },
            "name": "cluster1"
          },
          "spec": {
            "backup": {
              "backrestRepoImage": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
              "resources": {
                "requests": {
                  "cpu": "200m",
                  "memory": "48Mi"
                }
              },
              "schedule": [
                {
                  "keep": 3,
                  "name": "sat-night-backup",
                  "schedule": "0 0 * * 6",
                  "storage": "local",
                  "type": "full"
                }
              ],
              "volumeSpec": {
                "accessmode": "ReadWriteOnce",
                "size": "1G",
                "storageclass": "",
                "storagetype": "dynamic"
              }
            },
            "database": "pgdb",
            "disableAutofail": false,
            "keepBackups": true,
            "keepData": true,
            "pause": false,
            "pgBadger": {
              "enabled": false,
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
              "port": 10000
            },
            "pgBouncer": {
              "expose": {
                "serviceType": "ClusterIP"
              },
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
              "resources": {
                "requests": {
                  "cpu": "1",
                  "memory": "128Mi"
                }
              },
              "size": 3
            },
            "pgPrimary": {
              "expose": {
                "serviceType": "ClusterIP"
              },
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
              "resources": {
                "requests": {
                  "cpu": "500m",
                  "memory": "256Mi"
                }
              },
              "tolerations": [],
              "volumeSpec": {
                "accessmode": "ReadWriteOnce",
                "size": "1G",
                "storageclass": "",
                "storagetype": "dynamic"
              }
            },
            "pgReplicas": {
              "hotStandby": {
                "enableSyncStandby": false,
                "expose": {
                  "serviceType": "ClusterIP"
                },
                "resources": {
                  "requests": {
                    "cpu": "500m",
                    "memory": "256Mi"
                  }
                },
                "size": 2,
                "volumeSpec": {
                  "accessmode": "ReadWriteOnce",
                  "size": "1G",
                  "storageclass": "",
                  "storagetype": "dynamic"
                }
              }
            },
            "pmm": {
              "enabled": false,
              "image": "percona/pmm-client:2.29.1",
              "pmmSecret": "cluster1-pmm-secret",
              "resources": {
                "requests": {
                  "cpu": "500m",
                  "memory": "200M"
                }
              },
              "serverHost": "monitoring-service",
              "serverUser": "admin"
            },
            "port": "5432",
            "standby": false,
            "tlsOnly": false,
            "upgradeOptions": {
              "apply": "disabled",
              "schedule": "0 4 * * *",
              "versionServiceEndpoint": "https://check.percona.com"
            },
            "user": "pguser"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/percona/percona-postgresql-operator-bundle@sha256:7995df96b3a7a033321d46097387922a70c011746082d0967fa9cfe8ce75a43d",
      "bundle_path_digest": "sha256:7995df96b3a7a033321d46097387922a70c011746082d0967fa9cfe8ce75a43d",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T12:58:42.680000+00:00",
      "csv_description": "\n## Percona is Cloud Native\n\nPercona Operator for PostgreSQL automates and simplifies deploying and managing open source PostgreSQL clusters on Kubernetes. Percona Operator for PostgreSQL is based on Postgres Operator developed by Crunchy Data.\nWhether you need to get a simple PostgreSQL cluster up and running, need to deploy a high availability, fault tolerant cluster in production, or are running your own database-as-a-service, the Operator provides the essential features you need to keep your clusters healthy.\n\nConsult the [documentation](https://www.percona.com/doc/kubernetes-operator-for-postgresql/index.html) on the Percona Operator for PostgreSQL for complete details on capabilities and options.\n\n### Supported Features\n\n* **PostgreSQL Cluster Provisioning**\nCreate, Scale, & Delete PostgreSQL clusters with ease, while fully customizing your Pods and PostgreSQL configuration.\n* **High Availability**\n\n    Safe, automated failover backed by a distributed consensus based high-availability solution. Uses Pod Anti-Affinity to help resiliency; you can configure how much\n    enforced this can be. Failed primaries automatically heal, allowing for faster recovery time. Support for standby PostgreSQL clusters that work both within and\n    across multiple Kubernetes clusters.\n\n* **Disaster Recovery**\n\n    Backups and restores leverage the open source pgBackRest utility and includes support for full, incremental, and differential backups as well as efficient delta\n    restores. Set how long you want your backups retained for. Works great with very large databases!\n\n* **Communication Security**\n\n    Secure communication between your applications and data servers by enabling TLS for your PostgreSQL servers, including the ability to enforce all of your connections\n    to use TLS.\n\n* **Monitoring**\n\n    Track the health of your PostgreSQL clusters with Percona Monitoring and Management (PMM).\n\n* **PostgreSQL User Management**\n\n    Quickly add and remove users from your PostgreSQL clusters with powerful commands. Manage password expiration policies or use your preferred PostgreSQL authentication\n    scheme.\n\n* **Updates Management**\n\n    Safely apply PostgreSQL updates with minimal availability impact to your PostgreSQL clusters.\n\n* **Advanced Replication Support**\n\n    Choose between asynchronous replication and synchronous replication for workloads that are sensitive to losing transactions.\n\n* **Clone**\n\n    Create new clusters from your existing clusters or backups.\n\n* **Connection Pooling**\n\n    Use [pgBouncer](https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/pgbouncer/) for connection pooling\n\n* **Affinity and Tolerations**\n\n    Have your PostgreSQL clusters deployed to Kubernetes Nodes of your preference with node affinity, or designate which nodes Kubernetes can schedule PostgreSQL instances\n    to with Kubernetes  tolerations.\n\n* **Scheduled Backups**\n\n    Choose the type of backup (full, incremental, differential) and how frequently you want it to occur on each PostgreSQL cluster.\n\n* **Backup to S3**\n\n    Store your backups in Amazon S3 or any object storage system that supports the S3 protocol. The Operator can backup, restore, and create new clusters from these backups.\n\n* **Multi-Namespace Support**\n\n    You can control how the Operator leverages Kubernetes Namespaces with several different deployment models:\n\n    * Deploy the Operator and all PostgreSQL clusters to the same namespace\n    * Deploy the Operator to one Namespace, and all PostgreSQL clusters to a different Namespace\n    * Deploy the Operator to one Namespace, and have your PostgreSQL clusters managed across multiple Namespaces\n\n* **Full Customizability**\n\n    The Operator not only makes it easy to get PostgreSQL up and running on Kubernetes-enabled platforms, but also allows you to further customize your deployments:\n\n    * Selecting different storage classes for your primary, replica, and backup storage\n    * Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters\n    * Use your own container image repository, including `imagePullSecrets` and private repositories  support\n    * Customize your PostgreSQL configuration\n    * Bring your own trusted certificate authority (CA) for use with the Operator API server\n    * Override your PostgreSQL configuration for each cluster\n    * Use your own custom images, re-define the image for each container separately\n\n### Release highlights\n* **What's new**\n\n    * The automated upgrade is now disabled by default to prevent an unplanned downtimes for user applications and to provide defaults more focused on strict user\u2019s control over the cluster\n    * Flexible anti-affinity configuration is now available, which allows the Operator to isolate PostgreSQL cluster instances on different Kubernetes nodes or to increase its availability by placing\nPostgreSQL instances in different availability zones\n    * Add possibility for postgres user to connect to PostgreSQL through PgBouncer with a new pgBouncer.exposePostgresUser Custom Resource option\n    * A new build and testing guide allows user to easily experiment with the source code of the Operator\n    * Fix the bug in the instruction on passing custom configuration options for PostgreSQL which made it usable for the new cluster only\n    * Fix the bug which caused the Operator crash without pgReplicas section in Custom Resource definition\n    * Fix the bug which caused the Operator to make connection requests to Version Service even with disabled Smart Update\n    * Fix the bug due to which restoring S3 backup from storage with self-signed certificates didn\u2019t work, by introducing the special backup.storages.verifyTLS option to address this issue\n",
      "csv_display_name": "Percona Operator for PostgreSQL",
      "csv_metadata_description": "Percona Operator for PostgreSQL manages the lifecycle of Percona PostgreSQL cluster instances.",
      "csv_name": "percona-postgresql-operator-certified.v1.3.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:56:21.251000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "percona-postgresql-operator-certified",
      "provided_apis": [
        {
          "group": "pg.percona.com",
          "kind": "PerconaPGCluster",
          "plural": "perconapgclusters",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgcluster",
          "plural": "pgclusters",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgpolicy",
          "plural": "pgpolicies",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgreplica",
          "plural": "pgreplicas",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgtask",
          "plural": "pgtasks",
          "version": "v1"
        }
      ],
      "provider": "Percona",
      "related_images": [
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "pgbackrest-repo"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "pgbadger"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "postgres-ha"
        },
        {
          "digest": "sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "name": "rmdata"
        },
        {
          "digest": "sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "name": "percona-postgresql-operator-c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a-annotation"
        },
        {
          "digest": "sha256:362eb3c33eaa99ab39fdf42962a38f2818253e1841aa55f656a16bcb79ac5489",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:362eb3c33eaa99ab39fdf42962a38f2818253e1841aa55f656a16bcb79ac5489",
          "name": "apiserver"
        },
        {
          "digest": "sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "name": "operator"
        },
        {
          "digest": "sha256:5e6a484f7a559e90e79aa7a034bdcf9415848015c08d244b176de17d47deeb12",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:5e6a484f7a559e90e79aa7a034bdcf9415848015c08d244b176de17d47deeb12",
          "name": "scheduler"
        },
        {
          "digest": "sha256:8089fb1d73bf16757a28d94b1c08649556457d5f61cdfdc33bd75cea1147f012",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:8089fb1d73bf16757a28d94b1c08649556457d5f61cdfdc33bd75cea1147f012",
          "name": "event"
        },
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "pgo_backrest"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "pgo_backrest_repo"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "crunchy_pgbadger"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "crunchy_pgbouncer"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "crunchy_postgres_ha"
        },
        {
          "digest": "sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "name": "pgo_rmdata"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "percona-postgresql-operator-containers-b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1-annotation"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "percona-postgresql-operator-containers-0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74-annotation"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "percona-postgresql-operator-containers-0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a-annotation"
        },
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "percona-postgresql-operator-containers-8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee-annotation"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "percona-postgresql-operator-containers-65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.3.0",
      "version_original": "1.3.0"
    },
    {
      "_id": "62fb9784b289bcc7af678a07",
      "alm_examples": [
        {
          "api_version": "pg.percona.com/v1",
          "kind": "PerconaPGCluster",
          "metadata": {
            "labels": {
              "pgo-version": "1.3.0"
            },
            "name": "cluster1"
          },
          "spec": {
            "backup": {
              "backrestRepoImage": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
              "resources": {
                "requests": {
                  "cpu": "200m",
                  "memory": "48Mi"
                }
              },
              "schedule": [
                {
                  "keep": 3,
                  "name": "sat-night-backup",
                  "schedule": "0 0 * * 6",
                  "storage": "local",
                  "type": "full"
                }
              ],
              "volumeSpec": {
                "accessmode": "ReadWriteOnce",
                "size": "1G",
                "storageclass": "",
                "storagetype": "dynamic"
              }
            },
            "database": "pgdb",
            "disableAutofail": false,
            "keepBackups": true,
            "keepData": true,
            "pause": false,
            "pgBadger": {
              "enabled": false,
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
              "port": 10000
            },
            "pgBouncer": {
              "expose": {
                "serviceType": "ClusterIP"
              },
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
              "resources": {
                "requests": {
                  "cpu": "1",
                  "memory": "128Mi"
                }
              },
              "size": 3
            },
            "pgPrimary": {
              "expose": {
                "serviceType": "ClusterIP"
              },
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
              "resources": {
                "requests": {
                  "cpu": "500m",
                  "memory": "256Mi"
                }
              },
              "tolerations": [],
              "volumeSpec": {
                "accessmode": "ReadWriteOnce",
                "size": "1G",
                "storageclass": "",
                "storagetype": "dynamic"
              }
            },
            "pgReplicas": {
              "hotStandby": {
                "enableSyncStandby": false,
                "expose": {
                  "serviceType": "ClusterIP"
                },
                "resources": {
                  "requests": {
                    "cpu": "500m",
                    "memory": "256Mi"
                  }
                },
                "size": 2,
                "volumeSpec": {
                  "accessmode": "ReadWriteOnce",
                  "size": "1G",
                  "storageclass": "",
                  "storagetype": "dynamic"
                }
              }
            },
            "pmm": {
              "enabled": false,
              "image": "percona/pmm-client:2.29.1",
              "pmmSecret": "cluster1-pmm-secret",
              "resources": {
                "requests": {
                  "cpu": "500m",
                  "memory": "200M"
                }
              },
              "serverHost": "monitoring-service",
              "serverUser": "admin"
            },
            "port": "5432",
            "standby": false,
            "tlsOnly": false,
            "upgradeOptions": {
              "apply": "disabled",
              "schedule": "0 4 * * *",
              "versionServiceEndpoint": "https://check.percona.com"
            },
            "user": "pguser"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/percona/percona-postgresql-operator-bundle@sha256:7995df96b3a7a033321d46097387922a70c011746082d0967fa9cfe8ce75a43d",
      "bundle_path_digest": "sha256:7995df96b3a7a033321d46097387922a70c011746082d0967fa9cfe8ce75a43d",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T13:11:32.679000+00:00",
      "csv_description": "\n## Percona is Cloud Native\n\nPercona Operator for PostgreSQL automates and simplifies deploying and managing open source PostgreSQL clusters on Kubernetes. Percona Operator for PostgreSQL is based on Postgres Operator developed by Crunchy Data.\nWhether you need to get a simple PostgreSQL cluster up and running, need to deploy a high availability, fault tolerant cluster in production, or are running your own database-as-a-service, the Operator provides the essential features you need to keep your clusters healthy.\n\nConsult the [documentation](https://www.percona.com/doc/kubernetes-operator-for-postgresql/index.html) on the Percona Operator for PostgreSQL for complete details on capabilities and options.\n\n### Supported Features\n\n* **PostgreSQL Cluster Provisioning**\nCreate, Scale, & Delete PostgreSQL clusters with ease, while fully customizing your Pods and PostgreSQL configuration.\n* **High Availability**\n\n    Safe, automated failover backed by a distributed consensus based high-availability solution. Uses Pod Anti-Affinity to help resiliency; you can configure how much\n    enforced this can be. Failed primaries automatically heal, allowing for faster recovery time. Support for standby PostgreSQL clusters that work both within and\n    across multiple Kubernetes clusters.\n\n* **Disaster Recovery**\n\n    Backups and restores leverage the open source pgBackRest utility and includes support for full, incremental, and differential backups as well as efficient delta\n    restores. Set how long you want your backups retained for. Works great with very large databases!\n\n* **Communication Security**\n\n    Secure communication between your applications and data servers by enabling TLS for your PostgreSQL servers, including the ability to enforce all of your connections\n    to use TLS.\n\n* **Monitoring**\n\n    Track the health of your PostgreSQL clusters with Percona Monitoring and Management (PMM).\n\n* **PostgreSQL User Management**\n\n    Quickly add and remove users from your PostgreSQL clusters with powerful commands. Manage password expiration policies or use your preferred PostgreSQL authentication\n    scheme.\n\n* **Updates Management**\n\n    Safely apply PostgreSQL updates with minimal availability impact to your PostgreSQL clusters.\n\n* **Advanced Replication Support**\n\n    Choose between asynchronous replication and synchronous replication for workloads that are sensitive to losing transactions.\n\n* **Clone**\n\n    Create new clusters from your existing clusters or backups.\n\n* **Connection Pooling**\n\n    Use [pgBouncer](https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/pgbouncer/) for connection pooling\n\n* **Affinity and Tolerations**\n\n    Have your PostgreSQL clusters deployed to Kubernetes Nodes of your preference with node affinity, or designate which nodes Kubernetes can schedule PostgreSQL instances\n    to with Kubernetes  tolerations.\n\n* **Scheduled Backups**\n\n    Choose the type of backup (full, incremental, differential) and how frequently you want it to occur on each PostgreSQL cluster.\n\n* **Backup to S3**\n\n    Store your backups in Amazon S3 or any object storage system that supports the S3 protocol. The Operator can backup, restore, and create new clusters from these backups.\n\n* **Multi-Namespace Support**\n\n    You can control how the Operator leverages Kubernetes Namespaces with several different deployment models:\n\n    * Deploy the Operator and all PostgreSQL clusters to the same namespace\n    * Deploy the Operator to one Namespace, and all PostgreSQL clusters to a different Namespace\n    * Deploy the Operator to one Namespace, and have your PostgreSQL clusters managed across multiple Namespaces\n\n* **Full Customizability**\n\n    The Operator not only makes it easy to get PostgreSQL up and running on Kubernetes-enabled platforms, but also allows you to further customize your deployments:\n\n    * Selecting different storage classes for your primary, replica, and backup storage\n    * Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters\n    * Use your own container image repository, including `imagePullSecrets` and private repositories  support\n    * Customize your PostgreSQL configuration\n    * Bring your own trusted certificate authority (CA) for use with the Operator API server\n    * Override your PostgreSQL configuration for each cluster\n    * Use your own custom images, re-define the image for each container separately\n\n### Release highlights\n* **What's new**\n\n    * The automated upgrade is now disabled by default to prevent an unplanned downtimes for user applications and to provide defaults more focused on strict user\u2019s control over the cluster\n    * Flexible anti-affinity configuration is now available, which allows the Operator to isolate PostgreSQL cluster instances on different Kubernetes nodes or to increase its availability by placing\nPostgreSQL instances in different availability zones\n    * Add possibility for postgres user to connect to PostgreSQL through PgBouncer with a new pgBouncer.exposePostgresUser Custom Resource option\n    * A new build and testing guide allows user to easily experiment with the source code of the Operator\n    * Fix the bug in the instruction on passing custom configuration options for PostgreSQL which made it usable for the new cluster only\n    * Fix the bug which caused the Operator crash without pgReplicas section in Custom Resource definition\n    * Fix the bug which caused the Operator to make connection requests to Version Service even with disabled Smart Update\n    * Fix the bug due to which restoring S3 backup from storage with self-signed certificates didn\u2019t work, by introducing the special backup.storages.verifyTLS option to address this issue\n",
      "csv_display_name": "Percona Operator for PostgreSQL",
      "csv_metadata_description": "Percona Operator for PostgreSQL manages the lifecycle of Percona PostgreSQL cluster instances.",
      "csv_name": "percona-postgresql-operator-certified.v1.3.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:46:59.483000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "percona-postgresql-operator-certified",
      "provided_apis": [
        {
          "group": "pg.percona.com",
          "kind": "Pgcluster",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgpolicy",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgreplica",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgtask",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "PerconaPGCluster",
          "version": "v1"
        }
      ],
      "provider": "Percona",
      "related_images": [
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "pgbackrest-repo"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "pgbadger"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "postgres-ha"
        },
        {
          "digest": "sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "name": "rmdata"
        },
        {
          "digest": "sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "name": "percona-postgresql-operator-c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a-annotation"
        },
        {
          "digest": "sha256:362eb3c33eaa99ab39fdf42962a38f2818253e1841aa55f656a16bcb79ac5489",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:362eb3c33eaa99ab39fdf42962a38f2818253e1841aa55f656a16bcb79ac5489",
          "name": "apiserver"
        },
        {
          "digest": "sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "name": "operator"
        },
        {
          "digest": "sha256:5e6a484f7a559e90e79aa7a034bdcf9415848015c08d244b176de17d47deeb12",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:5e6a484f7a559e90e79aa7a034bdcf9415848015c08d244b176de17d47deeb12",
          "name": "scheduler"
        },
        {
          "digest": "sha256:8089fb1d73bf16757a28d94b1c08649556457d5f61cdfdc33bd75cea1147f012",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:8089fb1d73bf16757a28d94b1c08649556457d5f61cdfdc33bd75cea1147f012",
          "name": "event"
        },
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "pgo_backrest"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "pgo_backrest_repo"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "crunchy_pgbadger"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "crunchy_pgbouncer"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "crunchy_postgres_ha"
        },
        {
          "digest": "sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "name": "pgo_rmdata"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "percona-postgresql-operator-containers-b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1-annotation"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "percona-postgresql-operator-containers-0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74-annotation"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "percona-postgresql-operator-containers-0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a-annotation"
        },
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "percona-postgresql-operator-containers-8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee-annotation"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "percona-postgresql-operator-containers-65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "1.3.0",
      "version_original": "1.3.0"
    },
    {
      "_id": "62fb9800bb3b9bafa1352828",
      "alm_examples": [
        {
          "api_version": "pg.percona.com/v1",
          "kind": "PerconaPGCluster",
          "metadata": {
            "labels": {
              "pgo-version": "1.3.0"
            },
            "name": "cluster1"
          },
          "spec": {
            "backup": {
              "backrestRepoImage": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
              "resources": {
                "requests": {
                  "cpu": "200m",
                  "memory": "48Mi"
                }
              },
              "schedule": [
                {
                  "keep": 3,
                  "name": "sat-night-backup",
                  "schedule": "0 0 * * 6",
                  "storage": "local",
                  "type": "full"
                }
              ],
              "volumeSpec": {
                "accessmode": "ReadWriteOnce",
                "size": "1G",
                "storageclass": "",
                "storagetype": "dynamic"
              }
            },
            "database": "pgdb",
            "disableAutofail": false,
            "keepBackups": true,
            "keepData": true,
            "pause": false,
            "pgBadger": {
              "enabled": false,
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
              "port": 10000
            },
            "pgBouncer": {
              "expose": {
                "serviceType": "ClusterIP"
              },
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
              "resources": {
                "requests": {
                  "cpu": "1",
                  "memory": "128Mi"
                }
              },
              "size": 3
            },
            "pgPrimary": {
              "expose": {
                "serviceType": "ClusterIP"
              },
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
              "resources": {
                "requests": {
                  "cpu": "500m",
                  "memory": "256Mi"
                }
              },
              "tolerations": [],
              "volumeSpec": {
                "accessmode": "ReadWriteOnce",
                "size": "1G",
                "storageclass": "",
                "storagetype": "dynamic"
              }
            },
            "pgReplicas": {
              "hotStandby": {
                "enableSyncStandby": false,
                "expose": {
                  "serviceType": "ClusterIP"
                },
                "resources": {
                  "requests": {
                    "cpu": "500m",
                    "memory": "256Mi"
                  }
                },
                "size": 2,
                "volumeSpec": {
                  "accessmode": "ReadWriteOnce",
                  "size": "1G",
                  "storageclass": "",
                  "storagetype": "dynamic"
                }
              }
            },
            "pmm": {
              "enabled": false,
              "image": "percona/pmm-client:2.29.1",
              "pmmSecret": "cluster1-pmm-secret",
              "resources": {
                "requests": {
                  "cpu": "500m",
                  "memory": "200M"
                }
              },
              "serverHost": "monitoring-service",
              "serverUser": "admin"
            },
            "port": "5432",
            "standby": false,
            "tlsOnly": false,
            "upgradeOptions": {
              "apply": "disabled",
              "schedule": "0 4 * * *",
              "versionServiceEndpoint": "https://check.percona.com"
            },
            "user": "pguser"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/percona/percona-postgresql-operator-bundle@sha256:7995df96b3a7a033321d46097387922a70c011746082d0967fa9cfe8ce75a43d",
      "bundle_path_digest": "sha256:7995df96b3a7a033321d46097387922a70c011746082d0967fa9cfe8ce75a43d",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T13:13:36.080000+00:00",
      "csv_description": "\n## Percona is Cloud Native\n\nPercona Operator for PostgreSQL automates and simplifies deploying and managing open source PostgreSQL clusters on Kubernetes. Percona Operator for PostgreSQL is based on Postgres Operator developed by Crunchy Data.\nWhether you need to get a simple PostgreSQL cluster up and running, need to deploy a high availability, fault tolerant cluster in production, or are running your own database-as-a-service, the Operator provides the essential features you need to keep your clusters healthy.\n\nConsult the [documentation](https://www.percona.com/doc/kubernetes-operator-for-postgresql/index.html) on the Percona Operator for PostgreSQL for complete details on capabilities and options.\n\n### Supported Features\n\n* **PostgreSQL Cluster Provisioning**\nCreate, Scale, & Delete PostgreSQL clusters with ease, while fully customizing your Pods and PostgreSQL configuration.\n* **High Availability**\n\n    Safe, automated failover backed by a distributed consensus based high-availability solution. Uses Pod Anti-Affinity to help resiliency; you can configure how much\n    enforced this can be. Failed primaries automatically heal, allowing for faster recovery time. Support for standby PostgreSQL clusters that work both within and\n    across multiple Kubernetes clusters.\n\n* **Disaster Recovery**\n\n    Backups and restores leverage the open source pgBackRest utility and includes support for full, incremental, and differential backups as well as efficient delta\n    restores. Set how long you want your backups retained for. Works great with very large databases!\n\n* **Communication Security**\n\n    Secure communication between your applications and data servers by enabling TLS for your PostgreSQL servers, including the ability to enforce all of your connections\n    to use TLS.\n\n* **Monitoring**\n\n    Track the health of your PostgreSQL clusters with Percona Monitoring and Management (PMM).\n\n* **PostgreSQL User Management**\n\n    Quickly add and remove users from your PostgreSQL clusters with powerful commands. Manage password expiration policies or use your preferred PostgreSQL authentication\n    scheme.\n\n* **Updates Management**\n\n    Safely apply PostgreSQL updates with minimal availability impact to your PostgreSQL clusters.\n\n* **Advanced Replication Support**\n\n    Choose between asynchronous replication and synchronous replication for workloads that are sensitive to losing transactions.\n\n* **Clone**\n\n    Create new clusters from your existing clusters or backups.\n\n* **Connection Pooling**\n\n    Use [pgBouncer](https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/pgbouncer/) for connection pooling\n\n* **Affinity and Tolerations**\n\n    Have your PostgreSQL clusters deployed to Kubernetes Nodes of your preference with node affinity, or designate which nodes Kubernetes can schedule PostgreSQL instances\n    to with Kubernetes  tolerations.\n\n* **Scheduled Backups**\n\n    Choose the type of backup (full, incremental, differential) and how frequently you want it to occur on each PostgreSQL cluster.\n\n* **Backup to S3**\n\n    Store your backups in Amazon S3 or any object storage system that supports the S3 protocol. The Operator can backup, restore, and create new clusters from these backups.\n\n* **Multi-Namespace Support**\n\n    You can control how the Operator leverages Kubernetes Namespaces with several different deployment models:\n\n    * Deploy the Operator and all PostgreSQL clusters to the same namespace\n    * Deploy the Operator to one Namespace, and all PostgreSQL clusters to a different Namespace\n    * Deploy the Operator to one Namespace, and have your PostgreSQL clusters managed across multiple Namespaces\n\n* **Full Customizability**\n\n    The Operator not only makes it easy to get PostgreSQL up and running on Kubernetes-enabled platforms, but also allows you to further customize your deployments:\n\n    * Selecting different storage classes for your primary, replica, and backup storage\n    * Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters\n    * Use your own container image repository, including `imagePullSecrets` and private repositories  support\n    * Customize your PostgreSQL configuration\n    * Bring your own trusted certificate authority (CA) for use with the Operator API server\n    * Override your PostgreSQL configuration for each cluster\n    * Use your own custom images, re-define the image for each container separately\n\n### Release highlights\n* **What's new**\n\n    * The automated upgrade is now disabled by default to prevent an unplanned downtimes for user applications and to provide defaults more focused on strict user\u2019s control over the cluster\n    * Flexible anti-affinity configuration is now available, which allows the Operator to isolate PostgreSQL cluster instances on different Kubernetes nodes or to increase its availability by placing\nPostgreSQL instances in different availability zones\n    * Add possibility for postgres user to connect to PostgreSQL through PgBouncer with a new pgBouncer.exposePostgresUser Custom Resource option\n    * A new build and testing guide allows user to easily experiment with the source code of the Operator\n    * Fix the bug in the instruction on passing custom configuration options for PostgreSQL which made it usable for the new cluster only\n    * Fix the bug which caused the Operator crash without pgReplicas section in Custom Resource definition\n    * Fix the bug which caused the Operator to make connection requests to Version Service even with disabled Smart Update\n    * Fix the bug due to which restoring S3 backup from storage with self-signed certificates didn\u2019t work, by introducing the special backup.storages.verifyTLS option to address this issue\n",
      "csv_display_name": "Percona Operator for PostgreSQL",
      "csv_metadata_description": "Percona Operator for PostgreSQL manages the lifecycle of Percona PostgreSQL cluster instances.",
      "csv_name": "percona-postgresql-operator-certified.v1.3.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:01:34.513000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "percona-postgresql-operator-certified",
      "provided_apis": [
        {
          "group": "pg.percona.com",
          "kind": "Pgtask",
          "plural": "pgtasks",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "PerconaPGCluster",
          "plural": "perconapgclusters",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgcluster",
          "plural": "pgclusters",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgpolicy",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgreplica",
          "plural": "pgreplicas",
          "version": "v1"
        }
      ],
      "provider": "Percona",
      "related_images": [
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "pgbackrest-repo"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "pgbadger"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "postgres-ha"
        },
        {
          "digest": "sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "name": "rmdata"
        },
        {
          "digest": "sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "name": "percona-postgresql-operator-c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a-annotation"
        },
        {
          "digest": "sha256:362eb3c33eaa99ab39fdf42962a38f2818253e1841aa55f656a16bcb79ac5489",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:362eb3c33eaa99ab39fdf42962a38f2818253e1841aa55f656a16bcb79ac5489",
          "name": "apiserver"
        },
        {
          "digest": "sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "name": "operator"
        },
        {
          "digest": "sha256:5e6a484f7a559e90e79aa7a034bdcf9415848015c08d244b176de17d47deeb12",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:5e6a484f7a559e90e79aa7a034bdcf9415848015c08d244b176de17d47deeb12",
          "name": "scheduler"
        },
        {
          "digest": "sha256:8089fb1d73bf16757a28d94b1c08649556457d5f61cdfdc33bd75cea1147f012",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:8089fb1d73bf16757a28d94b1c08649556457d5f61cdfdc33bd75cea1147f012",
          "name": "event"
        },
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "pgo_backrest"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "pgo_backrest_repo"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "crunchy_pgbadger"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "crunchy_pgbouncer"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "crunchy_postgres_ha"
        },
        {
          "digest": "sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "name": "pgo_rmdata"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "percona-postgresql-operator-containers-b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1-annotation"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "percona-postgresql-operator-containers-0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74-annotation"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "percona-postgresql-operator-containers-0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a-annotation"
        },
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "percona-postgresql-operator-containers-8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee-annotation"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "percona-postgresql-operator-containers-65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.3.0",
      "version_original": "1.3.0"
    },
    {
      "_id": "62fb988ae07d71e4beb72db8",
      "alm_examples": [
        {
          "api_version": "pg.percona.com/v1",
          "kind": "PerconaPGCluster",
          "metadata": {
            "labels": {
              "pgo-version": "1.3.0"
            },
            "name": "cluster1"
          },
          "spec": {
            "backup": {
              "backrestRepoImage": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
              "resources": {
                "requests": {
                  "cpu": "200m",
                  "memory": "48Mi"
                }
              },
              "schedule": [
                {
                  "keep": 3,
                  "name": "sat-night-backup",
                  "schedule": "0 0 * * 6",
                  "storage": "local",
                  "type": "full"
                }
              ],
              "volumeSpec": {
                "accessmode": "ReadWriteOnce",
                "size": "1G",
                "storageclass": "",
                "storagetype": "dynamic"
              }
            },
            "database": "pgdb",
            "disableAutofail": false,
            "keepBackups": true,
            "keepData": true,
            "pause": false,
            "pgBadger": {
              "enabled": false,
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
              "port": 10000
            },
            "pgBouncer": {
              "expose": {
                "serviceType": "ClusterIP"
              },
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
              "resources": {
                "requests": {
                  "cpu": "1",
                  "memory": "128Mi"
                }
              },
              "size": 3
            },
            "pgPrimary": {
              "expose": {
                "serviceType": "ClusterIP"
              },
              "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
              "resources": {
                "requests": {
                  "cpu": "500m",
                  "memory": "256Mi"
                }
              },
              "tolerations": [],
              "volumeSpec": {
                "accessmode": "ReadWriteOnce",
                "size": "1G",
                "storageclass": "",
                "storagetype": "dynamic"
              }
            },
            "pgReplicas": {
              "hotStandby": {
                "enableSyncStandby": false,
                "expose": {
                  "serviceType": "ClusterIP"
                },
                "resources": {
                  "requests": {
                    "cpu": "500m",
                    "memory": "256Mi"
                  }
                },
                "size": 2,
                "volumeSpec": {
                  "accessmode": "ReadWriteOnce",
                  "size": "1G",
                  "storageclass": "",
                  "storagetype": "dynamic"
                }
              }
            },
            "pmm": {
              "enabled": false,
              "image": "percona/pmm-client:2.29.1",
              "pmmSecret": "cluster1-pmm-secret",
              "resources": {
                "requests": {
                  "cpu": "500m",
                  "memory": "200M"
                }
              },
              "serverHost": "monitoring-service",
              "serverUser": "admin"
            },
            "port": "5432",
            "standby": false,
            "tlsOnly": false,
            "upgradeOptions": {
              "apply": "disabled",
              "schedule": "0 4 * * *",
              "versionServiceEndpoint": "https://check.percona.com"
            },
            "user": "pguser"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/percona/percona-postgresql-operator-bundle@sha256:7995df96b3a7a033321d46097387922a70c011746082d0967fa9cfe8ce75a43d",
      "bundle_path_digest": "sha256:7995df96b3a7a033321d46097387922a70c011746082d0967fa9cfe8ce75a43d",
      "capabilities": [
        "Deep Insights"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T13:15:54.549000+00:00",
      "csv_description": "\n## Percona is Cloud Native\n\nPercona Operator for PostgreSQL automates and simplifies deploying and managing open source PostgreSQL clusters on Kubernetes. Percona Operator for PostgreSQL is based on Postgres Operator developed by Crunchy Data.\nWhether you need to get a simple PostgreSQL cluster up and running, need to deploy a high availability, fault tolerant cluster in production, or are running your own database-as-a-service, the Operator provides the essential features you need to keep your clusters healthy.\n\nConsult the [documentation](https://www.percona.com/doc/kubernetes-operator-for-postgresql/index.html) on the Percona Operator for PostgreSQL for complete details on capabilities and options.\n\n### Supported Features\n\n* **PostgreSQL Cluster Provisioning**\nCreate, Scale, & Delete PostgreSQL clusters with ease, while fully customizing your Pods and PostgreSQL configuration.\n* **High Availability**\n\n    Safe, automated failover backed by a distributed consensus based high-availability solution. Uses Pod Anti-Affinity to help resiliency; you can configure how much\n    enforced this can be. Failed primaries automatically heal, allowing for faster recovery time. Support for standby PostgreSQL clusters that work both within and\n    across multiple Kubernetes clusters.\n\n* **Disaster Recovery**\n\n    Backups and restores leverage the open source pgBackRest utility and includes support for full, incremental, and differential backups as well as efficient delta\n    restores. Set how long you want your backups retained for. Works great with very large databases!\n\n* **Communication Security**\n\n    Secure communication between your applications and data servers by enabling TLS for your PostgreSQL servers, including the ability to enforce all of your connections\n    to use TLS.\n\n* **Monitoring**\n\n    Track the health of your PostgreSQL clusters with Percona Monitoring and Management (PMM).\n\n* **PostgreSQL User Management**\n\n    Quickly add and remove users from your PostgreSQL clusters with powerful commands. Manage password expiration policies or use your preferred PostgreSQL authentication\n    scheme.\n\n* **Updates Management**\n\n    Safely apply PostgreSQL updates with minimal availability impact to your PostgreSQL clusters.\n\n* **Advanced Replication Support**\n\n    Choose between asynchronous replication and synchronous replication for workloads that are sensitive to losing transactions.\n\n* **Clone**\n\n    Create new clusters from your existing clusters or backups.\n\n* **Connection Pooling**\n\n    Use [pgBouncer](https://access.crunchydata.com/documentation/postgres-operator/latest/tutorial/pgbouncer/) for connection pooling\n\n* **Affinity and Tolerations**\n\n    Have your PostgreSQL clusters deployed to Kubernetes Nodes of your preference with node affinity, or designate which nodes Kubernetes can schedule PostgreSQL instances\n    to with Kubernetes  tolerations.\n\n* **Scheduled Backups**\n\n    Choose the type of backup (full, incremental, differential) and how frequently you want it to occur on each PostgreSQL cluster.\n\n* **Backup to S3**\n\n    Store your backups in Amazon S3 or any object storage system that supports the S3 protocol. The Operator can backup, restore, and create new clusters from these backups.\n\n* **Multi-Namespace Support**\n\n    You can control how the Operator leverages Kubernetes Namespaces with several different deployment models:\n\n    * Deploy the Operator and all PostgreSQL clusters to the same namespace\n    * Deploy the Operator to one Namespace, and all PostgreSQL clusters to a different Namespace\n    * Deploy the Operator to one Namespace, and have your PostgreSQL clusters managed across multiple Namespaces\n\n* **Full Customizability**\n\n    The Operator not only makes it easy to get PostgreSQL up and running on Kubernetes-enabled platforms, but also allows you to further customize your deployments:\n\n    * Selecting different storage classes for your primary, replica, and backup storage\n    * Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters\n    * Use your own container image repository, including `imagePullSecrets` and private repositories  support\n    * Customize your PostgreSQL configuration\n    * Bring your own trusted certificate authority (CA) for use with the Operator API server\n    * Override your PostgreSQL configuration for each cluster\n    * Use your own custom images, re-define the image for each container separately\n\n### Release highlights\n* **What's new**\n\n    * The automated upgrade is now disabled by default to prevent an unplanned downtimes for user applications and to provide defaults more focused on strict user\u2019s control over the cluster\n    * Flexible anti-affinity configuration is now available, which allows the Operator to isolate PostgreSQL cluster instances on different Kubernetes nodes or to increase its availability by placing\nPostgreSQL instances in different availability zones\n    * Add possibility for postgres user to connect to PostgreSQL through PgBouncer with a new pgBouncer.exposePostgresUser Custom Resource option\n    * A new build and testing guide allows user to easily experiment with the source code of the Operator\n    * Fix the bug in the instruction on passing custom configuration options for PostgreSQL which made it usable for the new cluster only\n    * Fix the bug which caused the Operator crash without pgReplicas section in Custom Resource definition\n    * Fix the bug which caused the Operator to make connection requests to Version Service even with disabled Smart Update\n    * Fix the bug due to which restoring S3 backup from storage with self-signed certificates didn\u2019t work, by introducing the special backup.storages.verifyTLS option to address this issue\n",
      "csv_display_name": "Percona Operator for PostgreSQL",
      "csv_metadata_description": "Percona Operator for PostgreSQL manages the lifecycle of Percona PostgreSQL cluster instances.",
      "csv_name": "percona-postgresql-operator-certified.v1.3.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:07:17.678000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "percona-postgresql-operator-certified",
      "provided_apis": [
        {
          "group": "pg.percona.com",
          "kind": "Pgtask",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "PerconaPGCluster",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgcluster",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgpolicy",
          "version": "v1"
        },
        {
          "group": "pg.percona.com",
          "kind": "Pgreplica",
          "version": "v1"
        }
      ],
      "provider": "Percona",
      "related_images": [
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "pgbackrest-repo"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "pgbadger"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "postgres-ha"
        },
        {
          "digest": "sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "name": "rmdata"
        },
        {
          "digest": "sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "name": "percona-postgresql-operator-c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a-annotation"
        },
        {
          "digest": "sha256:362eb3c33eaa99ab39fdf42962a38f2818253e1841aa55f656a16bcb79ac5489",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:362eb3c33eaa99ab39fdf42962a38f2818253e1841aa55f656a16bcb79ac5489",
          "name": "apiserver"
        },
        {
          "digest": "sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:c997d3b902bd95185252ae6727bca1fef5424ff1d8c6c716e2f419b88946716a",
          "name": "operator"
        },
        {
          "digest": "sha256:5e6a484f7a559e90e79aa7a034bdcf9415848015c08d244b176de17d47deeb12",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:5e6a484f7a559e90e79aa7a034bdcf9415848015c08d244b176de17d47deeb12",
          "name": "scheduler"
        },
        {
          "digest": "sha256:8089fb1d73bf16757a28d94b1c08649556457d5f61cdfdc33bd75cea1147f012",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:8089fb1d73bf16757a28d94b1c08649556457d5f61cdfdc33bd75cea1147f012",
          "name": "event"
        },
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "pgo_backrest"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "pgo_backrest_repo"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "crunchy_pgbadger"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "crunchy_pgbouncer"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "crunchy_postgres_ha"
        },
        {
          "digest": "sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator@sha256:3c7688d4ad7bbfda10a773862b58a070f6eb11106a003fd909081ed51edd520d",
          "name": "pgo_rmdata"
        },
        {
          "digest": "sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1",
          "name": "percona-postgresql-operator-containers-b885df785f1ec057e04d9e7fefc0805d93f479ac812d6b8df4161a178cab21a1-annotation"
        },
        {
          "digest": "sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74",
          "name": "percona-postgresql-operator-containers-0863a651319c5339898343adb4bbf09ef6e50478badd5927a1582abd5e3c3b74-annotation"
        },
        {
          "digest": "sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a",
          "name": "percona-postgresql-operator-containers-0bde250fde02d7e0a234277780d83785f5ef04c48dbf1e57c127d0f2ccbaa34a-annotation"
        },
        {
          "digest": "sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee",
          "name": "percona-postgresql-operator-containers-8cfc2320d663152ec46adb79355935e161879249796e4689b3243a75653089ee-annotation"
        },
        {
          "digest": "sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "image": "registry.connect.redhat.com/percona/percona-postgresql-operator-containers@sha256:65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e",
          "name": "percona-postgresql-operator-containers-65746063ec9fab1869065f48394bb40b164c33e8102c48c42439812b2a6ba43e-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.3.0",
      "version_original": "1.3.0"
    },
    {
      "_id": "62fbd564bb3b9bafa1354691",
      "alm_examples": [
        {
          "api_version": "machinelearning.seldon.io/v1",
          "kind": "SeldonDeployment",
          "metadata": {
            "labels": {
              "app": "seldon",
              "app.kubernetes.io/instance": "seldon1",
              "app.kubernetes.io/name": "seldon",
              "app.kubernetes.io/version": "v0.5"
            },
            "name": "seldon-model"
          },
          "spec": {
            "name": "test-deployment",
            "predictors": [
              {
                "componentSpecs": [
                  {
                    "spec": {
                      "containers": [
                        {
                          "image": "seldonio/mock_classifier:1.6.0",
                          "name": "classifier"
                        }
                      ]
                    }
                  }
                ],
                "graph": {
                  "children": [],
                  "name": "classifier",
                  "type": "MODEL"
                },
                "name": "example",
                "replicas": 1
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/seldonio/seldon-core-operator-bundle@sha256:08768fe00addf43bfcd19acead3e84d860be67f0334b6fe56873863ed68b7ae7",
      "bundle_path_digest": "sha256:08768fe00addf43bfcd19acead3e84d860be67f0334b6fe56873863ed68b7ae7",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T17:35:32.307000+00:00",
      "csv_description": "The Seldon operator enables for native operation of production machine learning workloads, including monitoring and operations of language-agnostic models with the benefits of real-time metrics and log analysis.\n   \n## Overview\nSeldon Core is an open source platform for deploying machine learning models on a Kubernetes cluster.\n\n* Deploy machine learning models in the cloud or on-premise.\n* Get metrics and ensure proper governance and compliance for your running machine learning models.\n* Create powerful inference graphs made up of multiple components.\n* Provide a consistent serving layer for models built using heterogeneous ML toolkits.\n\nYou can get started by following the guides in our documentation at https://docs.seldon.io/projects/seldon-core/en/latest/workflow/README.html\n",
      "csv_display_name": "Seldon Operator",
      "csv_metadata_description": "The Seldon operator for management, monitoring and operations of machine learning systems through the Seldon Engine. Once installed, the Seldon Operator provides multiple functions which facilitate the productisation, monitoring and maintenance of machine learning systems at scale.",
      "csv_name": "seldon-operator.v1.14.1-2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:02:14.854000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "seldon-operator-certified",
      "provided_apis": [
        {
          "group": "machinelearning.seldon.io",
          "kind": "SeldonDeployment",
          "version": "v1"
        }
      ],
      "provider": "Seldon Technologies",
      "related_images": [
        {
          "digest": "sha256:41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282",
          "image": "registry.connect.redhat.com/seldonio/seldon-core-operator@sha256:41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282",
          "name": "seldon-core-operator-41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282-annotation"
        },
        {
          "digest": "sha256:41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282",
          "image": "registry.connect.redhat.com/seldonio/seldon-core-operator@sha256:41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282",
          "name": "manager"
        },
        {
          "digest": "sha256:0cdbe17a8d73b0dd9798e8acee9694282a3f4e9c2eb80b0c3bbc2958dbb452ab",
          "image": "registry.connect.redhat.com/seldonio/seldon-core-executor@sha256:0cdbe17a8d73b0dd9798e8acee9694282a3f4e9c2eb80b0c3bbc2958dbb452ab",
          "name": "executor"
        },
        {
          "digest": "sha256:075230960142845dba8decfa007c8a48474aad8cc90317ff77192c8fa9c1b84d",
          "image": "registry.connect.redhat.com/seldonio/mock-classifier@sha256:075230960142845dba8decfa007c8a48474aad8cc90317ff77192c8fa9c1b84d",
          "name": "mock_classifier"
        },
        {
          "digest": "sha256:a0280c13136dcc870194af72630b9d2f7fc8bcff4edb54dd3bfbce36741af50c",
          "image": "registry.connect.redhat.com/seldonio/rclone-storage-initializer@sha256:a0280c13136dcc870194af72630b9d2f7fc8bcff4edb54dd3bfbce36741af50c",
          "name": "rclone_storage_initializer"
        },
        {
          "digest": "sha256:84a13b0c96d87f9d784bba1efae2d7a65453ebd6555359fa6d50e851a0520dc9",
          "image": "registry.connect.redhat.com/seldonio/sklearnserver@sha256:84a13b0c96d87f9d784bba1efae2d7a65453ebd6555359fa6d50e851a0520dc9",
          "name": "sklearnserver"
        },
        {
          "digest": "sha256:246681e031cd1b5bcde5400d9d86e50daebc13c2db7d2568f641ef391fb66dab",
          "image": "registry.connect.redhat.com/seldonio/mlflowserver@sha256:246681e031cd1b5bcde5400d9d86e50daebc13c2db7d2568f641ef391fb66dab",
          "name": "mlflowserver"
        },
        {
          "digest": "sha256:266c697d5349c7d636240aa139b25c53e403ded43bb8a3eef8e7e9e40215dff2",
          "image": "registry.connect.redhat.com/seldonio/xgboostserver@sha256:266c697d5349c7d636240aa139b25c53e403ded43bb8a3eef8e7e9e40215dff2",
          "name": "xgboostserver"
        },
        {
          "digest": "sha256:d89faa03a12acf0028a2556ac4072a6d37ec3287d890396c99ebee076f64c519",
          "image": "registry.connect.redhat.com/seldonio/tfproxy@sha256:d89faa03a12acf0028a2556ac4072a6d37ec3287d890396c99ebee076f64c519",
          "name": "tfproxy"
        },
        {
          "digest": "sha256:747f7389db946fa331fb41f0cad84bfc3f7bff8cf189a7a69d08a888f50ae60e",
          "image": "registry.connect.redhat.com/seldonio/tensorflow-serving@sha256:747f7389db946fa331fb41f0cad84bfc3f7bff8cf189a7a69d08a888f50ae60e",
          "name": "tensorflow"
        },
        {
          "digest": "sha256:2ab919e49b61638cf0921b6efb0f4db55465b02833c83ade21d6c3f81b8fbcf2",
          "image": "registry.connect.redhat.com/seldonio/alibiexplainer@sha256:2ab919e49b61638cf0921b6efb0f4db55465b02833c83ade21d6c3f81b8fbcf2",
          "name": "explainer"
        },
        {
          "digest": "sha256:a0280c13136dcc870194af72630b9d2f7fc8bcff4edb54dd3bfbce36741af50c",
          "image": "registry.connect.redhat.com/seldonio/rclone-storage-initializer@sha256:a0280c13136dcc870194af72630b9d2f7fc8bcff4edb54dd3bfbce36741af50c",
          "name": "storage_initializer"
        }
      ],
      "replaces": null,
      "skip_range": ">=1.12.0 <1.14.1-2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.14.1-2",
      "version_original": "1.14.1-2"
    },
    {
      "_id": "62fbd580e07d71e4beb74c21",
      "alm_examples": [
        {
          "api_version": "machinelearning.seldon.io/v1",
          "kind": "SeldonDeployment",
          "metadata": {
            "labels": {
              "app": "seldon",
              "app.kubernetes.io/instance": "seldon1",
              "app.kubernetes.io/name": "seldon",
              "app.kubernetes.io/version": "v0.5"
            },
            "name": "seldon-model"
          },
          "spec": {
            "name": "test-deployment",
            "predictors": [
              {
                "componentSpecs": [
                  {
                    "spec": {
                      "containers": [
                        {
                          "image": "seldonio/mock_classifier:1.6.0",
                          "name": "classifier"
                        }
                      ]
                    }
                  }
                ],
                "graph": {
                  "children": [],
                  "name": "classifier",
                  "type": "MODEL"
                },
                "name": "example",
                "replicas": 1
              }
            ]
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/seldonio/seldon-core-operator-bundle@sha256:08768fe00addf43bfcd19acead3e84d860be67f0334b6fe56873863ed68b7ae7",
      "bundle_path_digest": "sha256:08768fe00addf43bfcd19acead3e84d860be67f0334b6fe56873863ed68b7ae7",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T17:36:00.553000+00:00",
      "csv_description": "The Seldon operator enables for native operation of production machine learning workloads, including monitoring and operations of language-agnostic models with the benefits of real-time metrics and log analysis.\n   \n## Overview\nSeldon Core is an open source platform for deploying machine learning models on a Kubernetes cluster.\n\n* Deploy machine learning models in the cloud or on-premise.\n* Get metrics and ensure proper governance and compliance for your running machine learning models.\n* Create powerful inference graphs made up of multiple components.\n* Provide a consistent serving layer for models built using heterogeneous ML toolkits.\n\nYou can get started by following the guides in our documentation at https://docs.seldon.io/projects/seldon-core/en/latest/workflow/README.html\n",
      "csv_display_name": "Seldon Operator",
      "csv_metadata_description": "The Seldon operator for management, monitoring and operations of machine learning systems through the Seldon Engine. Once installed, the Seldon Operator provides multiple functions which facilitate the productisation, monitoring and maintenance of machine learning systems at scale.",
      "csv_name": "seldon-operator.v1.14.1-2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:00:38.335000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "seldon-operator-certified",
      "provided_apis": [
        {
          "group": "machinelearning.seldon.io",
          "kind": "SeldonDeployment",
          "version": "v1"
        }
      ],
      "provider": "Seldon Technologies",
      "related_images": [
        {
          "digest": "sha256:41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282",
          "image": "registry.connect.redhat.com/seldonio/seldon-core-operator@sha256:41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282",
          "name": "seldon-core-operator-41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282-annotation"
        },
        {
          "digest": "sha256:41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282",
          "image": "registry.connect.redhat.com/seldonio/seldon-core-operator@sha256:41c921605c9a6ee1bd97583feaad70179fb46a3494c447c83caec25c20541282",
          "name": "manager"
        },
        {
          "digest": "sha256:0cdbe17a8d73b0dd9798e8acee9694282a3f4e9c2eb80b0c3bbc2958dbb452ab",
          "image": "registry.connect.redhat.com/seldonio/seldon-core-executor@sha256:0cdbe17a8d73b0dd9798e8acee9694282a3f4e9c2eb80b0c3bbc2958dbb452ab",
          "name": "executor"
        },
        {
          "digest": "sha256:075230960142845dba8decfa007c8a48474aad8cc90317ff77192c8fa9c1b84d",
          "image": "registry.connect.redhat.com/seldonio/mock-classifier@sha256:075230960142845dba8decfa007c8a48474aad8cc90317ff77192c8fa9c1b84d",
          "name": "mock_classifier"
        },
        {
          "digest": "sha256:a0280c13136dcc870194af72630b9d2f7fc8bcff4edb54dd3bfbce36741af50c",
          "image": "registry.connect.redhat.com/seldonio/rclone-storage-initializer@sha256:a0280c13136dcc870194af72630b9d2f7fc8bcff4edb54dd3bfbce36741af50c",
          "name": "rclone_storage_initializer"
        },
        {
          "digest": "sha256:84a13b0c96d87f9d784bba1efae2d7a65453ebd6555359fa6d50e851a0520dc9",
          "image": "registry.connect.redhat.com/seldonio/sklearnserver@sha256:84a13b0c96d87f9d784bba1efae2d7a65453ebd6555359fa6d50e851a0520dc9",
          "name": "sklearnserver"
        },
        {
          "digest": "sha256:246681e031cd1b5bcde5400d9d86e50daebc13c2db7d2568f641ef391fb66dab",
          "image": "registry.connect.redhat.com/seldonio/mlflowserver@sha256:246681e031cd1b5bcde5400d9d86e50daebc13c2db7d2568f641ef391fb66dab",
          "name": "mlflowserver"
        },
        {
          "digest": "sha256:266c697d5349c7d636240aa139b25c53e403ded43bb8a3eef8e7e9e40215dff2",
          "image": "registry.connect.redhat.com/seldonio/xgboostserver@sha256:266c697d5349c7d636240aa139b25c53e403ded43bb8a3eef8e7e9e40215dff2",
          "name": "xgboostserver"
        },
        {
          "digest": "sha256:d89faa03a12acf0028a2556ac4072a6d37ec3287d890396c99ebee076f64c519",
          "image": "registry.connect.redhat.com/seldonio/tfproxy@sha256:d89faa03a12acf0028a2556ac4072a6d37ec3287d890396c99ebee076f64c519",
          "name": "tfproxy"
        },
        {
          "digest": "sha256:747f7389db946fa331fb41f0cad84bfc3f7bff8cf189a7a69d08a888f50ae60e",
          "image": "registry.connect.redhat.com/seldonio/tensorflow-serving@sha256:747f7389db946fa331fb41f0cad84bfc3f7bff8cf189a7a69d08a888f50ae60e",
          "name": "tensorflow"
        },
        {
          "digest": "sha256:2ab919e49b61638cf0921b6efb0f4db55465b02833c83ade21d6c3f81b8fbcf2",
          "image": "registry.connect.redhat.com/seldonio/alibiexplainer@sha256:2ab919e49b61638cf0921b6efb0f4db55465b02833c83ade21d6c3f81b8fbcf2",
          "name": "explainer"
        },
        {
          "digest": "sha256:a0280c13136dcc870194af72630b9d2f7fc8bcff4edb54dd3bfbce36741af50c",
          "image": "registry.connect.redhat.com/seldonio/rclone-storage-initializer@sha256:a0280c13136dcc870194af72630b9d2f7fc8bcff4edb54dd3bfbce36741af50c",
          "name": "storage_initializer"
        }
      ],
      "replaces": null,
      "skip_range": ">=1.12.0 <1.14.1-2",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.14.1-2",
      "version_original": "1.14.1-2"
    },
    {
      "_id": "62fbdc6fe07d71e4beb74fec",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.7"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "bundle_path_digest": "sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T18:05:35.214000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.13.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-21T21:30:11.738000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator-ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195-annotation"
        },
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "42.13.0",
      "version_original": "42.13.0"
    },
    {
      "_id": "62fbe04130a80ba5a2096ab2",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.7"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "bundle_path_digest": "sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T18:21:53.218000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.13.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-21T21:41:53.589000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator-ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195-annotation"
        },
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "42.13.0",
      "version_original": "42.13.0"
    },
    {
      "_id": "62fbe056bb3b9bafa1354bab",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.7"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "bundle_path_digest": "sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T18:22:14.484000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.13.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-21T21:47:58.208000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator-ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195-annotation"
        },
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "42.13.0",
      "version_original": "42.13.0"
    },
    {
      "_id": "62fbe0ddbb3b9bafa1354bc3",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.7"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "bundle_path_digest": "sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T18:24:29.672000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.13.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-21T21:48:22.654000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator-ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195-annotation"
        },
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "42.13.0",
      "version_original": "42.13.0"
    },
    {
      "_id": "62fbe0f3b289bcc7af67ade5",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.7"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "bundle_path_digest": "sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T18:24:51.611000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.13.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-21T21:52:08.863000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator-ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195-annotation"
        },
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "42.13.0",
      "version_original": "42.13.0"
    },
    {
      "_id": "62fbe277bb3b9bafa1354c35",
      "alm_examples": [
        {
          "api_version": "charts.helm.k8s.io/v1",
          "kind": "Xl",
          "metadata": {
            "name": "xl-release"
          },
          "spec": {
            "global": {
              "customImageNames": false,
              "repository": "registry.connect.redhat.com/turbonomic",
              "tag": "8.5.7"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/turbonomic/t8c-operator-bundle@sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "bundle_path_digest": "sha256:c5c898eff1a7189cd0e748bdcc08a40b5a05d23cd69d86a9716c0435b8719ec9",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-16T18:31:19.789000+00:00",
      "csv_description": "### Realtime Decision Automation for Multicloud Applications\nTurbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:\n* Continuous placement of workload across multiple clouds both on-prem and public clouds providers.\n* Continuous scaling for applications and the underlying infrastructure.\n\nIt assures application performance by giving workloads the resources they need when they need them.\n\n### How does it work?\nTurbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.\nTurbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.\nTurbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.",
      "csv_display_name": "Turbonomic Platform Operator",
      "csv_metadata_description": "Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints.",
      "csv_name": "t8c-operator.v42.13.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-21T21:51:26.584000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "t8c-certified",
      "provided_apis": [
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1alpha1"
        },
        {
          "group": "charts.helm.k8s.io",
          "kind": "Xl",
          "plural": "xls",
          "version": "v1"
        }
      ],
      "provider": "Turbonomic, Inc.",
      "related_images": [
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator-ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195-annotation"
        },
        {
          "digest": "sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "image": "registry.connect.redhat.com/turbonomic/t8c-operator@sha256:ecf118a07bd533a95e93c67b48aaa0ba442be529c23af50e7a835ab710696195",
          "name": "t8c-operator"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "42.13.0",
      "version_original": "42.13.0"
    },
    {
      "_id": "62fce463bb3b9bafa13584d4",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "bundle_path_digest": "sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T12:51:47.385000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:00:50.443000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "image": "registry.connect.redhat.com/trilio/web@sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.10.3",
      "version_original": "2.10.3"
    },
    {
      "_id": "62fce467bb3b9bafa13584d6",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "bundle_path_digest": "sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T12:51:51.028000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:50:24.099000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "image": "registry.connect.redhat.com/trilio/web@sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.10.3",
      "version_original": "2.10.3"
    },
    {
      "_id": "62fce486b289bcc7af67e726",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "bundle_path_digest": "sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T12:52:22.897000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:55:40.129000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "image": "registry.connect.redhat.com/trilio/web@sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.10.3",
      "version_original": "2.10.3"
    },
    {
      "_id": "62fce8ac30a80ba5a209a3a9",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "bundle_path_digest": "sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T13:10:04.355000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:28:50.267000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "image": "registry.connect.redhat.com/trilio/web@sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.10.3",
      "version_original": "2.10.3"
    },
    {
      "_id": "62fce8bfe07d71e4beb78c45",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "bundle_path_digest": "sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T13:10:23.041000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:13:17.592000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "image": "registry.connect.redhat.com/trilio/web@sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "2.10.3",
      "version_original": "2.10.3"
    },
    {
      "_id": "62fce8f430a80ba5a209a3b4",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "bundle_path_digest": "sha256:b7ab313d7aa6a902ae023b7726c74c1a0f0d340716e7594558c0013cfaaf7f36",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T13:11:16.854000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:39:46.452000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "image": "registry.connect.redhat.com/trilio/web@sha256:ffda02d6487b7f7a1f810363efc4613872919c28326f4806f0c921c07ddcded7",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:c1742bf2c723d49b936b055daa69df3a6834d62d357f8b6eb7d9a2064112787c",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.3",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "2.10.3",
      "version_original": "2.10.3"
    },
    {
      "_id": "62fd3c5d54966df4ef12e650",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-08-17T19:07:09.083000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:14:20.184000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd3c5eb289bcc7af6803bc",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:07:10.188000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:14:12.634000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "version": "v3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd3c96e07d71e4beb7a8f8",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-08-17T19:08:06.261000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:08:30.713000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd3c96b289bcc7af680401",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:08:06.820000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:08:38.622000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd3d0eb289bcc7af68041c",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-08-17T19:10:06.513000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:14:14.395000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd3d0f54966df4ef12e69f",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:10:07.215000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:30:03.718000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd3db8b289bcc7af680426",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-08-17T19:12:56.598000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:01:23.955000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd3db954966df4ef12e6a7",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:12:57.111000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:01:29.890000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd40aae07d71e4beb7aa53",
      "alm_examples": [
        {
          "api_version": "anzo.cambridgesemantics.com/v1",
          "kind": "Anzo",
          "metadata": {
            "name": "agent01"
          },
          "spec": {
            "nodeConfig": {
              "spec": {
                "replicas": 1,
                "selector": {
                  "matchLabels": {
                    "app": "anzo"
                  }
                },
                "serviceName": "anzo-agent01",
                "template": {
                  "metadata": {
                    "labels": {
                      "app": "anzo"
                    }
                  },
                  "spec": {
                    "containers": [
                      {
                        "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
                        "name": "anzo",
                        "resources": {
                          "limits": {
                            "cpu": "4000m",
                            "memory": "12Gi"
                          },
                          "requests": {
                            "cpu": "4000m",
                            "memory": "12Gi"
                          }
                        }
                      }
                    ],
                    "serviceAccountName": "anzo-operator"
                  }
                }
              }
            },
            "role": "AnzoAgent"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzo-operator-bundle@sha256:c8551af31d709ae98163e917e358c7d565eb6301ac0441ed09c8f4fe4faeb51d",
      "bundle_path_digest": "sha256:c8551af31d709ae98163e917e358c7d565eb6301ac0441ed09c8f4fe4faeb51d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:25:30.450000+00:00",
      "csv_description": "The Anzo Operator provides the way to install and configure an anzo agent setup on Red Hat K8S environment.\nCurrently, this is supported only through when deployed as an Anzo Agent as part of an Anzo Unstructured deployment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzo/blob/v2.0.2/README.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzo/userdoc/cloud-deployments.htm )\n\n### Support\n\nWe offer Support to our customers through [ Cambridge Semantics Customer Center ]( https://customercenter.cambridgesemantics.com/ ).",
      "csv_display_name": "Anzo Operator",
      "csv_metadata_description": "kubernetes operator for Anzo",
      "csv_name": "anzo-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:36:04.507000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "anzo-operator",
      "provided_apis": [
        {
          "group": "anzo.cambridgesemantics.com",
          "kind": "Anzo",
          "plural": "anzos",
          "version": "v1"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "name": "anzo"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "anzo-operator"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "anzo-operator-9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036-annotation"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "manager"
        },
        {
          "digest": "sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "name": "anzo-845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.0.2",
      "version_original": "2.0.2"
    },
    {
      "_id": "62fd40b0e07d71e4beb7aa58",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:25:36.873000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:19:50.097000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd40b130a80ba5a209c1eb",
      "alm_examples": [
        {
          "api_version": "anzo.cambridgesemantics.com/v1",
          "kind": "Anzo",
          "metadata": {
            "name": "agent01"
          },
          "spec": {
            "nodeConfig": {
              "spec": {
                "replicas": 1,
                "selector": {
                  "matchLabels": {
                    "app": "anzo"
                  }
                },
                "serviceName": "anzo-agent01",
                "template": {
                  "metadata": {
                    "labels": {
                      "app": "anzo"
                    }
                  },
                  "spec": {
                    "containers": [
                      {
                        "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
                        "name": "anzo",
                        "resources": {
                          "limits": {
                            "cpu": "4000m",
                            "memory": "12Gi"
                          },
                          "requests": {
                            "cpu": "4000m",
                            "memory": "12Gi"
                          }
                        }
                      }
                    ],
                    "serviceAccountName": "anzo-operator"
                  }
                }
              }
            },
            "role": "AnzoAgent"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzo-operator-bundle@sha256:c8551af31d709ae98163e917e358c7d565eb6301ac0441ed09c8f4fe4faeb51d",
      "bundle_path_digest": "sha256:c8551af31d709ae98163e917e358c7d565eb6301ac0441ed09c8f4fe4faeb51d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:25:37.679000+00:00",
      "csv_description": "The Anzo Operator provides the way to install and configure an anzo agent setup on Red Hat K8S environment.\nCurrently, this is supported only through when deployed as an Anzo Agent as part of an Anzo Unstructured deployment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzo/blob/v2.0.2/README.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzo/userdoc/cloud-deployments.htm )\n\n### Support\n\nWe offer Support to our customers through [ Cambridge Semantics Customer Center ]( https://customercenter.cambridgesemantics.com/ ).",
      "csv_display_name": "Anzo Operator",
      "csv_metadata_description": "kubernetes operator for Anzo",
      "csv_name": "anzo-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:41:38.158000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "anzo-operator",
      "provided_apis": [
        {
          "group": "anzo.cambridgesemantics.com",
          "kind": "Anzo",
          "plural": "anzos",
          "version": "v1"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "name": "anzo"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "anzo-operator"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "anzo-operator-9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036-annotation"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "manager"
        },
        {
          "digest": "sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "name": "anzo-845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "2.0.2",
      "version_original": "2.0.2"
    },
    {
      "_id": "62fd40b2e07d71e4beb7aa5a",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-08-17T19:25:38.545000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T11:52:32.599000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd40d130a80ba5a209c1ee",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "beta",
      "creation_date": "2022-08-17T19:26:09.876000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:09:54.073000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd40d2e07d71e4beb7aa5d",
      "alm_examples": [
        {
          "api_version": "anzo.cambridgesemantics.com/v1",
          "kind": "Anzo",
          "metadata": {
            "name": "agent01"
          },
          "spec": {
            "nodeConfig": {
              "spec": {
                "replicas": 1,
                "selector": {
                  "matchLabels": {
                    "app": "anzo"
                  }
                },
                "serviceName": "anzo-agent01",
                "template": {
                  "metadata": {
                    "labels": {
                      "app": "anzo"
                    }
                  },
                  "spec": {
                    "containers": [
                      {
                        "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
                        "name": "anzo",
                        "resources": {
                          "limits": {
                            "cpu": "4000m",
                            "memory": "12Gi"
                          },
                          "requests": {
                            "cpu": "4000m",
                            "memory": "12Gi"
                          }
                        }
                      }
                    ],
                    "serviceAccountName": "anzo-operator"
                  }
                }
              }
            },
            "role": "AnzoAgent"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzo-operator-bundle@sha256:c8551af31d709ae98163e917e358c7d565eb6301ac0441ed09c8f4fe4faeb51d",
      "bundle_path_digest": "sha256:c8551af31d709ae98163e917e358c7d565eb6301ac0441ed09c8f4fe4faeb51d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:26:10.345000+00:00",
      "csv_description": "The Anzo Operator provides the way to install and configure an anzo agent setup on Red Hat K8S environment.\nCurrently, this is supported only through when deployed as an Anzo Agent as part of an Anzo Unstructured deployment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzo/blob/v2.0.2/README.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzo/userdoc/cloud-deployments.htm )\n\n### Support\n\nWe offer Support to our customers through [ Cambridge Semantics Customer Center ]( https://customercenter.cambridgesemantics.com/ ).",
      "csv_display_name": "Anzo Operator",
      "csv_metadata_description": "kubernetes operator for Anzo",
      "csv_name": "anzo-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:38:28.986000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "anzo-operator",
      "provided_apis": [
        {
          "group": "anzo.cambridgesemantics.com",
          "kind": "Anzo",
          "plural": "anzos",
          "version": "v1"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "name": "anzo"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "anzo-operator"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "anzo-operator-9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036-annotation"
        },
        {
          "digest": "sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-operator@sha256:9e3b1ed808574acf72ac40598477a94dc1b1402d55ea68e5ee0de40dc45ef036",
          "name": "manager"
        },
        {
          "digest": "sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo@sha256:845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f",
          "name": "anzo-845b22d31888c0a4b2106ae96a239843ad7465654d3e50e0294cba6dd2a5545f-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "2.0.2",
      "version_original": "2.0.2"
    },
    {
      "_id": "62fd40d354966df4ef12e7d7",
      "alm_examples": [
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "ClusterMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "clustermaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "IndexerCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "indexercluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "LicenseMaster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "licensemaster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "MonitoringConsole",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "monitoringconsole-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "SearchHeadCluster",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "searchheadcluster-sample"
          },
          "spec": {}
        },
        {
          "api_version": "enterprise.splunk.com/v3",
          "kind": "Standalone",
          "metadata": {
            "finalizers": [
              "enterprise.splunk.com/delete-pvc"
            ],
            "name": "standalone-sample"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/splunk/splunk-operator-bundle@sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "bundle_path_digest": "sha256:4f570d1c7f3371d0673ac081dad2951ab1b8180a22e141d29684eb625f549c88",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T19:26:11.024000+00:00",
      "csv_description": "# Getting Started with the Splunk Operator for Kubernetes\nThe Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices. \n\nThe Splunk Operator runs as a container, and uses the Kubernetes [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) and [custom resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) objects to create and manage a scalable and sustainable Splunk Enterprise environment.\n\nThis guide is intended to help new users get up and running with the\nSplunk Operator for Kubernetes. It is divided into the following sections:\n\n* [Known Issues for the Splunk Operator](#known-issues-for-the-splunk-operator)\n* [Prerequisites for the Splunk Operator](#prerequisites-for-the-splunk-operator)\n* [Installing the Splunk Operator](#installing-the-splunk-operator)\n* [Creating Splunk Enterprise Deployments](#creating-a-splunk-enterprise-deployment)\n* [Securing Splunk Deployments in Kubernetes](Security.md)\n* [Contacting Support](#contacting-support)\n\n## Support Resources\n\nSPLUNK SUPPORTED: The Splunk Operator for Kubernetes is a supported method for deploying distributed Splunk Enterprise environments using containers.\n\nCOMMUNITY DEVELOPED: Splunk Operator for Kubernetes is an open source product developed by Splunkers with contributions from the community of partners and customers. The primary reason why Splunk is taking this approach is to push product development closer to those that use and depend upon it. This direct connection will help us all be more successful and move at a rapid pace.\n\nIf you're interested in contributing to the SOK open source project, review the [Contributing to the Project](CONTRIBUTING.md) page.\n\n**Community Support & Discussions on\n[Slack](https://splunk-usergroups.slack.com)** channel #splunk-operator-for-kubernetes\n\n**File Issues or Enhancements in\n[GitHub](https://github.com/splunk/splunk-operator/issues)** splunk/splunk-operator\n\n\n## Known Issues for the Splunk Operator\n\nReview the [Change Log](ChangeLog.md) page for a history of changes in each release.\n\n## Prerequisites for the Splunk Operator\n\n### Supported Kubernetes Versions\n\n- Kubernetes, version 1.16.2+ and later (x86 64-bit only).\n\nThe Splunk Operator should work with any [CNCF certified distribution](https://www.cncf.io/certification/software-conformance/) of Kubernetes. We do not have platform recommendations, but this is a table of platforms that our developers, customers, and partners have used successfully with the Splunk Operator.\n\n<table>\n<tr><td> Splunk Development & Testing Platforms </td><td> Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) </td></tr>\n<tr><td> Customer Reported Platforms </td><td> Microsoft Azure Kubernetes Service (AKS), Red Hat OpenShift </td></tr>\n<tr><td> Partner Tested Platforms</td><td> HPE Ezmeral</td></tr>\n<tr><td> Other Platforms </td><td>CNCF certified distribution</td></tr>\n</table>\n\n### Splunk Enterprise Compatibility\nEach Splunk Operator release has specific Splunk Enterprise compatibility requirements. Before installing or upgrading the Splunk Operator, review the [Change Log](ChangeLog.md) to verify version compatibility with Splunk Enterprise releases.\n\n### Splunk Apps Installation\n\nApps and add-ons can be installed using the Splunk Operator by following the instructions given at [Installing Splunk Apps](Examples.md#installing-splunk-apps).  Premium apps such as Enterprise Security and IT Service Intelligence are currently not supported.\n\n\n### Docker requirements\nThe Splunk Operator requires these docker images to be present or available to your Kubernetes cluster:\n\n* `splunk/splunk-operator`: The Splunk Operator image built by this repository or the [official release](https://hub.docker.com/r/splunk/splunk-operator) (2.0.0 or later)\n* `splunk/splunk:<version>`: The [Splunk Enterprise image](https://github.com/splunk/docker-splunk) (9.0.0 or later)\n\nAll of the Splunk Enterprise images are publicly available on [Docker Hub](https://hub.docker.com/). If your cluster does not have access to pull from Docker Hub, see the [Required Images Documentation](Images.md) page.\n\nReview the [Change Log](ChangeLog.md) page for a history of changes and Splunk Enterprise compatibility for each release.\n\n### Hardware Resources Requirements\nThe resource guidelines for running production Splunk Enterprise instances in pods through the Splunk Operator are the same as running Splunk Enterprise natively on a supported operating system and file system. Refer to the Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware) for additional details.  We would also recommend following the same guidance on [Splunk Enterprise for disabling Transparent Huge Pages (THP)](https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP) for the nodes in your Kubernetes cluster.  Please be aware that this may impact performance of other non-Splunk workloads.\n\n#### Minimum Reference Hardware\nBased on Splunk Enterprise [Reference Hardware documentation](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware), a summary of the minimum reference hardware requirements is given below.\n\n| Standalone        | Search Head / Search Head Cluster | Indexer Cluster |\n| ---------- | ------- | ------- |\n| _Each Standalone Pod: 12 Physical CPU Cores or 24 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Search Head Pod: 16 Physical CPU Cores or 32 vCPU at 2Ghz or greater per core, 12GB RAM._| _Each Indexer Pod: 12 Physical CPU cores, or 24 vCPU at 2GHz or greater per core, 12GB RAM._ |\n\n\n#### _Using Kubernetes Quality of Service Classes_\n\nIn addition to the guidelines provided in the reference hardware, [Kubernetes Quality of Service Classes](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)  can be used to configure CPU/Mem resources allocations that map to your _service level objectives_. For further information on utilizing Kubernetes Quality of Service (QoS) classes, see the table below:\n\n\n| QoS        | Summary| Description |\n| ---------- | ------- | ------- |\n| _Guaranteed_ | _CPU/Mem ```requests``` = CPU/Mem ```limits```_    | _When the CPU and memory  ```requests``` and ```limits``` values are equal, the pod is given a QoS class of Guaranteed. This level of service is recommended for Splunk Enterprise ___production environments___._ |\n| _Burstable_ | _CPU/Mem ```requests``` < CPU/Mem ```limits```_  | _When the CPU and memory  ```requests``` value is set lower than the ```limits``` the pod is given a QoS class of Burstable. This level of service is useful in a user acceptance testing ___(UAT) environment___, where the pods run with minimum resources, and Kubernetes allocates additional resources depending on usage._|\n| _BestEffort_ | _No CPU/Mem ```requests``` or ```limits``` are set_ | _When the ```requests``` or ```limits``` values are not set, the pod is given a QoS class of BestEffort. This level of service is sufficient for ___testing, or a small development task___._ |\n\nExamples on how to implement these QoS are given at [Examples of Guaranteed and Burstable QoS](CustomResources.md#examples-of-guaranteed-and-burstable-qos) section.\n\n\n### Storage guidelines\nThe Splunk Operator uses Kubernetes [Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) to store all of your Splunk Enterprise configuration (\"$SPLUNK_HOME/etc\" path) and event (\"$SPLUNK_HOME/var\" path) data. If one of the underlying machines fail, Kubernetes will automatically try to recover by restarting the Splunk Enterprise pods on another machine that is able to reuse the same data volumes. This minimizes the maintenance burden on your operations team by reducing the impact of common hardware failures to the equivalent of a service restart. \nThe use of Persistent Volume Claims requires that your cluster is configured to support one or more Kubernetes persistent [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/). See the [Setting Up a Persistent Storage for Splunk](StorageClass.md) page for more\ninformation.\n\n### What Storage Type To Use?\n\nThe Kubernetes infrastructure must have access to storage that meets or exceeds the recommendations provided in the Splunk Enterprise storage type recommendations at [Reference Hardware documentation - what storage type to use for a given role?](https://docs.splunk.com/Documentation/Splunk/latest/Capacity/Referencehardware#What_storage_type_should_I_use_for_a_role.3F) In summary, Indexers with SmartStore need NVMe or SSD storage to provide the necessary IOPs for a successful Splunk Enterprise environment.\n\n\n### Splunk SmartStore Required\nFor production environments, we are requiring the use of Splunk SmartStore. As a Splunk Enterprise deployment's data volume increases, demand for storage typically outpaces demand for compute resources. [Splunk's SmartStore Feature](https://docs.splunk.com/Documentation/Splunk/latest/Indexer/AboutSmartStore) allows you to manage your indexer storage and compute resources in a ___cost-effective___ manner by scaling those resources separately. SmartStore utilizes a fast storage cache on each indexer node to keep recent data locally available for search and keep other data in a remote object store. Look into the [SmartStore Resource Guide](SmartStore.md) document for configuring and using SmartStore through operator.\n\n## Installing the Splunk Operator\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for specific namespace by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-namespace.yaml\n```\n\nA Kubernetes cluster administrator can install and start the Splunk Operator for cluster-wide by running:\n```\nkubectl apply -f https://github.com/splunk/splunk-operator/releases/download/2.0.0/splunk-operator-cluster.yaml\n```\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\nThe [Advanced Installation Instructions](Install.md) page offers guidance for advanced configurations, including the use of private image registries, installation at cluster scope, and installing the Splunk Operator as a user who is not a Kubernetes administrator. Users of Red Hat OpenShift should review the [Red Hat OpenShift](OpenShift.md) page.\n\n*Note: We recommended that the Splunk Enterprise Docker image is copied to a private registry, or directly onto your Kubernetes workers before creating large Splunk Enterprise deployments. See the [Required Images Documentation](Images.md) page, and the [Advanced Installation Instructions](Install.md) page for guidance on working with copies of the Docker images.*\n\nAfter the Splunk Operator starts, you'll see a single pod running within your current namespace:\n```\n$ kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nsplunk-operator-75f5d4d85b-8pshn   1/1     Running   0          5s\n```\n## Upgrading the Splunk Operator\n\nFor information on upgrading the Splunk Operator, see the [How to upgrade Splunk Operator and Splunk Enterprise Deployments](SplunkOperatorUpgrade.md) page.\n\n## Creating a Splunk Enterprise deployment\n\nThe `Standalone` custom resource is used to create a single instance deployment of Splunk Enterprise. For example:\n\n1. Run the command to create a deployment named \u201cs1\u201d:\n\n\n```yaml\ncat <<EOF | kubectl apply -n splunk-operator -f -\napiVersion: enterprise.splunk.com/v3\nkind: Standalone\nmetadata:\n  name: s1\n  finalizers:\n  - enterprise.splunk.com/delete-pvc\nEOF\n```\n\n**The `enterprise.splunk.com/delete-pvc` finalizer is optional, and tells the Splunk Operator to remove any Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) associated with the instance if you delete the pod.**\n\nWithin a few minutes, you'll see new pods running in your namespace:\n\n```\n$ kubectl get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nsplunk-operator-7c5599546c-wt4xl        1/1    Running   0          11h\nsplunk-s1-standalone-0                  1/1    Running   0          45s\n```\n\n*Note: if your shell prints a `%` at the end, leave that out when you copy the output.*\n\n2. You can use a simple network port forward to open port 8000 for Splunk Web access:\n\n```\nkubectl port-forward splunk-s1-standalone-0 8000\n```\n\n3. Get your passwords for the namespace. The Splunk Enterprise passwords used in the namespace are generated automatically. To learn how to find and read the passwords, see the [Reading global kubernetes secret object](Examples.md#reading-global-kubernetes-secret-object) page.\n\n\n4. Log into Splunk Enterprise at http://localhost:8000 using the `admin` account with the password.\n\n5. To delete your standalone deployment, run:\n\n```\nkubectl delete standalone s1\n``` \n\nThe `Standalone` custom resource is just one of the resources the Splunk Operator provides. You can find more custom resources and the parameters they support on the [Custom Resource Guide](CustomResources.md) page.\n\nFor additional deployment examples, including Splunk Enterprise clusters, see the \n[Configuring Splunk Enterprise Deployments](Examples.md) page.\n\nFor additional guidance on making Splunk Enterprise ports accessible outside of Kubernetes, see the [Configuring Ingress](Ingress.md) page.\n\n## Contacting Support\nIf you are a Splunk Enterprise customer with a valid support entitlement contract and have a Splunk-related question, you can open a support case on the https://www.splunk.com/ support portal.\"\n",
      "csv_display_name": "Splunk Operator",
      "csv_metadata_description": "The Splunk Operator for Kubernetes enables you to quickly and easily deploy Splunk Enterprise on your choice of private or public cloud provider. The Operator simplifies scaling and management of Splunk Enterprise by automating administrative workflows using Kubernetes best practices.",
      "csv_name": "splunk-operator.v2.0.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:46:10.523000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "splunk-operator",
      "provided_apis": [
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "MonitoringConsole",
          "plural": "monitoringconsoles",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "SearchHeadCluster",
          "plural": "searchheadclusters",
          "version": "v1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "Standalone",
          "plural": "standalones",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1alpha1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta1"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "ClusterMaster",
          "plural": "clustermasters",
          "version": "v1beta3"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "IndexerCluster",
          "plural": "indexerclusters",
          "version": "v2"
        },
        {
          "group": "enterprise.splunk.com",
          "kind": "LicenseMaster",
          "plural": "licensemasters",
          "version": "v1beta3"
        }
      ],
      "provider": "Splunk Inc.",
      "related_images": [
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk-enterprise"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "splunk-operator-c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50-annotation"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "image": "docker.io/splunk/splunk-operator@sha256:c4e0d314622699496f675760aad314520d050a66627fdf33e1e21fa28ca85d50",
          "name": "manager"
        },
        {
          "digest": "sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "image": "docker.io/splunk/splunk@sha256:adcdf7ae7dc8033b14122404c900361d46a27ec6cf98dcf6e627ecf07fd84c0e",
          "name": "splunk_enterprise"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "2.0.0",
      "version_original": "2.0.0"
    },
    {
      "_id": "62fd55de30a80ba5a209cac4",
      "alm_examples": [
        {
          "api_version": "astra.netapp.io/v1",
          "kind": "AstraControlCenter",
          "metadata": {
            "name": "astra"
          },
          "spec": {
            "accountName": "Example",
            "additionalValues": {},
            "astraAddress": "astra.example.com",
            "astraResourcesScaler": "Default",
            "astraVersion": "22.08.0-20",
            "autoSupport": {
              "enrolled": true
            },
            "crds": {
              "externalCertManager": false,
              "externalTraefik": false
            },
            "email": "admin@example.com",
            "firstName": "SRE",
            "imageRegistry": {
              "name": "example.registry/astra",
              "secret": "astra-registry-cred"
            },
            "ingressType": "Generic",
            "lastName": "Admin",
            "storageClass": "ontap-gold",
            "volumeReclaimPolicy": "Retain"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/netapp/acc-operator-bundle@sha256:44b510c0373bc187f0d476e4566901d2cce339206c3e1498cdfb6cb67999786f",
      "bundle_path_digest": "sha256:44b510c0373bc187f0d476e4566901d2cce339206c3e1498cdfb6cb67999786f",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T20:55:58.289000+00:00",
      "csv_description": "Astra Control is an application-aware data management solution that manages, protects and moves data-rich Kubernetes workloads in both public clouds and on-premises.\n\nAstra Control enables data protection, disaster recovery, and migration for your Kubernetes workloads, leveraging NetApp's industry-leading data management technology\nfor snapshots, backups, replication and cloning.\n\n### How to deploy Astra Control\n\nRefer to [Installation Procedure](https://docs.netapp.com/us-en/astra-control-center/get-started/acc_operatorhub_install.html) to deploy Astra Control Center using\nthe Operator.\n\n### Documentation\n\nRefer to [Astra Control Center Documentation](https://docs.netapp.com/us-en/astra-control-center/index.html) to complete the setup and start managing applications.\n\nNOTE: The version listed under *Latest version* on this page might not reflect the actual version of NetApp Astra Control Center you are installing. The version in the file name of the Astra Control Center bundle that you download from the NetApp Support Site is the version of Astra Control Center that will be installed.\n",
      "csv_display_name": "netapp-acc-operator",
      "csv_metadata_description": "Install, configure and monitor Astra Control Center",
      "csv_name": "acc-operator.v22.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:01:49.981000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "acc-operator",
      "provided_apis": [
        {
          "group": "astra.netapp.io",
          "kind": "AstraControlCenter",
          "version": "v1"
        }
      ],
      "provider": "NetApp",
      "related_images": [
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:05efc8b68000e1176dcc8cc6b57c5f7eb8172dc16626595356b7e4e5befd5093",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:05efc8b68000e1176dcc8cc6b57c5f7eb8172dc16626595356b7e4e5befd5093",
          "name": "acc-operator"
        },
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:05efc8b68000e1176dcc8cc6b57c5f7eb8172dc16626595356b7e4e5befd5093",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:05efc8b68000e1176dcc8cc6b57c5f7eb8172dc16626595356b7e4e5befd5093",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "22.8.0",
      "version_original": "22.8.0"
    },
    {
      "_id": "62fd578030a80ba5a209cb1e",
      "alm_examples": [
        {
          "api_version": "astra.netapp.io/v1",
          "kind": "AstraControlCenter",
          "metadata": {
            "name": "astra"
          },
          "spec": {
            "accountName": "Example",
            "additionalValues": {},
            "astraAddress": "astra.example.com",
            "astraResourcesScaler": "Default",
            "astraVersion": "22.08.0-20",
            "autoSupport": {
              "enrolled": true
            },
            "crds": {
              "externalCertManager": false,
              "externalTraefik": false
            },
            "email": "admin@example.com",
            "firstName": "SRE",
            "imageRegistry": {
              "name": "example.registry/astra",
              "secret": "astra-registry-cred"
            },
            "ingressType": "Generic",
            "lastName": "Admin",
            "storageClass": "ontap-gold",
            "volumeReclaimPolicy": "Retain"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/netapp/acc-operator-bundle@sha256:44b510c0373bc187f0d476e4566901d2cce339206c3e1498cdfb6cb67999786f",
      "bundle_path_digest": "sha256:44b510c0373bc187f0d476e4566901d2cce339206c3e1498cdfb6cb67999786f",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-17T21:02:56.978000+00:00",
      "csv_description": "Astra Control is an application-aware data management solution that manages, protects and moves data-rich Kubernetes workloads in both public clouds and on-premises.\n\nAstra Control enables data protection, disaster recovery, and migration for your Kubernetes workloads, leveraging NetApp's industry-leading data management technology\nfor snapshots, backups, replication and cloning.\n\n### How to deploy Astra Control\n\nRefer to [Installation Procedure](https://docs.netapp.com/us-en/astra-control-center/get-started/acc_operatorhub_install.html) to deploy Astra Control Center using\nthe Operator.\n\n### Documentation\n\nRefer to [Astra Control Center Documentation](https://docs.netapp.com/us-en/astra-control-center/index.html) to complete the setup and start managing applications.\n\nNOTE: The version listed under *Latest version* on this page might not reflect the actual version of NetApp Astra Control Center you are installing. The version in the file name of the Astra Control Center bundle that you download from the NetApp Support Site is the version of Astra Control Center that will be installed.\n",
      "csv_display_name": "netapp-acc-operator",
      "csv_metadata_description": "Install, configure and monitor Astra Control Center",
      "csv_name": "acc-operator.v22.8.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:13:09.645000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "acc-operator",
      "provided_apis": [
        {
          "group": "astra.netapp.io",
          "kind": "AstraControlCenter",
          "version": "v1"
        }
      ],
      "provider": "NetApp",
      "related_images": [
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:05efc8b68000e1176dcc8cc6b57c5f7eb8172dc16626595356b7e4e5befd5093",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:05efc8b68000e1176dcc8cc6b57c5f7eb8172dc16626595356b7e4e5befd5093",
          "name": "acc-operator"
        },
        {
          "digest": "sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:7a8b4b60345cff84089835a561b4b65c5eccb4585283314c90ce2108c063c87d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:05efc8b68000e1176dcc8cc6b57c5f7eb8172dc16626595356b7e4e5befd5093",
          "image": "registry.connect.redhat.com/netapp/acc-operator-22-8-0@sha256:05efc8b68000e1176dcc8cc6b57c5f7eb8172dc16626595356b7e4e5befd5093",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "22.8.0",
      "version_original": "22.8.0"
    },
    {
      "_id": "62fe398053cb8f6da454f4b9",
      "alm_examples": [
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "HostDeviceNetwork",
          "metadata": {
            "name": "example-hostdevice-network"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.3.225/28\",\n  \"exclude\": [\n   \"192.168.3.229/30\",\n   \"192.168.3.236/32\"\n  ]\n}\n",
            "networkNamespace": "default",
            "resourceName": "hostdev"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "MacvlanNetwork",
          "metadata": {
            "name": "example-macvlannetwork"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.2.225/24\",\n  \"exclude\": [\n   \"192.168.2.229/30\",\n   \"192.168.2.236/32\"\n  ]\n}\n",
            "master": "ens2f0",
            "mode": "bridge",
            "mtu": 1500,
            "networkNamespace": "default"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "NicClusterPolicy",
          "metadata": {
            "name": "nic-cluster-policy"
          },
          "spec": {
            "ofedDriver": {
              "image": "mofed",
              "livenessProbe": {
                "initialDelaySeconds": 30,
                "periodSeconds": 30
              },
              "readinessProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 30
              },
              "repository": "mellanox",
              "startupProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 20
              },
              "version": "5.5-1.0.3.2"
            },
            "rdmaSharedDevicePlugin": {
              "config": "{\n  \"configList\": [\n    {\n      \"resourceName\": \"rdma_shared_device_a\",\n      \"rdmaHcaMax\": 1000,\n      \"selectors\": {\n        \"ifNames\": [\"ens2f0\"]\n      }\n    }\n  ]\n}\n",
              "image": "k8s-rdma-shared-dev-plugin",
              "repository": "nvcr.io/nvidia/cloud-native",
              "version": "v1.2.1-ubi"
            },
            "sriovDevicePlugin": {
              "config": "{\n  \"resourceList\": [\n      {\n          \"resourcePrefix\": \"nvidia.com\",\n          \"resourceName\": \"hostdev\",\n          \"selectors\": {\n              \"vendors\": [\"15b3\"],\n              \"isRdma\": true\n          }\n      }\n  ]\n}\n",
              "image": "sriov-device-plugin",
              "repository": "docker.io/nfvpe",
              "version": "v3.3"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia-network-operator/nvidia-network-operator@sha256:5d0554c40fd2ad6d901b0a6384f48bacb478ba22345706448fd66258b4439c3d",
      "bundle_path_digest": "sha256:5d0554c40fd2ad6d901b0a6384f48bacb478ba22345706448fd66258b4439c3d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.3.0",
      "creation_date": "2022-08-18T13:07:12.023000+00:00",
      "csv_description": "## NVIDIA Network Operator\nThe NVIDIA Network Operator simplifies the provisioning and management of NVIDIA networking resources  in a Kubernetes cluster. The operator automatically installs the required host networking software - bringing together all the needed components to provide high-speed network connectivity. These components include the NVIDIA networking driver, Kubernetes device plugin, CNI plugins, IP address management (IPAM) plugin and others.\nThe NVIDIA Network Operator works in conjunction with the NVIDIA GPU Operator to deliver high-throughput, low-latency networking for scale-out, GPU computing clusters.\n\nThe Network Operator uses Node Feature Discovery (NFD) labels in order to properly schedule resources.\nNodes can be labelled manually or using the NFD Operator. An example of `NodeFeatureDiscovery`\nconfiguration is available in the documentation.\nThe following NFD labels are used:\n`feature.node.kubernetes.io/pci-15b3.present` for nodes containing NVIDIA Networking hardware.\n`feature.node.kubernetes.io/pci-10de.present` for nodes containing NVIDIA GPU hardware.\n\nThe Network Operator is open-source. For more information on contributions and release artifacts, see the [GitHub repo](https://github.com/Mellanox/network-operator/)\n",
      "csv_display_name": "NVIDIA Network Operator",
      "csv_metadata_description": "Deploy and manage NVIDIA networking resources in Kubernetes",
      "csv_name": "nvidia-network-operator.v1.2.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:55:05.563000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "nvidia-network-operator",
      "provided_apis": [
        {
          "group": "mellanox.com",
          "kind": "HostDeviceNetwork",
          "plural": "hostdevicenetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "MacvlanNetwork",
          "plural": "macvlannetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "NicClusterPolicy",
          "plural": "nicclusterpolicies",
          "version": "v1alpha1"
        }
      ],
      "provider": "NVIDIA",
      "related_images": [
        {
          "digest": "sha256:0a5108443c64fc013984be500e7db3d89c04418446f0bbc3241ed6e1c449b773",
          "image": "nvcr.io/nvidia/mellanox/mofed-5.6-1.0.3.3@sha256:0a5108443c64fc013984be500e7db3d89c04418446f0bbc3241ed6e1c449b773",
          "name": "mofed"
        },
        {
          "digest": "sha256:16a53286fecdb1e587d3c4c042078974674c3e86c9e98d7dae282f6eb4ee2d8c",
          "image": "ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin@sha256:16a53286fecdb1e587d3c4c042078974674c3e86c9e98d7dae282f6eb4ee2d8c",
          "name": "sriov-network-device-plugin"
        },
        {
          "digest": "sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "image": "nvcr.io/nvidia/cloud-native/k8s-rdma-shared-dev-plugin@sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "name": "rdma-shared-device-plugin"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:90bd3c4d2620a7f1005afc7c1d101d31a64d2239b0bf33a701c4426a54e4e381",
          "image": "nvcr.io/nvidia/cloud-native/network-operator@sha256:90bd3c4d2620a7f1005afc7c1d101d31a64d2239b0bf33a701c4426a54e4e381",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.2.0",
      "version_original": "1.2.0"
    },
    {
      "_id": "62fe398053cb8f6da454f4bb",
      "alm_examples": [
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "HostDeviceNetwork",
          "metadata": {
            "name": "example-hostdevice-network"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.3.225/28\",\n  \"exclude\": [\n   \"192.168.3.229/30\",\n   \"192.168.3.236/32\"\n  ]\n}\n",
            "networkNamespace": "default",
            "resourceName": "hostdev"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "MacvlanNetwork",
          "metadata": {
            "name": "example-macvlannetwork"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.2.225/24\",\n  \"exclude\": [\n   \"192.168.2.229/30\",\n   \"192.168.2.236/32\"\n  ]\n}\n",
            "master": "ens2f0",
            "mode": "bridge",
            "mtu": 1500,
            "networkNamespace": "default"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "NicClusterPolicy",
          "metadata": {
            "name": "nic-cluster-policy"
          },
          "spec": {
            "ofedDriver": {
              "image": "mofed",
              "livenessProbe": {
                "initialDelaySeconds": 30,
                "periodSeconds": 30
              },
              "readinessProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 30
              },
              "repository": "nvcr.io/nvidia/mellanox",
              "startupProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 20
              },
              "version": "5.6-1.0.3.3"
            },
            "rdmaSharedDevicePlugin": {
              "config": "{\n  \"configList\": [\n    {\n      \"resourceName\": \"rdma_shared_device_a\",\n      \"rdmaHcaMax\": 1000,\n      \"selectors\": {\n        \"ifNames\": [\"ens2f0\"]\n      }\n    }\n  ]\n}\n",
              "image": "k8s-rdma-shared-dev-plugin",
              "repository": "nvcr.io/nvidia/cloud-native",
              "version": "v1.3.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia-network-operator/nvidia-network-operator@sha256:92722308ad575e31059d00ea02213dbaf1a653f8d571aeef755e1a6b6ff60fc2",
      "bundle_path_digest": "sha256:92722308ad575e31059d00ea02213dbaf1a653f8d571aeef755e1a6b6ff60fc2",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.3.0",
      "creation_date": "2022-08-18T13:07:12.473000+00:00",
      "csv_description": "## NVIDIA Network Operator\nThe NVIDIA Network Operator simplifies the provisioning and management of NVIDIA networking resources  in a Kubernetes cluster. The operator automatically installs the required host networking software - bringing together all the needed components to provide high-speed network connectivity. These components include the NVIDIA networking driver, Kubernetes device plugin, CNI plugins, IP address management (IPAM) plugin and others.\nThe NVIDIA Network Operator works in conjunction with the NVIDIA GPU Operator to deliver high-throughput, low-latency networking for scale-out, GPU computing clusters.\n\nThe Network Operator uses Node Feature Discovery (NFD) labels in order to properly schedule resources.\nNodes can be labelled manually or using the NFD Operator. An example of `NodeFeatureDiscovery`\nconfiguration is available in the documentation.\nThe following NFD labels are used:\n`feature.node.kubernetes.io/pci-15b3.present` for nodes containing NVIDIA Networking hardware.\n`feature.node.kubernetes.io/pci-10de.present` for nodes containing NVIDIA GPU hardware.\n\nThe Network Operator is open-source. For more information on contributions and release artifacts, see the [GitHub repo](https://github.com/Mellanox/network-operator/)\n",
      "csv_display_name": "NVIDIA Network Operator",
      "csv_metadata_description": "Deploy and manage NVIDIA networking resources in Kubernetes",
      "csv_name": "nvidia-network-operator.v1.2.1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:55:16.448000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "nvidia-network-operator",
      "provided_apis": [
        {
          "group": "mellanox.com",
          "kind": "HostDeviceNetwork",
          "plural": "hostdevicenetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "MacvlanNetwork",
          "plural": "macvlannetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "NicClusterPolicy",
          "plural": "nicclusterpolicies",
          "version": "v1alpha1"
        }
      ],
      "provider": "NVIDIA",
      "related_images": [
        {
          "digest": "sha256:0a5108443c64fc013984be500e7db3d89c04418446f0bbc3241ed6e1c449b773",
          "image": "nvcr.io/nvidia/mellanox/mofed-5.6-1.0.3.3@sha256:0a5108443c64fc013984be500e7db3d89c04418446f0bbc3241ed6e1c449b773",
          "name": "mofed"
        },
        {
          "digest": "sha256:16a53286fecdb1e587d3c4c042078974674c3e86c9e98d7dae282f6eb4ee2d8c",
          "image": "ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin@sha256:16a53286fecdb1e587d3c4c042078974674c3e86c9e98d7dae282f6eb4ee2d8c",
          "name": "sriov-network-device-plugin"
        },
        {
          "digest": "sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "image": "nvcr.io/nvidia/cloud-native/k8s-rdma-shared-dev-plugin@sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "name": "rdma-shared-device-plugin"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:90bd3c4d2620a7f1005afc7c1d101d31a64d2239b0bf33a701c4426a54e4e381",
          "image": "nvcr.io/nvidia/cloud-native/network-operator@sha256:90bd3c4d2620a7f1005afc7c1d101d31a64d2239b0bf33a701c4426a54e4e381",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.2.1",
      "version_original": "1.2.1"
    },
    {
      "_id": "62fe3980051dd1e2227e693d",
      "alm_examples": [
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "HostDeviceNetwork",
          "metadata": {
            "name": "example-hostdevice-network"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.3.225/28\",\n  \"exclude\": [\n   \"192.168.3.229/30\",\n   \"192.168.3.236/32\"\n  ]\n}\n",
            "networkNamespace": "default",
            "resourceName": "hostdev"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "IPoIBNetwork",
          "metadata": {
            "name": "example-ipoibnetwork"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.6.225/28\",\n  \"exclude\": [\n   \"192.168.6.229/30\",\n   \"192.168.6.236/32\"\n  ]\n}\n",
            "master": "ibs3f1",
            "networkNamespace": "default"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "MacvlanNetwork",
          "metadata": {
            "name": "example-macvlannetwork"
          },
          "spec": {
            "ipam": "{\n  \"type\": \"whereabouts\",\n  \"range\": \"192.168.2.225/24\",\n  \"exclude\": [\n   \"192.168.2.229/30\",\n   \"192.168.2.236/32\"\n  ]\n}\n",
            "master": "ens2f0",
            "mode": "bridge",
            "mtu": 1500,
            "networkNamespace": "default"
          }
        },
        {
          "api_version": "mellanox.com/v1alpha1",
          "kind": "NicClusterPolicy",
          "metadata": {
            "name": "nic-cluster-policy"
          },
          "spec": {
            "ofedDriver": {
              "image": "mofed",
              "livenessProbe": {
                "initialDelaySeconds": 30,
                "periodSeconds": 30
              },
              "readinessProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 30
              },
              "repository": "nvcr.io/nvidia/mellanox",
              "startupProbe": {
                "initialDelaySeconds": 10,
                "periodSeconds": 20
              },
              "upgradePolicy": {
                "autoUpgrade": false,
                "drain": {
                  "deleteEmptyDir": true,
                  "enable": true,
                  "force": true,
                  "podSelector": "",
                  "timeoutSeconds": 300
                },
                "maxParallelUpgrades": 1
              },
              "version": "5.7-1.0.2.0"
            },
            "rdmaSharedDevicePlugin": {
              "config": "{\n  \"configList\": [\n    {\n      \"resourceName\": \"rdma_shared_device_a\",\n      \"rdmaHcaMax\": 1000,\n      \"selectors\": {\n        \"ifNames\": [\"ens2f0\"]\n      }\n    }\n  ]\n}\n",
              "image": "k8s-rdma-shared-dev-plugin",
              "repository": "nvcr.io/nvidia/cloud-native",
              "version": "v1.3.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/nvidia-network-operator/nvidia-network-operator@sha256:a3b894586a1a11b9a076a7634ac2228c2eff2778996bb34f4d13f2f18d552e6c",
      "bundle_path_digest": "sha256:a3b894586a1a11b9a076a7634ac2228c2eff2778996bb34f4d13f2f18d552e6c",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "v1.3.0",
      "creation_date": "2022-08-18T13:07:12.912000+00:00",
      "csv_description": "## NVIDIA Network Operator\nThe NVIDIA Network Operator simplifies the provisioning and management of NVIDIA networking resources  in a Kubernetes cluster. The operator automatically installs the required host networking software - bringing together all the needed components to provide high-speed network connectivity. These components include the NVIDIA networking driver, Kubernetes device plugin, CNI plugins, IP address management (IPAM) plugin and others.\nThe NVIDIA Network Operator works in conjunction with the NVIDIA GPU Operator to deliver high-throughput, low-latency networking for scale-out, GPU computing clusters.\n\nThe Network Operator uses Node Feature Discovery (NFD) labels in order to properly schedule resources.\nNodes can be labelled manually or using the NFD Operator. An example of `NodeFeatureDiscovery`\nconfiguration is available in the documentation.\nThe following NFD labels are used:\n`feature.node.kubernetes.io/pci-15b3.present` for nodes containing NVIDIA Networking hardware.\n`feature.node.kubernetes.io/pci-10de.present` for nodes containing NVIDIA GPU hardware.\n\nThe Network Operator is open-source. For more information on contributions and release artifacts, see the [GitHub repo](https://github.com/Mellanox/network-operator/)\n",
      "csv_display_name": "NVIDIA Network Operator",
      "csv_metadata_description": "Deploy and manage NVIDIA networking resources in Kubernetes",
      "csv_name": "nvidia-network-operator.v1.3.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:55:23.754000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "nvidia-network-operator",
      "provided_apis": [
        {
          "group": "mellanox.com",
          "kind": "IPoIBNetwork",
          "plural": "ipoibnetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "MacvlanNetwork",
          "plural": "macvlannetworks",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "NicClusterPolicy",
          "plural": "nicclusterpolicies",
          "version": "v1alpha1"
        },
        {
          "group": "mellanox.com",
          "kind": "HostDeviceNetwork",
          "plural": "hostdevicenetworks",
          "version": "v1alpha1"
        }
      ],
      "provider": "NVIDIA",
      "related_images": [
        {
          "digest": "sha256:b9a6d20434443b364a4426f61cdec3fda5d793fbfb3fe6607ebeafc80d5dbbd8",
          "image": "nvcr.io/nvidia/mellanox/mofed@sha256:b9a6d20434443b364a4426f61cdec3fda5d793fbfb3fe6607ebeafc80d5dbbd8",
          "name": "mofed"
        },
        {
          "digest": "sha256:f717f9778f48665b7c592f2225df51b755a1fe048125e034a286c564ee10fd37",
          "image": "ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin@sha256:f717f9778f48665b7c592f2225df51b755a1fe048125e034a286c564ee10fd37",
          "name": "sriov-network-device-plugin"
        },
        {
          "digest": "sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "image": "nvcr.io/nvidia/cloud-native/k8s-rdma-shared-dev-plugin@sha256:941ad9ff5013e9e7ad5abeb0ea9f79d45379cfae88a628d923f87d2259bdd132",
          "name": "rdma-shared-device-plugin"
        },
        {
          "digest": "sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:db06cc4c084dd0253134f156dddaaf53ef1c3fb3cc809e5d81711baa4029ea4c",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c994ad8894deacb61ca9dcea8983376f89750681a0c185ba860c9b6c0045c28c",
          "image": "nvcr.io/nvidia/cloud-native/network-operator@sha256:c994ad8894deacb61ca9dcea8983376f89750681a0c185ba860c9b6c0045c28c",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.3.0",
      "version_original": "1.3.0"
    },
    {
      "_id": "62fe748153cb8f6da455025f",
      "alm_examples": [
        {
          "api_version": "anzounstructured.clusters.cambridgesemantics.com/v1",
          "kind": "AnzoUnstructured",
          "metadata": {
            "name": "au01"
          },
          "spec": {
            "auWorker": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app": "anzounstructured"
                    }
                  },
                  "serviceName": "au-au01-w",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app": "anzounstructured"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
                          "name": "w",
                          "resources": {
                            "limits": {
                              "cpu": "2",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "unstructured-operator"
                    }
                  }
                }
              }
            },
            "msLeader": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app": "anzounstructured"
                    }
                  },
                  "serviceName": "au-au01-ms",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app": "anzounstructured"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
                          "name": "ms",
                          "resources": {
                            "limits": {
                              "cpu": "2",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "unstructured-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-operator-bundle@sha256:cf67a40d08722cbd7acb326e5ce371b56a1c40db721bdc34572c55007a6a0797",
      "bundle_path_digest": "sha256:cf67a40d08722cbd7acb326e5ce371b56a1c40db721bdc34572c55007a6a0797",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-18T17:18:57.042000+00:00",
      "csv_description": "The Anzo Unstructured Operator provides the way to install and configure an anzo unstructured setup on Red Hat K8S environment.\nCurrently, this is possible only through existing Anzo installation.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzo-unstructured/blob/v2.0.2/README.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzo/userdoc/cloud-deployments.htm )\n\n### Support\n\nWe offer Support to our customers through [ Cambridge Semantics Customer Center ]( https://customercenter.cambridgesemantics.com/ ).",
      "csv_display_name": "Anzo Unstructured Operator",
      "csv_metadata_description": "Kubernetes Operator for Anzo Unstructured",
      "csv_name": "anzounstructured-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:27:38.953000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "anzounstructured-operator",
      "provided_apis": [
        {
          "group": "anzounstructured.clusters.cambridgesemantics.com",
          "kind": "AnzoUnstructured",
          "plural": "anzounstructureds",
          "version": "v1"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo-microservices-leader"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo-unstructured-worker"
        },
        {
          "digest": "sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "image": "registry.connect.redhat.com/cambridgesemantics/unstructured-operator@sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "name": "unstructured-operator-991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446-annotation"
        },
        {
          "digest": "sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "image": "registry.connect.redhat.com/cambridgesemantics/unstructured-operator@sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "name": "manager"
        },
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo_microservices_leader"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo_unstructured_worker"
        },
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo-unstructured-leader-0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f-annotation"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo-unstructured-worker-7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "2.0.202",
      "version_original": "2.0.202"
    },
    {
      "_id": "62fe760e051dd1e2227e773d",
      "alm_examples": [
        {
          "api_version": "anzounstructured.clusters.cambridgesemantics.com/v1",
          "kind": "AnzoUnstructured",
          "metadata": {
            "name": "au01"
          },
          "spec": {
            "auWorker": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app": "anzounstructured"
                    }
                  },
                  "serviceName": "au-au01-w",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app": "anzounstructured"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
                          "name": "w",
                          "resources": {
                            "limits": {
                              "cpu": "2",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "unstructured-operator"
                    }
                  }
                }
              }
            },
            "msLeader": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app": "anzounstructured"
                    }
                  },
                  "serviceName": "au-au01-ms",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app": "anzounstructured"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
                          "name": "ms",
                          "resources": {
                            "limits": {
                              "cpu": "2",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "unstructured-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-operator-bundle@sha256:cf67a40d08722cbd7acb326e5ce371b56a1c40db721bdc34572c55007a6a0797",
      "bundle_path_digest": "sha256:cf67a40d08722cbd7acb326e5ce371b56a1c40db721bdc34572c55007a6a0797",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-18T17:25:34.110000+00:00",
      "csv_description": "The Anzo Unstructured Operator provides the way to install and configure an anzo unstructured setup on Red Hat K8S environment.\nCurrently, this is possible only through existing Anzo installation.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzo-unstructured/blob/v2.0.2/README.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzo/userdoc/cloud-deployments.htm )\n\n### Support\n\nWe offer Support to our customers through [ Cambridge Semantics Customer Center ]( https://customercenter.cambridgesemantics.com/ ).",
      "csv_display_name": "Anzo Unstructured Operator",
      "csv_metadata_description": "Kubernetes Operator for Anzo Unstructured",
      "csv_name": "anzounstructured-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:21:39.430000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "anzounstructured-operator",
      "provided_apis": [
        {
          "group": "anzounstructured.clusters.cambridgesemantics.com",
          "kind": "AnzoUnstructured",
          "plural": "anzounstructureds",
          "version": "v1"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo-microservices-leader"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo-unstructured-worker"
        },
        {
          "digest": "sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "image": "registry.connect.redhat.com/cambridgesemantics/unstructured-operator@sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "name": "unstructured-operator-991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446-annotation"
        },
        {
          "digest": "sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "image": "registry.connect.redhat.com/cambridgesemantics/unstructured-operator@sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "name": "manager"
        },
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo_microservices_leader"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo_unstructured_worker"
        },
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo-unstructured-leader-0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f-annotation"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo-unstructured-worker-7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.0.202",
      "version_original": "2.0.202"
    },
    {
      "_id": "62fe761bdca7d12fdf8dc0d5",
      "alm_examples": [
        {
          "api_version": "anzounstructured.clusters.cambridgesemantics.com/v1",
          "kind": "AnzoUnstructured",
          "metadata": {
            "name": "au01"
          },
          "spec": {
            "auWorker": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app": "anzounstructured"
                    }
                  },
                  "serviceName": "au-au01-w",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app": "anzounstructured"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
                          "name": "w",
                          "resources": {
                            "limits": {
                              "cpu": "2",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "unstructured-operator"
                    }
                  }
                }
              }
            },
            "msLeader": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app": "anzounstructured"
                    }
                  },
                  "serviceName": "au-au01-ms",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app": "anzounstructured"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
                          "name": "ms",
                          "resources": {
                            "limits": {
                              "cpu": "2",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "unstructured-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-operator-bundle@sha256:cf67a40d08722cbd7acb326e5ce371b56a1c40db721bdc34572c55007a6a0797",
      "bundle_path_digest": "sha256:cf67a40d08722cbd7acb326e5ce371b56a1c40db721bdc34572c55007a6a0797",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-18T17:25:47.218000+00:00",
      "csv_description": "The Anzo Unstructured Operator provides the way to install and configure an anzo unstructured setup on Red Hat K8S environment.\nCurrently, this is possible only through existing Anzo installation.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzo-unstructured/blob/v2.0.2/README.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzo/userdoc/cloud-deployments.htm )\n\n### Support\n\nWe offer Support to our customers through [ Cambridge Semantics Customer Center ]( https://customercenter.cambridgesemantics.com/ ).",
      "csv_display_name": "Anzo Unstructured Operator",
      "csv_metadata_description": "Kubernetes Operator for Anzo Unstructured",
      "csv_name": "anzounstructured-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:01:18.178000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "anzounstructured-operator",
      "provided_apis": [
        {
          "group": "anzounstructured.clusters.cambridgesemantics.com",
          "kind": "AnzoUnstructured",
          "plural": "anzounstructureds",
          "version": "v1"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo-microservices-leader"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo-unstructured-worker"
        },
        {
          "digest": "sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "image": "registry.connect.redhat.com/cambridgesemantics/unstructured-operator@sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "name": "unstructured-operator-991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446-annotation"
        },
        {
          "digest": "sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "image": "registry.connect.redhat.com/cambridgesemantics/unstructured-operator@sha256:991705873d77bfea28f2c92e571f29597e17eb59f879ab898301261a7e6fa446",
          "name": "manager"
        },
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo_microservices_leader"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo_unstructured_worker"
        },
        {
          "digest": "sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-leader@sha256:0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f",
          "name": "anzo-unstructured-leader-0d0e0a92af2dd71d4bce81bf1d7016ad128a245bc94091d632ff0471e1ae037f-annotation"
        },
        {
          "digest": "sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzo-unstructured-worker@sha256:7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4",
          "name": "anzo-unstructured-worker-7422d8e0d74ecfa5bfff06dfaf84961308b3b3e1740d80ec9cf71b2f2511f1f4-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "2.0.202",
      "version_original": "2.0.202"
    },
    {
      "_id": "62fe7979f49810331c93798a",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-18T17:40:09.179000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:18:20.486000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe797a53cb8f6da4550340",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "unstable",
      "creation_date": "2022-08-18T17:40:10.534000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:17:44.239000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe79fe07f6a613524f8e17",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-18T17:42:22.944000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:45:57.582000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe79ffdca7d12fdf8dc16d",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "unstable",
      "creation_date": "2022-08-18T17:42:23.548000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T11:46:01.395000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe7a5707f6a613524f8e30",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-18T17:43:51.128000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:49:59.308000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe7a57dca7d12fdf8dc18b",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "unstable",
      "creation_date": "2022-08-18T17:43:51.800000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T11:50:04.795000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe7ab8051dd1e2227e77f4",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "unstable",
      "creation_date": "2022-08-18T17:45:28.372000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T11:47:37.944000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe7ab9dca7d12fdf8dc1a0",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-18T17:45:29.523000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:22:39.602000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe7bdbf49810331c9379cf",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "unstable",
      "creation_date": "2022-08-18T17:50:19.904000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:12:18.058000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62fe7bdc53cb8f6da455039f",
      "alm_examples": [
        {
          "api_version": "apps.gitlab.com/v1beta1",
          "kind": "GitLab",
          "metadata": {
            "name": "gitlab",
            "namespace": "gitlab-system"
          },
          "spec": {
            "chart": {
              "values": {
                "certmanager": {
                  "install": false
                },
                "global": {
                  "hosts": {
                    "domain": "example.com"
                  },
                  "ingress": {
                    "configureCertmanager": false,
                    "tls": {}
                  }
                }
              },
              "version": "6.2.2"
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/gitlab/gitlab-operator-bundle@sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "bundle_path_digest": "sha256:ed0b672abe5cc0bead650973485bb7a5ecef7eb8062e1e65b2da6a9cf7ef561b",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-18T17:50:20.412000+00:00",
      "csv_description": "# Overview\n\nThe GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.\n\n[Documentation](https://docs.gitlab.com/charts/installation/operator.html)\n\nThe operator, while new and still actively being developed, aims to:\n- ease installation and configuration of GitLab instances\n- offer seamless upgrades from version to version\n\n## GitLab\n\nGitLab is a complete open-source DevOps platform, delivered as a single application, fundamentally changing the way Development, Security, and Ops teams collaborate and build software. From idea to production, GitLab helps teams improve cycle time from weeks to minutes, reduce development process costs and decrease time to market while increasing developer productivity.\n\nBuilt on Open Source, GitLab delivers new innovations and features on the same day of every month by leveraging contributions from a passionate, global community of thousands of developers and millions of users. Over 100,000 of the world\u2019s most demanding organizations trust GitLab to deliver great software at new speeds.\n\nIf you would like to enable advanced DevOps capabilities and activate enterprise features such as security, risk, and compliance capabilities, please contact our sales team to purchase an enterprise license.\n\n# Prerequisites\n\nPlease visit [Prerequisites](https://docs.gitlab.com/charts/installation/operator.html#prerequisites) section of GitLab Operator Documentation.\n\n## IngressClass\n\nCluster-wide `IngressClass` should be created prior to Operator setup, as OLM does not currently support this object type:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  # Ensure this value matches `spec.chart.values.global.ingress.class`\n  # in the GitLab CR on the next step.\n  name: gitlab-nginx\nspec:\n  controller: k8s.io/ingress-nginx\n```\n",
      "csv_display_name": "GitLab",
      "csv_metadata_description": "The GitLab operator is responsible for managing the full lifecycle of GitLab instances in your Kubernetes or Openshift container platforms.",
      "csv_name": "gitlab-operator-kubernetes.v0.10.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:40:38.565000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "gitlab-operator-kubernetes",
      "provided_apis": [
        {
          "group": "apps.gitlab.com",
          "kind": "GitLab",
          "version": "v1beta1"
        }
      ],
      "provider": "GitLab Inc",
      "related_images": [
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "cloud-native/gitlab-operator-04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97-annotation"
        },
        {
          "digest": "sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "image": "registry.gitlab.com/gitlab-org/cloud-native/gitlab-operator@sha256:04b049c54017737f41e14ca1ffdf80aed3d175f1b743a24fc87350d14b83de97",
          "name": "manager"
        },
        {
          "digest": "sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "image": "gcr.io/kubebuilder/kube-rbac-proxy@sha256:e10d1d982dd653db74ca87a1d1ad017bc5ef1aeb651bdea089debf16485b080b",
          "name": "kube-rbac-proxy"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.10.2",
      "version_original": "0.10.2"
    },
    {
      "_id": "62ffdf5a051dd1e2227ebc73",
      "alm_examples": [
        {
          "api_version": "cilium.io/v1alpha1",
          "kind": "CiliumConfig",
          "metadata": {
            "name": "cilium-openshift-default",
            "namespace": "placeholder"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/isovalent/cilium-ee-olm-metadata@sha256:8ec87e78e7bb6c6983d9b84f2cf82c084aea33e8ebbd8e40e26356f1a6d53d36",
      "bundle_path_digest": "sha256:8ec87e78e7bb6c6983d9b84f2cf82c084aea33e8ebbd8e40e26356f1a6d53d36",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "1.11",
      "creation_date": "2022-08-19T19:07:06.767000+00:00",
      "csv_description": "Cilium Enterprise - eBPF-based Networking, Security, and Observability",
      "csv_display_name": "Cilium Enterprise",
      "csv_metadata_description": "",
      "csv_name": "cilium.v1.11.8-x9da5374",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:02:56.832000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "cilium-enterprise",
      "provided_apis": [
        {
          "group": "cilium.io",
          "kind": "CiliumConfig",
          "version": "v1alpha1"
        }
      ],
      "provider": "Isovalent",
      "related_images": [
        {
          "digest": "sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "image": "quay.io/isovalent/cilium@sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "name": "cilium"
        },
        {
          "digest": "sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "image": "quay.io/isovalent/hubble-relay@sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "name": "hubble-relay"
        },
        {
          "digest": "sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "image": "quay.io/isovalent/operator-generic@sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "name": "cilium-operator"
        },
        {
          "digest": "sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "image": "quay.io/isovalent/cilium@sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "name": "preflight"
        },
        {
          "digest": "sha256:856339f0735c08f8ab7cc229e8d52e4b881a020128777b17527f0e62de990ed1",
          "image": "quay.io/isovalent/clustermesh-apiserver@sha256:856339f0735c08f8ab7cc229e8d52e4b881a020128777b17527f0e62de990ed1",
          "name": "clustermesh"
        },
        {
          "digest": "sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "image": "quay.io/cilium/certgen@sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "name": "certgen"
        },
        {
          "digest": "sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "name": "hubble-ui-backend"
        },
        {
          "digest": "sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "name": "hubble-ui-frontend"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble-ui-oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd-operator"
        },
        {
          "digest": "sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "image": "quay.io/cilium/startup-script@sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "name": "nodeinit"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh-etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble-enterprise"
        },
        {
          "digest": "sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "name": "hubble-enterprise-metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble-enterprise-operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble-export"
        },
        {
          "digest": "sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "name": "hubble-export-fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble-export-s3"
        },
        {
          "digest": "sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "image": "quay.io/isovalent/hubble-rbac@sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "name": "hubble-rbac"
        },
        {
          "digest": "sha256:e88657808b3e3f3d43fb1b8051f8c1dffb39879ce134eb0389a8e4340e7ad11b",
          "image": "registry.connect.redhat.com/isovalent/cilium-ee-olm@sha256:e88657808b3e3f3d43fb1b8051f8c1dffb39879ce134eb0389a8e4340e7ad11b",
          "name": "operator"
        },
        {
          "digest": "sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "image": "quay.io/isovalent/hubble-relay@sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "name": "hubble_relay"
        },
        {
          "digest": "sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "image": "quay.io/isovalent/operator-generic@sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "name": "cilium_operator"
        },
        {
          "digest": "sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "name": "hubble_ui_be"
        },
        {
          "digest": "sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "name": "hubble_ui_fe"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble_ui_oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd_operator"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh_etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble_enterprise"
        },
        {
          "digest": "sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "name": "hubble_enterprise_metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble_enterprise_operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble_export"
        },
        {
          "digest": "sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "name": "hubble_export_fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble_export_s3"
        },
        {
          "digest": "sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "image": "quay.io/isovalent/hubble-rbac@sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "name": "hubble_rbac"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.11.8+x9da5374",
      "version_original": "1.11.8+x9da5374"
    },
    {
      "_id": "62ffdf707892413c524f82b0",
      "alm_examples": [
        {
          "api_version": "cilium.io/v1alpha1",
          "kind": "CiliumConfig",
          "metadata": {
            "name": "cilium-openshift-default",
            "namespace": "placeholder"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/isovalent/cilium-ee-olm-metadata@sha256:8ec87e78e7bb6c6983d9b84f2cf82c084aea33e8ebbd8e40e26356f1a6d53d36",
      "bundle_path_digest": "sha256:8ec87e78e7bb6c6983d9b84f2cf82c084aea33e8ebbd8e40e26356f1a6d53d36",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "1.11",
      "creation_date": "2022-08-19T19:07:28.484000+00:00",
      "csv_description": "Cilium Enterprise - eBPF-based Networking, Security, and Observability",
      "csv_display_name": "Cilium Enterprise",
      "csv_metadata_description": "",
      "csv_name": "cilium.v1.11.8-x9da5374",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:42:20.832000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "cilium-enterprise",
      "provided_apis": [
        {
          "group": "cilium.io",
          "kind": "CiliumConfig",
          "plural": "ciliumconfigs",
          "version": "v1alpha1"
        }
      ],
      "provider": "Isovalent",
      "related_images": [
        {
          "digest": "sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "image": "quay.io/isovalent/cilium@sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "name": "cilium"
        },
        {
          "digest": "sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "image": "quay.io/isovalent/hubble-relay@sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "name": "hubble-relay"
        },
        {
          "digest": "sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "image": "quay.io/isovalent/operator-generic@sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "name": "cilium-operator"
        },
        {
          "digest": "sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "image": "quay.io/isovalent/cilium@sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "name": "preflight"
        },
        {
          "digest": "sha256:856339f0735c08f8ab7cc229e8d52e4b881a020128777b17527f0e62de990ed1",
          "image": "quay.io/isovalent/clustermesh-apiserver@sha256:856339f0735c08f8ab7cc229e8d52e4b881a020128777b17527f0e62de990ed1",
          "name": "clustermesh"
        },
        {
          "digest": "sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "image": "quay.io/cilium/certgen@sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "name": "certgen"
        },
        {
          "digest": "sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "name": "hubble-ui-backend"
        },
        {
          "digest": "sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "name": "hubble-ui-frontend"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble-ui-oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd-operator"
        },
        {
          "digest": "sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "image": "quay.io/cilium/startup-script@sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "name": "nodeinit"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh-etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble-enterprise"
        },
        {
          "digest": "sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "name": "hubble-enterprise-metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble-enterprise-operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble-export"
        },
        {
          "digest": "sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "name": "hubble-export-fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble-export-s3"
        },
        {
          "digest": "sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "image": "quay.io/isovalent/hubble-rbac@sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "name": "hubble-rbac"
        },
        {
          "digest": "sha256:e88657808b3e3f3d43fb1b8051f8c1dffb39879ce134eb0389a8e4340e7ad11b",
          "image": "registry.connect.redhat.com/isovalent/cilium-ee-olm@sha256:e88657808b3e3f3d43fb1b8051f8c1dffb39879ce134eb0389a8e4340e7ad11b",
          "name": "operator"
        },
        {
          "digest": "sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "image": "quay.io/isovalent/hubble-relay@sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "name": "hubble_relay"
        },
        {
          "digest": "sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "image": "quay.io/isovalent/operator-generic@sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "name": "cilium_operator"
        },
        {
          "digest": "sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "name": "hubble_ui_be"
        },
        {
          "digest": "sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "name": "hubble_ui_fe"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble_ui_oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd_operator"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh_etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble_enterprise"
        },
        {
          "digest": "sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "name": "hubble_enterprise_metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble_enterprise_operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble_export"
        },
        {
          "digest": "sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "name": "hubble_export_fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble_export_s3"
        },
        {
          "digest": "sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "image": "quay.io/isovalent/hubble-rbac@sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "name": "hubble_rbac"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.11.8+x9da5374",
      "version_original": "1.11.8+x9da5374"
    },
    {
      "_id": "62ffe2f6051dd1e2227ebcb0",
      "alm_examples": [
        {
          "api_version": "cilium.io/v1alpha1",
          "kind": "CiliumConfig",
          "metadata": {
            "name": "cilium-openshift-default",
            "namespace": "placeholder"
          },
          "spec": {}
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/isovalent/cilium-ee-olm-metadata@sha256:8ec87e78e7bb6c6983d9b84f2cf82c084aea33e8ebbd8e40e26356f1a6d53d36",
      "bundle_path_digest": "sha256:8ec87e78e7bb6c6983d9b84f2cf82c084aea33e8ebbd8e40e26356f1a6d53d36",
      "capabilities": [
        "Seamless Upgrades"
      ],
      "channel_name": "1.11",
      "creation_date": "2022-08-19T19:22:30.655000+00:00",
      "csv_description": "Cilium Enterprise - eBPF-based Networking, Security, and Observability",
      "csv_display_name": "Cilium Enterprise",
      "csv_metadata_description": "",
      "csv_name": "cilium.v1.11.8-x9da5374",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:45:19.352000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "cilium-enterprise",
      "provided_apis": [
        {
          "group": "cilium.io",
          "kind": "CiliumConfig",
          "plural": "ciliumconfigs",
          "version": "v1alpha1"
        }
      ],
      "provider": "Isovalent",
      "related_images": [
        {
          "digest": "sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "image": "quay.io/isovalent/cilium@sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "name": "cilium"
        },
        {
          "digest": "sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "image": "quay.io/isovalent/hubble-relay@sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "name": "hubble-relay"
        },
        {
          "digest": "sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "image": "quay.io/isovalent/operator-generic@sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "name": "cilium-operator"
        },
        {
          "digest": "sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "image": "quay.io/isovalent/cilium@sha256:07cfd3b32dea33bd4143f0ecc577d546fdc0e95ee9cd63d68826e2089e5a304d",
          "name": "preflight"
        },
        {
          "digest": "sha256:856339f0735c08f8ab7cc229e8d52e4b881a020128777b17527f0e62de990ed1",
          "image": "quay.io/isovalent/clustermesh-apiserver@sha256:856339f0735c08f8ab7cc229e8d52e4b881a020128777b17527f0e62de990ed1",
          "name": "clustermesh"
        },
        {
          "digest": "sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "image": "quay.io/cilium/certgen@sha256:0c2b71bb3469990e7990e7e26243617aa344b5a69a4ce465740b8577f9d48ab9",
          "name": "certgen"
        },
        {
          "digest": "sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "name": "hubble-ui-backend"
        },
        {
          "digest": "sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "name": "hubble-ui-frontend"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble-ui-oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd-operator"
        },
        {
          "digest": "sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "image": "quay.io/cilium/startup-script@sha256:1daf817f34000399fcb5da9a94cb299e2810d2c7a52e51de22ba0d4783b6ce84",
          "name": "nodeinit"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh-etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble-enterprise"
        },
        {
          "digest": "sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "name": "hubble-enterprise-metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble-enterprise-operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble-export"
        },
        {
          "digest": "sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "name": "hubble-export-fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble-export-s3"
        },
        {
          "digest": "sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "image": "quay.io/isovalent/hubble-rbac@sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "name": "hubble-rbac"
        },
        {
          "digest": "sha256:e88657808b3e3f3d43fb1b8051f8c1dffb39879ce134eb0389a8e4340e7ad11b",
          "image": "registry.connect.redhat.com/isovalent/cilium-ee-olm@sha256:e88657808b3e3f3d43fb1b8051f8c1dffb39879ce134eb0389a8e4340e7ad11b",
          "name": "operator"
        },
        {
          "digest": "sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "image": "quay.io/isovalent/hubble-relay@sha256:f4d9ce3a078dbebf93c9e5fb7d594d72cf9e7c4b52b4cfc912d87cfdb76a1e0b",
          "name": "hubble_relay"
        },
        {
          "digest": "sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "image": "quay.io/isovalent/operator-generic@sha256:948628cbf397ee14eda014e8a0d7ca06db9957f7ac47e6623a0480fe79b0ac40",
          "name": "cilium_operator"
        },
        {
          "digest": "sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "image": "quay.io/isovalent/hubble-ui-enterprise-backend@sha256:20364df292b5047f14c48805e6a62918bd32d7f2a36f34480cafc8c9ea3a5c9a",
          "name": "hubble_ui_be"
        },
        {
          "digest": "sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "image": "quay.io/isovalent/hubble-ui-enterprise@sha256:7af8f63d1b3844d65606374fbd770e09b4548915bb3d556a358d2b21aeb4bed1",
          "name": "hubble_ui_fe"
        },
        {
          "digest": "sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "image": "quay.io/oauth2-proxy/oauth2-proxy@sha256:ecd26b74a01f2b547ddaed4d32d35f8f5e09c378d5c1fc6cfa63f0adf659ac2b",
          "name": "hubble_ui_oauth"
        },
        {
          "digest": "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "image": "quay.io/cilium/cilium-etcd-operator@sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc",
          "name": "etcd_operator"
        },
        {
          "digest": "sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "image": "quay.io/coreos/etcd@sha256:04833b601fa130512450afa45c4fe484fee1293634f34c7ddc231bd193c74017",
          "name": "clustermesh_etcd"
        },
        {
          "digest": "sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "image": "quay.io/isovalent/hubble-enterprise@sha256:1e1d3e6c199546b2398f4166ff64210c04075bb407c8f07c5be8d3db08560d4a",
          "name": "hubble_enterprise"
        },
        {
          "digest": "sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "image": "quay.io/isovalent/hubble-enterprise-metadata@sha256:72c7aa3961f14df413569cd156820a45553bb24d393b11246a18bf8b6553e022",
          "name": "hubble_enterprise_metadata"
        },
        {
          "digest": "sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "image": "quay.io/isovalent/hubble-enterprise-operator@sha256:0d5e6a5d9c07780c18e5aa81d8f1c95c399bff2d36a44fa5f61ac89a788ca83b",
          "name": "hubble_enterprise_operator"
        },
        {
          "digest": "sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "image": "quay.io/cilium/hubble-export-stdout@sha256:f1b1475a3c89950481264f5f8ca63eea3b911a45dc83fdf15122d695a0fba9b3",
          "name": "hubble_export"
        },
        {
          "digest": "sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "image": "quay.io/isovalent/hubble-export-fluentd@sha256:eaf42a84821509b69e2ef2b658796062e79a90e63045a98689da3da99a12e492",
          "name": "hubble_export_fluentd"
        },
        {
          "digest": "sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "image": "quay.io/isovalent/hubble-export-s3@sha256:ce4f71f7f13301133b7d471b87fb81108b2c2da7fc29865ef96bc8431b2be72b",
          "name": "hubble_export_s3"
        },
        {
          "digest": "sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "image": "quay.io/isovalent/hubble-rbac@sha256:9ae9187c25b1d49e9338c6ba4c73be18adee551da2bb9a342602d5060103ae11",
          "name": "hubble_rbac"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.11.8+x9da5374",
      "version_original": "1.11.8+x9da5374"
    },
    {
      "_id": "62fffbe5dca7d12fdf8e0c78",
      "alm_examples": [
        {
          "api_version": "certification.redhat.com/v1alpha1",
          "kind": "OperatorPipeline",
          "metadata": {
            "name": "operatorpipeline-sample"
          },
          "spec": {
            "applyCIPipeline": true,
            "applyHostedPipeline": false,
            "applyReleasePipeline": false,
            "gitHubSecretName": "github-api-token",
            "kubeconfigSecretName": "kubeconfig",
            "operatorPipelinesRelease": "main",
            "pyxisSecretName": "pyxis-api-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/opdev/operator-certification-operator-bundle@sha256:5434186d5c4dcd3d6a664fb469c17d336b9ef58836f89446ea5130a335ae214d",
      "bundle_path_digest": "sha256:5434186d5c4dcd3d6a664fb469c17d336b9ef58836f89446ea5130a335ae214d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-19T21:08:53.126000+00:00",
      "csv_description": "A Kubernetes operator to provision resources for the operator certification pipeline. This operator is installed in all namespaces which can support multi-tenant scenarios. **Note:** This operator should only be used by Red Hat partners attempting to certify their operator(s).\n\n\n**Requirements**\n\n\nThe certification operator requires that you have the following tools installed, functional, and in your path.\n- [Install](https://docs.openshift.com/container-platform/4.8/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli) oc, the OpenShift CLI tool (tested with version 4.7.13)\n- [Install](https://tekton.dev/docs/cli/) tkn, the Tekton CLI tool (tested with version 0.19.1)\n- [Install](https://git-scm.com/downloads) git, the Git CLI tool (tested with 2.32.0)\n- The certification pipeline expects you to have the source files of your Operator bundle\n\n\n**Pre - Installation**\n\n\nThe below steps exist for simplicity and to make the installation clearer. The operator watches all namespaces, so if secrets/configs/etc already exist in another namespace, feel free to use the existing namespace when following the operator installation steps.\n\n\n**Create a new namespace where the following secrets will be applied.**\n```\noc new-project oco\n```\n\n\n**Add the kubeconfig secret which will be used to deploy the operator under test and run the certification checks.**\n* Open a terminal window\n* Set the `KUBECONFIG` environment variable\n```\nexport KUBECONFIG=/path/to/your/cluster/kubeconfig\n```\n> *This kubeconfig will be used to deploy the Operator under test and run the certification checks.*\n```\noc create secret generic kubeconfig --from-file=kubeconfig=$KUBECONFIG\n```\n\n\n**Configuring steps for submitting the results**\n- Add the github API token to the repo where the PR will be created\n```\noc create secret generic github-api-token --from-literal GITHUB_TOKEN=<github token>\n```\n- Add RedHat Container API access key\n\n  This API access key is specifically related to your unique partner account for Red Hat Connect portal. Instructions to obtain your API key can be found: [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/operator-cert-workflow.md#step-b---get-api-key)\n```\noc create secret generic pyxis-api-secret --from-literal pyxis_api_key=< API KEY >\n```\n\n- Optional pipeline configurations can be found [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/ci-pipeline.md#optional-configuration)\n\n\n**Execute the Pipeline (Development Iterations)**\n\n\nA pre-requisite to running a pipeline is that a `workspace-template.yaml` exists in the directory you want to execute the `tkn` commands from.\n\nTo create a workspace-template.yaml\n```\ncat <<EOF > workspace-template.yaml\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nEOF\n```\n\nThere are multiple ways to execute the Pipeline which can be found [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/ci-pipeline.md#execute-the-pipeline-development-iterations)",
      "csv_display_name": "Operator Certification Operator",
      "csv_metadata_description": "",
      "csv_name": "operator-certification-operator.v1.0.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:12:04.446000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "operator-certification-operator",
      "provided_apis": [
        {
          "group": "certification.redhat.com",
          "kind": "OperatorPipeline",
          "version": "v1alpha1"
        }
      ],
      "provider": "Red Hat",
      "related_images": [
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:9ed39145a27b56c2465f5076f2cb40fe7d84c8914908e4610ee5b5f9dbf90a58",
          "image": "quay.io/opdev/operator-certification-operator@sha256:9ed39145a27b56c2465f5076f2cb40fe7d84c8914908e4610ee5b5f9dbf90a58",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "1.0.3",
      "version_original": "1.0.3"
    },
    {
      "_id": "62fffc84051dd1e2227ec3fb",
      "alm_examples": [
        {
          "api_version": "certification.redhat.com/v1alpha1",
          "kind": "OperatorPipeline",
          "metadata": {
            "name": "operatorpipeline-sample"
          },
          "spec": {
            "applyCIPipeline": true,
            "applyHostedPipeline": false,
            "applyReleasePipeline": false,
            "gitHubSecretName": "github-api-token",
            "kubeconfigSecretName": "kubeconfig",
            "operatorPipelinesRelease": "main",
            "pyxisSecretName": "pyxis-api-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/opdev/operator-certification-operator-bundle@sha256:5434186d5c4dcd3d6a664fb469c17d336b9ef58836f89446ea5130a335ae214d",
      "bundle_path_digest": "sha256:5434186d5c4dcd3d6a664fb469c17d336b9ef58836f89446ea5130a335ae214d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-19T21:11:32.190000+00:00",
      "csv_description": "A Kubernetes operator to provision resources for the operator certification pipeline. This operator is installed in all namespaces which can support multi-tenant scenarios. **Note:** This operator should only be used by Red Hat partners attempting to certify their operator(s).\n\n\n**Requirements**\n\n\nThe certification operator requires that you have the following tools installed, functional, and in your path.\n- [Install](https://docs.openshift.com/container-platform/4.8/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli) oc, the OpenShift CLI tool (tested with version 4.7.13)\n- [Install](https://tekton.dev/docs/cli/) tkn, the Tekton CLI tool (tested with version 0.19.1)\n- [Install](https://git-scm.com/downloads) git, the Git CLI tool (tested with 2.32.0)\n- The certification pipeline expects you to have the source files of your Operator bundle\n\n\n**Pre - Installation**\n\n\nThe below steps exist for simplicity and to make the installation clearer. The operator watches all namespaces, so if secrets/configs/etc already exist in another namespace, feel free to use the existing namespace when following the operator installation steps.\n\n\n**Create a new namespace where the following secrets will be applied.**\n```\noc new-project oco\n```\n\n\n**Add the kubeconfig secret which will be used to deploy the operator under test and run the certification checks.**\n* Open a terminal window\n* Set the `KUBECONFIG` environment variable\n```\nexport KUBECONFIG=/path/to/your/cluster/kubeconfig\n```\n> *This kubeconfig will be used to deploy the Operator under test and run the certification checks.*\n```\noc create secret generic kubeconfig --from-file=kubeconfig=$KUBECONFIG\n```\n\n\n**Configuring steps for submitting the results**\n- Add the github API token to the repo where the PR will be created\n```\noc create secret generic github-api-token --from-literal GITHUB_TOKEN=<github token>\n```\n- Add RedHat Container API access key\n\n  This API access key is specifically related to your unique partner account for Red Hat Connect portal. Instructions to obtain your API key can be found: [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/operator-cert-workflow.md#step-b---get-api-key)\n```\noc create secret generic pyxis-api-secret --from-literal pyxis_api_key=< API KEY >\n```\n\n- Optional pipeline configurations can be found [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/ci-pipeline.md#optional-configuration)\n\n\n**Execute the Pipeline (Development Iterations)**\n\n\nA pre-requisite to running a pipeline is that a `workspace-template.yaml` exists in the directory you want to execute the `tkn` commands from.\n\nTo create a workspace-template.yaml\n```\ncat <<EOF > workspace-template.yaml\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nEOF\n```\n\nThere are multiple ways to execute the Pipeline which can be found [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/ci-pipeline.md#execute-the-pipeline-development-iterations)",
      "csv_display_name": "Operator Certification Operator",
      "csv_metadata_description": "",
      "csv_name": "operator-certification-operator.v1.0.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:55:59.445000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "operator-certification-operator",
      "provided_apis": [
        {
          "group": "certification.redhat.com",
          "kind": "OperatorPipeline",
          "version": "v1alpha1"
        }
      ],
      "provider": "Red Hat",
      "related_images": [
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:9ed39145a27b56c2465f5076f2cb40fe7d84c8914908e4610ee5b5f9dbf90a58",
          "image": "quay.io/opdev/operator-certification-operator@sha256:9ed39145a27b56c2465f5076f2cb40fe7d84c8914908e4610ee5b5f9dbf90a58",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "1.0.3",
      "version_original": "1.0.3"
    },
    {
      "_id": "62fffc84720db70dae937331",
      "alm_examples": [
        {
          "api_version": "certification.redhat.com/v1alpha1",
          "kind": "OperatorPipeline",
          "metadata": {
            "name": "operatorpipeline-sample"
          },
          "spec": {
            "applyCIPipeline": true,
            "applyHostedPipeline": false,
            "applyReleasePipeline": false,
            "gitHubSecretName": "github-api-token",
            "kubeconfigSecretName": "kubeconfig",
            "operatorPipelinesRelease": "main",
            "pyxisSecretName": "pyxis-api-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/opdev/operator-certification-operator-bundle@sha256:5434186d5c4dcd3d6a664fb469c17d336b9ef58836f89446ea5130a335ae214d",
      "bundle_path_digest": "sha256:5434186d5c4dcd3d6a664fb469c17d336b9ef58836f89446ea5130a335ae214d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-19T21:11:32.442000+00:00",
      "csv_description": "A Kubernetes operator to provision resources for the operator certification pipeline. This operator is installed in all namespaces which can support multi-tenant scenarios. **Note:** This operator should only be used by Red Hat partners attempting to certify their operator(s).\n\n\n**Requirements**\n\n\nThe certification operator requires that you have the following tools installed, functional, and in your path.\n- [Install](https://docs.openshift.com/container-platform/4.8/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli) oc, the OpenShift CLI tool (tested with version 4.7.13)\n- [Install](https://tekton.dev/docs/cli/) tkn, the Tekton CLI tool (tested with version 0.19.1)\n- [Install](https://git-scm.com/downloads) git, the Git CLI tool (tested with 2.32.0)\n- The certification pipeline expects you to have the source files of your Operator bundle\n\n\n**Pre - Installation**\n\n\nThe below steps exist for simplicity and to make the installation clearer. The operator watches all namespaces, so if secrets/configs/etc already exist in another namespace, feel free to use the existing namespace when following the operator installation steps.\n\n\n**Create a new namespace where the following secrets will be applied.**\n```\noc new-project oco\n```\n\n\n**Add the kubeconfig secret which will be used to deploy the operator under test and run the certification checks.**\n* Open a terminal window\n* Set the `KUBECONFIG` environment variable\n```\nexport KUBECONFIG=/path/to/your/cluster/kubeconfig\n```\n> *This kubeconfig will be used to deploy the Operator under test and run the certification checks.*\n```\noc create secret generic kubeconfig --from-file=kubeconfig=$KUBECONFIG\n```\n\n\n**Configuring steps for submitting the results**\n- Add the github API token to the repo where the PR will be created\n```\noc create secret generic github-api-token --from-literal GITHUB_TOKEN=<github token>\n```\n- Add RedHat Container API access key\n\n  This API access key is specifically related to your unique partner account for Red Hat Connect portal. Instructions to obtain your API key can be found: [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/operator-cert-workflow.md#step-b---get-api-key)\n```\noc create secret generic pyxis-api-secret --from-literal pyxis_api_key=< API KEY >\n```\n\n- Optional pipeline configurations can be found [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/ci-pipeline.md#optional-configuration)\n\n\n**Execute the Pipeline (Development Iterations)**\n\n\nA pre-requisite to running a pipeline is that a `workspace-template.yaml` exists in the directory you want to execute the `tkn` commands from.\n\nTo create a workspace-template.yaml\n```\ncat <<EOF > workspace-template.yaml\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nEOF\n```\n\nThere are multiple ways to execute the Pipeline which can be found [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/ci-pipeline.md#execute-the-pipeline-development-iterations)",
      "csv_display_name": "Operator Certification Operator",
      "csv_metadata_description": "",
      "csv_name": "operator-certification-operator.v1.0.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:01:03.307000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "operator-certification-operator",
      "provided_apis": [
        {
          "group": "certification.redhat.com",
          "kind": "OperatorPipeline",
          "version": "v1alpha1"
        }
      ],
      "provider": "Red Hat",
      "related_images": [
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:9ed39145a27b56c2465f5076f2cb40fe7d84c8914908e4610ee5b5f9dbf90a58",
          "image": "quay.io/opdev/operator-certification-operator@sha256:9ed39145a27b56c2465f5076f2cb40fe7d84c8914908e4610ee5b5f9dbf90a58",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "1.0.3",
      "version_original": "1.0.3"
    },
    {
      "_id": "62fffd47720db70dae937347",
      "alm_examples": [
        {
          "api_version": "certification.redhat.com/v1alpha1",
          "kind": "OperatorPipeline",
          "metadata": {
            "name": "operatorpipeline-sample"
          },
          "spec": {
            "applyCIPipeline": true,
            "applyHostedPipeline": false,
            "applyReleasePipeline": false,
            "gitHubSecretName": "github-api-token",
            "kubeconfigSecretName": "kubeconfig",
            "operatorPipelinesRelease": "main",
            "pyxisSecretName": "pyxis-api-secret"
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/opdev/operator-certification-operator-bundle@sha256:5434186d5c4dcd3d6a664fb469c17d336b9ef58836f89446ea5130a335ae214d",
      "bundle_path_digest": "sha256:5434186d5c4dcd3d6a664fb469c17d336b9ef58836f89446ea5130a335ae214d",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-19T21:14:47.339000+00:00",
      "csv_description": "A Kubernetes operator to provision resources for the operator certification pipeline. This operator is installed in all namespaces which can support multi-tenant scenarios. **Note:** This operator should only be used by Red Hat partners attempting to certify their operator(s).\n\n\n**Requirements**\n\n\nThe certification operator requires that you have the following tools installed, functional, and in your path.\n- [Install](https://docs.openshift.com/container-platform/4.8/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli) oc, the OpenShift CLI tool (tested with version 4.7.13)\n- [Install](https://tekton.dev/docs/cli/) tkn, the Tekton CLI tool (tested with version 0.19.1)\n- [Install](https://git-scm.com/downloads) git, the Git CLI tool (tested with 2.32.0)\n- The certification pipeline expects you to have the source files of your Operator bundle\n\n\n**Pre - Installation**\n\n\nThe below steps exist for simplicity and to make the installation clearer. The operator watches all namespaces, so if secrets/configs/etc already exist in another namespace, feel free to use the existing namespace when following the operator installation steps.\n\n\n**Create a new namespace where the following secrets will be applied.**\n```\noc new-project oco\n```\n\n\n**Add the kubeconfig secret which will be used to deploy the operator under test and run the certification checks.**\n* Open a terminal window\n* Set the `KUBECONFIG` environment variable\n```\nexport KUBECONFIG=/path/to/your/cluster/kubeconfig\n```\n> *This kubeconfig will be used to deploy the Operator under test and run the certification checks.*\n```\noc create secret generic kubeconfig --from-file=kubeconfig=$KUBECONFIG\n```\n\n\n**Configuring steps for submitting the results**\n- Add the github API token to the repo where the PR will be created\n```\noc create secret generic github-api-token --from-literal GITHUB_TOKEN=<github token>\n```\n- Add RedHat Container API access key\n\n  This API access key is specifically related to your unique partner account for Red Hat Connect portal. Instructions to obtain your API key can be found: [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/operator-cert-workflow.md#step-b---get-api-key)\n```\noc create secret generic pyxis-api-secret --from-literal pyxis_api_key=< API KEY >\n```\n\n- Optional pipeline configurations can be found [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/ci-pipeline.md#optional-configuration)\n\n\n**Execute the Pipeline (Development Iterations)**\n\n\nA pre-requisite to running a pipeline is that a `workspace-template.yaml` exists in the directory you want to execute the `tkn` commands from.\n\nTo create a workspace-template.yaml\n```\ncat <<EOF > workspace-template.yaml\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nEOF\n```\n\nThere are multiple ways to execute the Pipeline which can be found [here](https://github.com/redhat-openshift-ecosystem/certification-releases/blob/main/4.9/ga/ci-pipeline.md#execute-the-pipeline-development-iterations)",
      "csv_display_name": "Operator Certification Operator",
      "csv_metadata_description": "",
      "csv_name": "operator-certification-operator.v1.0.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:59:25.056000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "operator-certification-operator",
      "provided_apis": [
        {
          "group": "certification.redhat.com",
          "kind": "OperatorPipeline",
          "version": "v1alpha1"
        }
      ],
      "provider": "Red Hat",
      "related_images": [
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:9ed39145a27b56c2465f5076f2cb40fe7d84c8914908e4610ee5b5f9dbf90a58",
          "image": "quay.io/opdev/operator-certification-operator@sha256:9ed39145a27b56c2465f5076f2cb40fe7d84c8914908e4610ee5b5f9dbf90a58",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "1.0.3",
      "version_original": "1.0.3"
    },
    {
      "_id": "63003082dca7d12fdf8e15c3",
      "alm_examples": [
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollectionGroup",
          "metadata": {
            "name": "my-collection-group"
          },
          "spec": {
            "maxTTL": "",
            "names": [
              "my-collection"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollection",
          "metadata": {
            "name": "my-collection"
          },
          "spec": {
            "maxTTL": "",
            "name": "my-collection"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScopeGroup",
          "metadata": {
            "name": "my-scope-group"
          },
          "spec": {
            "collections": {},
            "names": [
              "my-scope"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScope",
          "metadata": {
            "name": "my-scope"
          },
          "spec": {
            "collections": {},
            "name": "my-scope"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMigrationReplication",
          "metadata": {
            "name": "default-migration"
          },
          "spec": {
            "bucket": "default",
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "bundle_path_digest": "sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "2.3.2",
      "creation_date": "2022-08-20T00:53:22.866000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.3.2-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:17:57.001000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollection",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollectionGroup",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMigrationReplication",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScope",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScopeGroup",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "version": "v2"
        }
      ],
      "provider": "Couchbase",
      "related_images": [
        {
          "digest": "sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase-server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase-backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase-metrics"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "exporter-d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "server-05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.3.2-1",
      "version_original": "2.3.2-1"
    },
    {
      "_id": "63003087720db70dae937c9b",
      "alm_examples": [
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollectionGroup",
          "metadata": {
            "name": "my-collection-group"
          },
          "spec": {
            "maxTTL": "",
            "names": [
              "my-collection"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollection",
          "metadata": {
            "name": "my-collection"
          },
          "spec": {
            "maxTTL": "",
            "name": "my-collection"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScopeGroup",
          "metadata": {
            "name": "my-scope-group"
          },
          "spec": {
            "collections": {},
            "names": [
              "my-scope"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScope",
          "metadata": {
            "name": "my-scope"
          },
          "spec": {
            "collections": {},
            "name": "my-scope"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMigrationReplication",
          "metadata": {
            "name": "default-migration"
          },
          "spec": {
            "bucket": "default",
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "bundle_path_digest": "sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T00:53:27.648000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.3.2-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:17:51.381000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollection",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollectionGroup",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMigrationReplication",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScope",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScopeGroup",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "version": "v2"
        }
      ],
      "provider": "Couchbase",
      "related_images": [
        {
          "digest": "sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase-server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase-backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase-metrics"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "exporter-d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "server-05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.3.2-1",
      "version_original": "2.3.2-1"
    },
    {
      "_id": "630030e353cb8f6da4555894",
      "alm_examples": [
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollectionGroup",
          "metadata": {
            "name": "my-collection-group"
          },
          "spec": {
            "maxTTL": "",
            "names": [
              "my-collection"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollection",
          "metadata": {
            "name": "my-collection"
          },
          "spec": {
            "maxTTL": "",
            "name": "my-collection"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScopeGroup",
          "metadata": {
            "name": "my-scope-group"
          },
          "spec": {
            "collections": {},
            "names": [
              "my-scope"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScope",
          "metadata": {
            "name": "my-scope"
          },
          "spec": {
            "collections": {},
            "name": "my-scope"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMigrationReplication",
          "metadata": {
            "name": "default-migration"
          },
          "spec": {
            "bucket": "default",
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "bundle_path_digest": "sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "2.3.2",
      "creation_date": "2022-08-20T00:54:59.678000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.3.2-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T11:47:26.394000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMigrationReplication",
          "plural": "couchbasemigrationreplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScope",
          "plural": "couchbasescopes",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScopeGroup",
          "plural": "couchbasescopegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollection",
          "plural": "couchbasecollections",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollectionGroup",
          "plural": "couchbasecollectiongroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        }
      ],
      "provider": "Couchbase",
      "related_images": [
        {
          "digest": "sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase-server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase-backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase-metrics"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "exporter-d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "server-05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.3.2-1",
      "version_original": "2.3.2-1"
    },
    {
      "_id": "630030e4dca7d12fdf8e15cb",
      "alm_examples": [
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollectionGroup",
          "metadata": {
            "name": "my-collection-group"
          },
          "spec": {
            "maxTTL": "",
            "names": [
              "my-collection"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollection",
          "metadata": {
            "name": "my-collection"
          },
          "spec": {
            "maxTTL": "",
            "name": "my-collection"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScopeGroup",
          "metadata": {
            "name": "my-scope-group"
          },
          "spec": {
            "collections": {},
            "names": [
              "my-scope"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScope",
          "metadata": {
            "name": "my-scope"
          },
          "spec": {
            "collections": {},
            "name": "my-scope"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMigrationReplication",
          "metadata": {
            "name": "default-migration"
          },
          "spec": {
            "bucket": "default",
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "bundle_path_digest": "sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T00:55:00.791000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.3.2-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:47:31.042000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMigrationReplication",
          "plural": "couchbasemigrationreplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScope",
          "plural": "couchbasescopes",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScopeGroup",
          "plural": "couchbasescopegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollection",
          "plural": "couchbasecollections",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollectionGroup",
          "plural": "couchbasecollectiongroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        }
      ],
      "provider": "Couchbase",
      "related_images": [
        {
          "digest": "sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase-server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase-backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase-metrics"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "exporter-d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "server-05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.3.2-1",
      "version_original": "2.3.2-1"
    },
    {
      "_id": "6300311b720db70dae937ca6",
      "alm_examples": [
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollectionGroup",
          "metadata": {
            "name": "my-collection-group"
          },
          "spec": {
            "maxTTL": "",
            "names": [
              "my-collection"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollection",
          "metadata": {
            "name": "my-collection"
          },
          "spec": {
            "maxTTL": "",
            "name": "my-collection"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScopeGroup",
          "metadata": {
            "name": "my-scope-group"
          },
          "spec": {
            "collections": {},
            "names": [
              "my-scope"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScope",
          "metadata": {
            "name": "my-scope"
          },
          "spec": {
            "collections": {},
            "name": "my-scope"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMigrationReplication",
          "metadata": {
            "name": "default-migration"
          },
          "spec": {
            "bucket": "default",
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "bundle_path_digest": "sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "2.3.2",
      "creation_date": "2022-08-20T00:55:55.417000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.3.2-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T11:44:04.726000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScope",
          "plural": "couchbasescopes",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMigrationReplication",
          "plural": "couchbasemigrationreplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScopeGroup",
          "plural": "couchbasescopegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollectionGroup",
          "plural": "couchbasecollectiongroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollection",
          "plural": "couchbasecollections",
          "version": "v2"
        }
      ],
      "provider": "Couchbase",
      "related_images": [
        {
          "digest": "sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase-server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase-backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase-metrics"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "exporter-d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "server-05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.3.2-1",
      "version_original": "2.3.2-1"
    },
    {
      "_id": "6300311cdca7d12fdf8e15ce",
      "alm_examples": [
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollectionGroup",
          "metadata": {
            "name": "my-collection-group"
          },
          "spec": {
            "maxTTL": "",
            "names": [
              "my-collection"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollection",
          "metadata": {
            "name": "my-collection"
          },
          "spec": {
            "maxTTL": "",
            "name": "my-collection"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScopeGroup",
          "metadata": {
            "name": "my-scope-group"
          },
          "spec": {
            "collections": {},
            "names": [
              "my-scope"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScope",
          "metadata": {
            "name": "my-scope"
          },
          "spec": {
            "collections": {},
            "name": "my-scope"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMigrationReplication",
          "metadata": {
            "name": "default-migration"
          },
          "spec": {
            "bucket": "default",
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "bundle_path_digest": "sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T00:55:55.998000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.3.2-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:44:08.548000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScope",
          "plural": "couchbasescopes",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMigrationReplication",
          "plural": "couchbasemigrationreplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScopeGroup",
          "plural": "couchbasescopegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollectionGroup",
          "plural": "couchbasecollectiongroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollection",
          "plural": "couchbasecollections",
          "version": "v2"
        }
      ],
      "provider": "Couchbase",
      "related_images": [
        {
          "digest": "sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase-server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase-backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase-metrics"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "exporter-d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "server-05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.3.2-1",
      "version_original": "2.3.2-1"
    },
    {
      "_id": "6300317c53cb8f6da455589b",
      "alm_examples": [
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollectionGroup",
          "metadata": {
            "name": "my-collection-group"
          },
          "spec": {
            "maxTTL": "",
            "names": [
              "my-collection"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollection",
          "metadata": {
            "name": "my-collection"
          },
          "spec": {
            "maxTTL": "",
            "name": "my-collection"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScopeGroup",
          "metadata": {
            "name": "my-scope-group"
          },
          "spec": {
            "collections": {},
            "names": [
              "my-scope"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScope",
          "metadata": {
            "name": "my-scope"
          },
          "spec": {
            "collections": {},
            "name": "my-scope"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMigrationReplication",
          "metadata": {
            "name": "default-migration"
          },
          "spec": {
            "bucket": "default",
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "bundle_path_digest": "sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T00:57:32.177000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.3.2-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:21:43.855000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollectionGroup",
          "plural": "couchbasecollectiongroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollection",
          "plural": "couchbasecollections",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMigrationReplication",
          "plural": "couchbasemigrationreplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScopeGroup",
          "plural": "couchbasescopegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScope",
          "plural": "couchbasescopes",
          "version": "v2"
        }
      ],
      "provider": "Couchbase",
      "related_images": [
        {
          "digest": "sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase-server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase-backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase-metrics"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "exporter-d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "server-05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.3.2-1",
      "version_original": "2.3.2-1"
    },
    {
      "_id": "6300317cdca7d12fdf8e15d1",
      "alm_examples": [
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCluster",
          "metadata": {
            "name": "cb-example"
          },
          "spec": {
            "backup": {
              "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              },
              "serviceAccountName": "couchbase-backup"
            },
            "buckets": {
              "managed": true,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            },
            "cluster": {
              "analyticsServiceMemoryQuota": "1Gi",
              "autoFailoverMaxCount": 3,
              "autoFailoverOnDataDiskIssues": true,
              "autoFailoverOnDataDiskIssuesTimePeriod": "120s",
              "autoFailoverServerGroup": false,
              "autoFailoverTimeout": "120s",
              "clusterName": "cb-example",
              "dataServiceMemoryQuota": "256Mi",
              "eventingServiceMemoryQuota": "256Mi",
              "indexServiceMemoryQuota": "256Mi",
              "indexStorageSetting": "memory_optimized",
              "searchServiceMemoryQuota": "256Mi"
            },
            "enablePreviewScaling": false,
            "hibernate": false,
            "hibernationStrategy": "Immediate",
            "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
            "logRetentionCount": 20,
            "logRetentionTime": "604800s",
            "monitoring": {
              "prometheus": {
                "enabled": false,
                "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4"
              }
            },
            "networking": {
              "adminConsoleServiceType": "NodePort",
              "adminConsoleServices": [
                "data"
              ],
              "exposeAdminConsole": true,
              "exposedFeatureServiceType": "NodePort",
              "exposedFeatures": [
                "xdcr"
              ]
            },
            "recoveryPolicy": "PrioritizeDataIntegrity",
            "security": {
              "adminSecret": "cb-example-auth",
              "rbac": {
                "managed": true,
                "selector": {
                  "matchLabels": {
                    "cluster": "cb-example"
                  }
                }
              }
            },
            "servers": [
              {
                "name": "all_services",
                "services": [
                  "data",
                  "index",
                  "query",
                  "search",
                  "eventing",
                  "analytics"
                ],
                "size": 3
              }
            ],
            "upgradeStrategy": "RollingUpgrade",
            "xdcr": {
              "managed": false,
              "selector": {
                "matchLabels": {
                  "cluster": "cb-example"
                }
              }
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "default"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "enableIndexReplica": true,
            "evictionPolicy": "valueOnly",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseEphemeralBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "ephemeral-bucket"
          },
          "spec": {
            "compressionMode": "passive",
            "conflictResolution": "lww",
            "enableFlush": false,
            "evictionPolicy": "noEviction",
            "ioPriority": "low",
            "memoryQuota": "100Mi",
            "replicas": 2
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMemcachedBucket",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "memcached-bucket"
          },
          "spec": {
            "enableFlush": false,
            "memoryQuota": "100Mi"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseUser",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-user"
          },
          "spec": {
            "authDomain": "local",
            "authSecret": "cb-example-auth",
            "fullName": "My User"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseGroup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-group"
          },
          "spec": {
            "roles": [
              {
                "bucket": "default",
                "name": "bucket_admin"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseRoleBinding",
          "metadata": {
            "name": "my-role-binding"
          },
          "spec": {
            "roleRef": {
              "kind": "CouchbaseGroup",
              "name": "my-group"
            },
            "subjects": [
              {
                "kind": "CouchbaseUser",
                "name": "my-user"
              }
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseReplication",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "my-replication"
          },
          "spec": {
            "bucket": "default",
            "compressionType": "Snappy",
            "filterExpression": "",
            "paused": false,
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackup",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-backup"
          },
          "spec": {
            "backOffLimit": 2,
            "backupRetention": "24h",
            "failedJobsHistoryLimit": 3,
            "full": {
              "schedule": "0 3 * * 6"
            },
            "incremental": {
              "schedule": "0 3 * * 1-6"
            },
            "logRetention": "24h",
            "size": "5Gi",
            "strategy": "full_incremental",
            "successfulJobsHistoryLimit": 1
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseBackupRestore",
          "metadata": {
            "labels": {
              "cluster": "cb-example"
            },
            "name": "cb-restore"
          },
          "spec": {
            "backOffLimit": 2,
            "backup": "cb-backup",
            "end": {
              "int": 1
            },
            "logRetention": "24h",
            "repo": "cb-example-2020-10-29T19_00_03",
            "start": {
              "int": 1
            }
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollectionGroup",
          "metadata": {
            "name": "my-collection-group"
          },
          "spec": {
            "maxTTL": "",
            "names": [
              "my-collection"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseCollection",
          "metadata": {
            "name": "my-collection"
          },
          "spec": {
            "maxTTL": "",
            "name": "my-collection"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScopeGroup",
          "metadata": {
            "name": "my-scope-group"
          },
          "spec": {
            "collections": {},
            "names": [
              "my-scope"
            ]
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseScope",
          "metadata": {
            "name": "my-scope"
          },
          "spec": {
            "collections": {},
            "name": "my-scope"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseMigrationReplication",
          "metadata": {
            "name": "default-migration"
          },
          "spec": {
            "bucket": "default",
            "remoteBucket": "default"
          }
        },
        {
          "api_version": "couchbase.com/v2",
          "kind": "CouchbaseAutoscaler",
          "metadata": {
            "name": "do.not.create.internal.only"
          },
          "spec": {
            "servers": "internal",
            "size": 2
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [
          "Disconnected"
        ],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/couchbase/operator-bundle@sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "bundle_path_digest": "sha256:5ef27c3e167ef5e6bee05125364a0021780eca77c8ffdad587bed251b838585e",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "2.3.2",
      "creation_date": "2022-08-20T00:57:32.703000+00:00",
      "csv_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.\n\n## Supported Features\n\n* **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.\n\n* **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.\n\n* **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.\n\n* **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.\n\n* **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.\n\n* **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.\n\n* **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.\n\n* **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.\n## Required Parameters\n* `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))\n\n## About Couchbase Server\n\nBuilt on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.\n",
      "csv_display_name": "Couchbase Operator",
      "csv_metadata_description": "The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments",
      "csv_name": "couchbase-operator.v2.3.2-1",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": false,
      "last_update_date": "2022-09-19T12:18:35.389000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "couchbase-enterprise-certified",
      "provided_apis": [
        {
          "group": "couchbase.com",
          "kind": "CouchbaseReplication",
          "plural": "couchbasereplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCluster",
          "plural": "couchbaseclusters",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseRoleBinding",
          "plural": "couchbaserolebindings",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseGroup",
          "plural": "couchbasegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollectionGroup",
          "plural": "couchbasecollectiongroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseCollection",
          "plural": "couchbasecollections",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMemcachedBucket",
          "plural": "couchbasememcachedbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseEphemeralBucket",
          "plural": "couchbaseephemeralbuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackupRestore",
          "plural": "couchbasebackuprestores",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBackup",
          "plural": "couchbasebackups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseBucket",
          "plural": "couchbasebuckets",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseUser",
          "plural": "couchbaseusers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseMigrationReplication",
          "plural": "couchbasemigrationreplications",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScopeGroup",
          "plural": "couchbasescopegroups",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseAutoscaler",
          "plural": "couchbaseautoscalers",
          "version": "v2"
        },
        {
          "group": "couchbase.com",
          "kind": "CouchbaseScope",
          "plural": "couchbasescopes",
          "version": "v2"
        }
      ],
      "provider": "Couchbase",
      "related_images": [
        {
          "digest": "sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "image": "registry.connect.redhat.com/couchbase/operator@sha256:3480123a4c056115b8fe7205fd62e25676045ea3248f8cdf3ec20bfff9168ee2",
          "name": "couchbase-operator"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase-server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase-backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase-metrics"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "couchbase_server"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "couchbase_backup"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "couchbase_metrics"
        },
        {
          "digest": "sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "image": "registry.connect.redhat.com/couchbase/exporter@sha256:d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4",
          "name": "exporter-d392e6c902f784abfc083c9bf5ce11895d0183347b6c21b259678fd85f312cd4-annotation"
        },
        {
          "digest": "sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "image": "registry.connect.redhat.com/couchbase/operator-backup@sha256:c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76",
          "name": "operator-backup-c0ab51854294d117c4ecf867b541ed6dc67410294d72f560cc33b038d98e4b76-annotation"
        },
        {
          "digest": "sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "image": "registry.connect.redhat.com/couchbase/server@sha256:05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af",
          "name": "server-05aad0f1d3a373b60dece893a9c185dcb0e0630aa6f0c0f310ad8767918fd2af-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.3.2-1",
      "version_original": "2.3.2-1"
    },
    {
      "_id": "6300a45353cb8f6da4556edb",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "bundle_path_digest": "sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T09:07:31.988000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:00:55.502000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "image": "registry.connect.redhat.com/trilio/web@sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.4",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "2.10.4",
      "version_original": "2.10.4"
    },
    {
      "_id": "6300a5367892413c524fa99f",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "bundle_path_digest": "sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T09:11:18.086000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:34:58.356000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "image": "registry.connect.redhat.com/trilio/web@sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.4",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.10.4",
      "version_original": "2.10.4"
    },
    {
      "_id": "6300a5d253cb8f6da4556efd",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "bundle_path_digest": "sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T09:13:54.170000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:50:29.893000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "image": "registry.connect.redhat.com/trilio/web@sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.4",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "2.10.4",
      "version_original": "2.10.4"
    },
    {
      "_id": "6300a6d17892413c524fa9b5",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "bundle_path_digest": "sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T09:18:09.786000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:55:44.759000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "image": "registry.connect.redhat.com/trilio/web@sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.4",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "2.10.4",
      "version_original": "2.10.4"
    },
    {
      "_id": "6300a8dadca7d12fdf8e2cf1",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "bundle_path_digest": "sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T09:26:50.558000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:29:33.295000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "image": "registry.connect.redhat.com/trilio/web@sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.4",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "2.10.4",
      "version_original": "2.10.4"
    },
    {
      "_id": "6300a8e9720db70dae93931f",
      "alm_examples": [
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "License",
          "metadata": {
            "name": "tvk-license-one",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "key": "xLkNDgwKD3jafZJNb8IwDIbv+RWRdo6Uj5a2SD1MwKRtjE0bsLOTuiISpFWaovHv145NQy3iPcb249d27j6xoE/tnkpJOZ/KaBrFVHLJycwjBFu5OQSk/8r7IOMJk5IsCttn0IHyB4+49hb2BEywR6SjjOBbJHtr0DW4Rd9cUnJJXsC6gA6cwcVXbf3pbKLvrRhPmersVYca3GlEfsdiB4Gs2oNG/1pumo5+EWaCfKA/on+cDysh5gWokjOjU8WEQGBaQMSM0DrDiTKTpCterOgV5d07+wXIm4Dleehh+3y9vd8s138MdduEqWq8YsLBAZsaDBbkudW4Gc1Ic56ITHMwrEwTwSIlY5bqLGGGx1kW6QQzUZKfncPotJfrf2u92UGDN37HDDorNgxPlAtOe3d0VRXYfAMj8ZzrMEYCIQCVaI/QJN+2M/QJWimd28dWOi/6o5s5I5+z30JrmzwGnQIhAKrSu8NeUIqXGEBTQvPKe3n2U0LNPht/ZAQIs5CZNeWlX02gk"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "Target",
          "metadata": {
            "labels": {
              "app": "triliovault"
            },
            "name": "triliovault-target",
            "namespace": "openshift-marketplace"
          },
          "spec": {
            "nfsCredentials": {
              "nfsExport": "00.00.00.00:/src/nfs/kubedata",
              "nfsOptions": "nfsvers=4"
            },
            "type": "NFS",
            "vendor": "Other"
          }
        },
        {
          "api_version": "triliovault.trilio.io/v1",
          "kind": "BackupPlan",
          "metadata": {
            "name": "tvk-backupplan"
          },
          "spec": {
            "backupConfig": {
              "retentionPolicy": {
                "name": "retention-policy",
                "namespace": "openshift-marketplace"
              },
              "schedulePolicy": {
                "fullBackupPolicy": {
                  "name": "full-bp-policy",
                  "namespace": "openshift-marketplace"
                },
                "incrementalBackupPolicy": {
                  "name": "inc-bp-policy",
                  "namespace": "openshift-marketplace"
                }
              },
              "target": {
                "name": "triliovault-target",
                "namespace": "openshift-marketplace"
              }
            },
            "backupPlanComponents": {
              "custom": [
                {
                  "matchLabels": {
                    "app": "nginx"
                  }
                }
              ],
              "helmReleases": [
                "sample-release"
              ],
              "operators": [
                {
                  "applicationResourceSelector": [
                    {
                      "matchLabels": {
                        "app": "etcd"
                      }
                    }
                  ],
                  "customResources": [
                    {
                      "groupVersionKind": {
                        "group": "etcd.database.coreos.com",
                        "kind": "EtcdCluster",
                        "version": "v1beta2"
                      },
                      "objects": [
                        "demo-etcd-cluster"
                      ]
                    }
                  ],
                  "operatorId": "demo-etcd-cluster",
                  "operatorResourceSelector": [
                    {
                      "matchLabels": {
                        "release": "demo-etcd-operator"
                      }
                    }
                  ]
                }
              ]
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/trilio/r-3445381-bundle@sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "bundle_path_digest": "sha256:8b622d7d5bf01e64c5c3acec0c9c6d18a1cfd0ded1e3a419e47f0616c86354d6",
      "capabilities": [
        "Full Lifecycle"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-20T09:27:05.014000+00:00",
      "csv_description": "TrilioVault for Kubernetes is an enterprise-grade, cloud-native platform purpose built for data protection and management of Kubernetes applications for IT managers, administrators and developers. TrilioVault supports upstream Kubernetes and OpenShift environments and offers the following features:\n  * Application-Centric - protects both data and metadata for Helm, Operator or custom Label based applications.\n  * Red Hat Certified - first backup and recovery solution with OpenShift Operator Certification.\n  * Native to Kubernetes & OpenShift: Packaged and deployed as an Operator, integrated via Kubernetes API and with all features that it provides.\n  * Infrastructure Agnostic: Compatible with any Storage (CSI, NFS, S3), or any Cloud (Private or Public).\n  * Application Deployment & Tooling: Helm, Operators, Labels, Prometheus, Fluentd.\nTutorials\n------------\nPlease click the link below to access the TrilioVault for Kubernetes \"How-To\" series for deployment, best practice and use-case videos\n<a href=\"https://www.trilio.io/triliovault-for-kubernetes\">TrilioVault for Kubernetes - Tutorials and How-To </a>\u201d\nLicensing\n--------\nCustomers can download a 30-day Free Trial or a 10-node Basic Edition at no cost.  You can also connect with the Trilio team for an Enterprise Edition license with no limitations and Premium Support.\nFor more information on license plans please vist:\n<a href=\"www.trilio.io/plans\"> Trilio Vault for Kubernetes licensing and plans </a>\nAbout Trilio\n----------------\nTrilio is trusted by global cloud infrastructure operators to deliver data protection, application resiliency, infrastructure migration and infrastructure version management. Our TrilioVault technology supports Kubernetes, OpenStack and Virtualization environments to recover from disasters, migrate tenant workloads, move workloads to new infrastructures and migrate to new infrastructure software distributions. www.trilio.io and @triliodata on Twitter.",
      "csv_display_name": "TrilioVault for Kubernetes",
      "csv_metadata_description": "Cloud-Native Data Protection for Kubernetes",
      "csv_name": "k8s-triliovault-stable.2.10.4",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": true,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:19:15.283000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "k8s-triliovault",
      "provided_apis": [
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackupPlan",
          "plural": "clusterbackupplans",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Target",
          "plural": "targets",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterRestore",
          "plural": "clusterrestores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Hook",
          "plural": "hooks",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "License",
          "plural": "licenses",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Restore",
          "plural": "restores",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Policy",
          "plural": "policies",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "ClusterBackup",
          "plural": "clusterbackups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "Backup",
          "plural": "backups",
          "version": "v1"
        },
        {
          "group": "triliovault.trilio.io",
          "kind": "BackupPlan",
          "plural": "backupplans",
          "version": "v1"
        }
      ],
      "provider": "Trilio",
      "related_images": [
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "control-plane-49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5-annotation"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-control-plane"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "k8s-triliovault-analyzer"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-admission-webhook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-exporter"
        },
        {
          "digest": "sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "image": "registry.connect.redhat.com/trilio/web@sha256:23967fd87017b5bd0ce317adbb6e3e473d2b396f54f8d47823088ddadaaeab3e",
          "name": "triliovault-web"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "triliovault-web-backend"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "triliovault-dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "webhook-init"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "dex-init"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "metamover"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "datamover"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "datastore_attacher"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "backup_scheduler"
        },
        {
          "digest": "sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "image": "registry.connect.redhat.com/trilio/metamover@sha256:3a0e1fa34791ae52a1b8123be1aab75b7948fbe08b9f42b06f14ebe887207d07",
          "name": "backup_cleaner"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "backup_retention"
        },
        {
          "digest": "sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "image": "registry.connect.redhat.com/trilio/datamover@sha256:eaf32c8a791ceb1fea70deb1173d2a6e7bff7a85bc66935676dc0743b07635d0",
          "name": "target_browser"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "hook"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "resource_cleaner"
        },
        {
          "digest": "sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "image": "registry.connect.redhat.com/trilio/minio@sha256:62ea930dfe5370eeaa574d88d94fb0120a5af0a1cc90a4a1831be7630a30844c",
          "name": "minio"
        },
        {
          "digest": "sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "image": "registry.connect.redhat.com/trilio/dex@sha256:7bcb3d85a4447f1fa684b76b8cb7fb314a9ddedbd447b336da8a3ea286b5834f",
          "name": "dex"
        },
        {
          "digest": "sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "image": "registry.connect.redhat.com/trilio/control-plane@sha256:49d57245a12a01fc0a1e35effe503143fc2ed68abdf91851a3ca6059292789e5",
          "name": "tvk_init"
        }
      ],
      "replaces": null,
      "skip_range": "<2.10.4",
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "2.10.4",
      "version_original": "2.10.4"
    },
    {
      "_id": "6303fc4753cb8f6da4560a7c",
      "alm_examples": [
        {
          "api_version": "anzograph.clusters.cambridgesemantics.com/v2",
          "kind": "AnzoGraph",
          "metadata": {
            "name": "azg01"
          },
          "spec": {
            "db": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_data": "anzograph-data-grpc",
                      "app_mgmt": "anzograph-mgmt-grpc"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_data": "anzograph-data-grpc",
                        "app_mgmt": "anzograph-mgmt-grpc"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
                          "name": "db",
                          "resources": {
                            "limits": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            },
                            "requests": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            },
            "deployFrontend": false,
            "frontend": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_mgmt": "anzograph-frontend"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_mgmt": "anzograph-frontend"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
                          "name": "frontend",
                          "resources": {
                            "limits": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator-bundle@sha256:588561c6c2075607c5713d5a9bf7904c4b840f5623b22c63aa7539c81b5f542a",
      "bundle_path_digest": "sha256:588561c6c2075607c5713d5a9bf7904c4b840f5623b22c63aa7539c81b5f542a",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-22T21:59:35.687000+00:00",
      "csv_description": "The AnzoGraph Operator provides the way to install and configure an AnzoGraph\ncluster on Red Hat K8S environment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzograph/blob/v2.0.2/README_openshift_marketplace.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzograph/userdoc/ )\n\n### Support\n\nWe offer Support to our customers with the AnzoGraph db Enterprise Edition License [here]( https://customercenter.cambridgesemantics.com/ ). For AnzoGraph db Free Edition questions, get help from our Anzograph User Community at Stack Overflow. When submitting a question, include the tag 'anzograph'.",
      "csv_display_name": "AnzoGraph Operator",
      "csv_metadata_description": "kubernetes operator for AnzoGraph DB",
      "csv_name": "anzograph-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:24:15.934000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "anzograph-operator",
      "provided_apis": [
        {
          "group": "anzograph.clusters.cambridgesemantics.com",
          "kind": "AnzoGraph",
          "version": "v2"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph@sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "name": "anzograph-allinone"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph-db"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph-frontend"
        },
        {
          "digest": "sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "name": "anzograph-operator-7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01-annotation"
        },
        {
          "digest": "sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "name": "manager"
        },
        {
          "digest": "sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph@sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "name": "anzograph_allinone"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph_db"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph_frontend"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph-frontend-3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8-annotation"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph-db-1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "2.0.202",
      "version_original": "2.0.202"
    },
    {
      "_id": "6303fc8a720db70dae942f77",
      "alm_examples": [
        {
          "api_version": "anzograph.clusters.cambridgesemantics.com/v2",
          "kind": "AnzoGraph",
          "metadata": {
            "name": "azg01"
          },
          "spec": {
            "db": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_data": "anzograph-data-grpc",
                      "app_mgmt": "anzograph-mgmt-grpc"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_data": "anzograph-data-grpc",
                        "app_mgmt": "anzograph-mgmt-grpc"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
                          "name": "db",
                          "resources": {
                            "limits": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            },
                            "requests": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            },
            "deployFrontend": false,
            "frontend": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_mgmt": "anzograph-frontend"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_mgmt": "anzograph-frontend"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
                          "name": "frontend",
                          "resources": {
                            "limits": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator-bundle@sha256:588561c6c2075607c5713d5a9bf7904c4b840f5623b22c63aa7539c81b5f542a",
      "bundle_path_digest": "sha256:588561c6c2075607c5713d5a9bf7904c4b840f5623b22c63aa7539c81b5f542a",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-22T22:00:42.898000+00:00",
      "csv_description": "The AnzoGraph Operator provides the way to install and configure an AnzoGraph\ncluster on Red Hat K8S environment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzograph/blob/v2.0.2/README_openshift_marketplace.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzograph/userdoc/ )\n\n### Support\n\nWe offer Support to our customers with the AnzoGraph db Enterprise Edition License [here]( https://customercenter.cambridgesemantics.com/ ). For AnzoGraph db Free Edition questions, get help from our Anzograph User Community at Stack Overflow. When submitting a question, include the tag 'anzograph'.",
      "csv_display_name": "AnzoGraph Operator",
      "csv_metadata_description": "kubernetes operator for AnzoGraph DB",
      "csv_name": "anzograph-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:25:30.632000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "anzograph-operator",
      "provided_apis": [
        {
          "group": "anzograph.clusters.cambridgesemantics.com",
          "kind": "AnzoGraph",
          "version": "v2"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph@sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "name": "anzograph-allinone"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph-db"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph-frontend"
        },
        {
          "digest": "sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "name": "anzograph-operator-7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01-annotation"
        },
        {
          "digest": "sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "name": "manager"
        },
        {
          "digest": "sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph@sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "name": "anzograph_allinone"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph_db"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph_frontend"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph-frontend-3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8-annotation"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph-db-1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "2.0.202",
      "version_original": "2.0.202"
    },
    {
      "_id": "6303fe6ddca7d12fdf8ec839",
      "alm_examples": [
        {
          "api_version": "anzograph.clusters.cambridgesemantics.com/v2",
          "kind": "AnzoGraph",
          "metadata": {
            "name": "azg01"
          },
          "spec": {
            "db": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_data": "anzograph-data-grpc",
                      "app_mgmt": "anzograph-mgmt-grpc"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_data": "anzograph-data-grpc",
                        "app_mgmt": "anzograph-mgmt-grpc"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
                          "name": "db",
                          "resources": {
                            "limits": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            },
                            "requests": {
                              "cpu": "8000m",
                              "memory": "8Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            },
            "deployFrontend": false,
            "frontend": {
              "nodeConfig": {
                "spec": {
                  "replicas": 1,
                  "selector": {
                    "matchLabels": {
                      "app_mgmt": "anzograph-frontend"
                    }
                  },
                  "serviceName": "anzograph-azg01",
                  "template": {
                    "metadata": {
                      "labels": {
                        "app_mgmt": "anzograph-frontend"
                      }
                    },
                    "spec": {
                      "containers": [
                        {
                          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
                          "name": "frontend",
                          "resources": {
                            "limits": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            },
                            "requests": {
                              "cpu": "2000m",
                              "memory": "4Gi"
                            }
                          }
                        }
                      ],
                      "serviceAccountName": "anzograph-operator"
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator-bundle@sha256:588561c6c2075607c5713d5a9bf7904c4b840f5623b22c63aa7539c81b5f542a",
      "bundle_path_digest": "sha256:588561c6c2075607c5713d5a9bf7904c4b840f5623b22c63aa7539c81b5f542a",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "stable",
      "creation_date": "2022-08-22T22:08:45.129000+00:00",
      "csv_description": "The AnzoGraph Operator provides the way to install and configure an AnzoGraph\ncluster on Red Hat K8S environment.\n\n### Installation\n Refer [installation instructions]( https://github.com/cambridgesemantics/csi-k8s-operator-anzograph/blob/v2.0.2/README_openshift_marketplace.md )\n\n### Documentation\n\nYou can find our documentation [here.]( https://docs.cambridgesemantics.com/anzograph/userdoc/ )\n\n### Support\n\nWe offer Support to our customers with the AnzoGraph db Enterprise Edition License [here]( https://customercenter.cambridgesemantics.com/ ). For AnzoGraph db Free Edition questions, get help from our Anzograph User Community at Stack Overflow. When submitting a question, include the tag 'anzograph'.",
      "csv_display_name": "AnzoGraph Operator",
      "csv_metadata_description": "kubernetes operator for AnzoGraph DB",
      "csv_name": "anzograph-operator.v2.0.202",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": false,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:32:56.269000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "anzograph-operator",
      "provided_apis": [
        {
          "group": "anzograph.clusters.cambridgesemantics.com",
          "kind": "AnzoGraph",
          "version": "v2"
        }
      ],
      "provider": "Cambridge Semantics Inc.",
      "related_images": [
        {
          "digest": "sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph@sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "name": "anzograph-allinone"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph-db"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph-frontend"
        },
        {
          "digest": "sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "name": "anzograph-operator-7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01-annotation"
        },
        {
          "digest": "sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-operator@sha256:7729e3dc9a4849df3a2edde2680bc56d3e76754585ed18774db51b653e562f01",
          "name": "manager"
        },
        {
          "digest": "sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph@sha256:6fa633a0e1f9a25f834784d2555f72184441db9d2ad278730048f8500ce46860",
          "name": "anzograph_allinone"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph_db"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph_frontend"
        },
        {
          "digest": "sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-frontend@sha256:3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8",
          "name": "anzograph-frontend-3eeaf6f2ab48b719326c2e9abe79f05d471aab2fe68c82dd0c05fcb54925f7a8-annotation"
        },
        {
          "digest": "sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "image": "registry.connect.redhat.com/cambridgesemantics/anzograph-db@sha256:1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32",
          "name": "anzograph-db-1b4c92baf50636c204e15935bb0626ffbd44ebd0d5e3ced139e59d0a9ef82a32-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "2.0.202",
      "version_original": "2.0.202"
    },
    {
      "_id": "6305ff5253cb8f6da456d65f",
      "alm_examples": [
        {
          "api_version": "fic.f5.com/v1",
          "kind": "F5IpamCtlr",
          "metadata": {
            "name": "f5ipamctlr-sample"
          },
          "spec": {
            "args": {
              "infoblox_grid_host": "<IP Address of Server>",
              "infoblox_labels": "<CIDR Labels>",
              "infoblox_netview": "<NetView>",
              "infoblox_password": "<PASSWORD>",
              "infoblox_username": "<USERNAME>",
              "infoblox_wapi_version": "<VERSION>",
              "insecure": true,
              "ip_range": "<IP RANGE LABELS>",
              "log_level": "<DEBUG_OR_INFO>",
              "orchestration": "kubernetes",
              "provider": "<infoblox_or_f5-ip-provider>"
            },
            "image": {
              "pullPolicy": "Always",
              "repo": "f5-ipam-controller",
              "user": "f5networks",
              "version": "latest"
            },
            "namespace": "kube-system",
            "rbac": {
              "create": true
            },
            "serviceAccount": {
              "create": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator-bundle@sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "bundle_path_digest": "sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T10:37:06.974000+00:00",
      "csv_description": "## Introduction\nThis Operator installs F5 IPAM Controller in your OpenShift Cluster. This enables to configure and deploy FIC using Helm Charts.\n## F5 IPAM Controller\nThe F5 IPAM Controller is a Docker container that runs in an orchestration environment and interfaces with an IPAM system. It allocates IP addresses from an IPAM system\u2019s address pool for hostnames in an orchestration environment. The F5 IPAM Controller watches orchestration-specific resources and consumes the hostnames within each resource.\n## Documentation\nRefer to F5 IPAM Controller documentation\n- https://github.com/F5Networks/f5-ipam-controller/blob/main/README.md\n## Prerequisites\n### Configure Infoblox with network and netview [Provider: Infoblox]\nRefer Infoblox documentation\n### Create persistent volume and persistent volume claim [Provider: f5-ip-provider]\nCreate persistent volume and persistent volume claim\n```\noc apply -f https://raw.githubusercontent.com/F5Networks/f5-ipam-controller/main/docs/config_examples/f5-ip-provider/localstorage-pv-pvc-example.yaml\n```\n",
      "csv_display_name": "F5 Ipam controller",
      "csv_metadata_description": "Operator to install F5 IPAM Controller.",
      "csv_name": "f5-ipam-controller-operator.v0.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:06:54.461000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "f5-ipam-controller-operator",
      "provided_apis": [
        {
          "group": "fic.f5.com",
          "kind": "F5IpamCtlr",
          "version": "v1"
        }
      ],
      "provider": "F5 Inc.",
      "related_images": [
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "f5-ipam-controller-operator-c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790-annotation"
        },
        {
          "digest": "sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "0.0.2",
      "version_original": "0.0.2"
    },
    {
      "_id": "6305ff54051dd1e222804c31",
      "alm_examples": [
        {
          "api_version": "fic.f5.com/v1",
          "kind": "F5IpamCtlr",
          "metadata": {
            "name": "f5ipamctlr-sample"
          },
          "spec": {
            "args": {
              "infoblox_grid_host": "<IP Address of Server>",
              "infoblox_labels": "<CIDR Labels>",
              "infoblox_netview": "<NetView>",
              "infoblox_password": "<PASSWORD>",
              "infoblox_username": "<USERNAME>",
              "infoblox_wapi_version": "<VERSION>",
              "insecure": true,
              "ip_range": "<IP RANGE LABELS>",
              "log_level": "<DEBUG_OR_INFO>",
              "orchestration": "kubernetes",
              "provider": "<infoblox_or_f5-ip-provider>"
            },
            "image": {
              "pullPolicy": "Always",
              "repo": "f5-ipam-controller",
              "user": "f5networks",
              "version": "latest"
            },
            "namespace": "kube-system",
            "rbac": {
              "create": true
            },
            "serviceAccount": {
              "create": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator-bundle@sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "bundle_path_digest": "sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T10:37:08.033000+00:00",
      "csv_description": "## Introduction\nThis Operator installs F5 IPAM Controller in your OpenShift Cluster. This enables to configure and deploy FIC using Helm Charts.\n## F5 IPAM Controller\nThe F5 IPAM Controller is a Docker container that runs in an orchestration environment and interfaces with an IPAM system. It allocates IP addresses from an IPAM system\u2019s address pool for hostnames in an orchestration environment. The F5 IPAM Controller watches orchestration-specific resources and consumes the hostnames within each resource.\n## Documentation\nRefer to F5 IPAM Controller documentation\n- https://github.com/F5Networks/f5-ipam-controller/blob/main/README.md\n## Prerequisites\n### Configure Infoblox with network and netview [Provider: Infoblox]\nRefer Infoblox documentation\n### Create persistent volume and persistent volume claim [Provider: f5-ip-provider]\nCreate persistent volume and persistent volume claim\n```\noc apply -f https://raw.githubusercontent.com/F5Networks/f5-ipam-controller/main/docs/config_examples/f5-ip-provider/localstorage-pv-pvc-example.yaml\n```\n",
      "csv_display_name": "F5 Ipam controller",
      "csv_metadata_description": "Operator to install F5 IPAM Controller.",
      "csv_name": "f5-ipam-controller-operator.v0.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:45:32.703000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "f5-ipam-controller-operator",
      "provided_apis": [
        {
          "group": "fic.f5.com",
          "kind": "F5IpamCtlr",
          "plural": "f5ipamctlrs",
          "version": "v1"
        }
      ],
      "provider": "F5 Inc.",
      "related_images": [
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "f5-ipam-controller-operator-c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790-annotation"
        },
        {
          "digest": "sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.0.2",
      "version_original": "0.0.2"
    },
    {
      "_id": "6305ff71a96406a9dbe65bfd",
      "alm_examples": [
        {
          "api_version": "fic.f5.com/v1",
          "kind": "F5IpamCtlr",
          "metadata": {
            "name": "f5ipamctlr-sample"
          },
          "spec": {
            "args": {
              "infoblox_grid_host": "<IP Address of Server>",
              "infoblox_labels": "<CIDR Labels>",
              "infoblox_netview": "<NetView>",
              "infoblox_password": "<PASSWORD>",
              "infoblox_username": "<USERNAME>",
              "infoblox_wapi_version": "<VERSION>",
              "insecure": true,
              "ip_range": "<IP RANGE LABELS>",
              "log_level": "<DEBUG_OR_INFO>",
              "orchestration": "kubernetes",
              "provider": "<infoblox_or_f5-ip-provider>"
            },
            "image": {
              "pullPolicy": "Always",
              "repo": "f5-ipam-controller",
              "user": "f5networks",
              "version": "latest"
            },
            "namespace": "kube-system",
            "rbac": {
              "create": true
            },
            "serviceAccount": {
              "create": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator-bundle@sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "bundle_path_digest": "sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T10:37:37.909000+00:00",
      "csv_description": "## Introduction\nThis Operator installs F5 IPAM Controller in your OpenShift Cluster. This enables to configure and deploy FIC using Helm Charts.\n## F5 IPAM Controller\nThe F5 IPAM Controller is a Docker container that runs in an orchestration environment and interfaces with an IPAM system. It allocates IP addresses from an IPAM system\u2019s address pool for hostnames in an orchestration environment. The F5 IPAM Controller watches orchestration-specific resources and consumes the hostnames within each resource.\n## Documentation\nRefer to F5 IPAM Controller documentation\n- https://github.com/F5Networks/f5-ipam-controller/blob/main/README.md\n## Prerequisites\n### Configure Infoblox with network and netview [Provider: Infoblox]\nRefer Infoblox documentation\n### Create persistent volume and persistent volume claim [Provider: f5-ip-provider]\nCreate persistent volume and persistent volume claim\n```\noc apply -f https://raw.githubusercontent.com/F5Networks/f5-ipam-controller/main/docs/config_examples/f5-ip-provider/localstorage-pv-pvc-example.yaml\n```\n",
      "csv_display_name": "F5 Ipam controller",
      "csv_metadata_description": "Operator to install F5 IPAM Controller.",
      "csv_name": "f5-ipam-controller-operator.v0.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:49:33.082000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "f5-ipam-controller-operator",
      "provided_apis": [
        {
          "group": "fic.f5.com",
          "kind": "F5IpamCtlr",
          "version": "v1"
        }
      ],
      "provider": "F5 Inc.",
      "related_images": [
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "f5-ipam-controller-operator-c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790-annotation"
        },
        {
          "digest": "sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.0.2",
      "version_original": "0.0.2"
    },
    {
      "_id": "630601ac53cb8f6da456d718",
      "alm_examples": [
        {
          "api_version": "fic.f5.com/v1",
          "kind": "F5IpamCtlr",
          "metadata": {
            "name": "f5ipamctlr-sample"
          },
          "spec": {
            "args": {
              "infoblox_grid_host": "<IP Address of Server>",
              "infoblox_labels": "<CIDR Labels>",
              "infoblox_netview": "<NetView>",
              "infoblox_password": "<PASSWORD>",
              "infoblox_username": "<USERNAME>",
              "infoblox_wapi_version": "<VERSION>",
              "insecure": true,
              "ip_range": "<IP RANGE LABELS>",
              "log_level": "<DEBUG_OR_INFO>",
              "orchestration": "kubernetes",
              "provider": "<infoblox_or_f5-ip-provider>"
            },
            "image": {
              "pullPolicy": "Always",
              "repo": "f5-ipam-controller",
              "user": "f5networks",
              "version": "latest"
            },
            "namespace": "kube-system",
            "rbac": {
              "create": true
            },
            "serviceAccount": {
              "create": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator-bundle@sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "bundle_path_digest": "sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T10:47:08.030000+00:00",
      "csv_description": "## Introduction\nThis Operator installs F5 IPAM Controller in your OpenShift Cluster. This enables to configure and deploy FIC using Helm Charts.\n## F5 IPAM Controller\nThe F5 IPAM Controller is a Docker container that runs in an orchestration environment and interfaces with an IPAM system. It allocates IP addresses from an IPAM system\u2019s address pool for hostnames in an orchestration environment. The F5 IPAM Controller watches orchestration-specific resources and consumes the hostnames within each resource.\n## Documentation\nRefer to F5 IPAM Controller documentation\n- https://github.com/F5Networks/f5-ipam-controller/blob/main/README.md\n## Prerequisites\n### Configure Infoblox with network and netview [Provider: Infoblox]\nRefer Infoblox documentation\n### Create persistent volume and persistent volume claim [Provider: f5-ip-provider]\nCreate persistent volume and persistent volume claim\n```\noc apply -f https://raw.githubusercontent.com/F5Networks/f5-ipam-controller/main/docs/config_examples/f5-ip-provider/localstorage-pv-pvc-example.yaml\n```\n",
      "csv_display_name": "F5 Ipam controller",
      "csv_metadata_description": "Operator to install F5 IPAM Controller.",
      "csv_name": "f5-ipam-controller-operator.v0.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:26:58.468000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "f5-ipam-controller-operator",
      "provided_apis": [
        {
          "group": "fic.f5.com",
          "kind": "F5IpamCtlr",
          "version": "v1"
        }
      ],
      "provider": "F5 Inc.",
      "related_images": [
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "f5-ipam-controller-operator-c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790-annotation"
        },
        {
          "digest": "sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.0.2",
      "version_original": "0.0.2"
    },
    {
      "_id": "630603b653cb8f6da456d78b",
      "alm_examples": [
        {
          "api_version": "fic.f5.com/v1",
          "kind": "F5IpamCtlr",
          "metadata": {
            "name": "f5ipamctlr-sample"
          },
          "spec": {
            "args": {
              "infoblox_grid_host": "<IP Address of Server>",
              "infoblox_labels": "<CIDR Labels>",
              "infoblox_netview": "<NetView>",
              "infoblox_password": "<PASSWORD>",
              "infoblox_username": "<USERNAME>",
              "infoblox_wapi_version": "<VERSION>",
              "insecure": true,
              "ip_range": "<IP RANGE LABELS>",
              "log_level": "<DEBUG_OR_INFO>",
              "orchestration": "kubernetes",
              "provider": "<infoblox_or_f5-ip-provider>"
            },
            "image": {
              "pullPolicy": "Always",
              "repo": "f5-ipam-controller",
              "user": "f5networks",
              "version": "latest"
            },
            "namespace": "kube-system",
            "rbac": {
              "create": true
            },
            "serviceAccount": {
              "create": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator-bundle@sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "bundle_path_digest": "sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T10:55:50.362000+00:00",
      "csv_description": "## Introduction\nThis Operator installs F5 IPAM Controller in your OpenShift Cluster. This enables to configure and deploy FIC using Helm Charts.\n## F5 IPAM Controller\nThe F5 IPAM Controller is a Docker container that runs in an orchestration environment and interfaces with an IPAM system. It allocates IP addresses from an IPAM system\u2019s address pool for hostnames in an orchestration environment. The F5 IPAM Controller watches orchestration-specific resources and consumes the hostnames within each resource.\n## Documentation\nRefer to F5 IPAM Controller documentation\n- https://github.com/F5Networks/f5-ipam-controller/blob/main/README.md\n## Prerequisites\n### Configure Infoblox with network and netview [Provider: Infoblox]\nRefer Infoblox documentation\n### Create persistent volume and persistent volume claim [Provider: f5-ip-provider]\nCreate persistent volume and persistent volume claim\n```\noc apply -f https://raw.githubusercontent.com/F5Networks/f5-ipam-controller/main/docs/config_examples/f5-ip-provider/localstorage-pv-pvc-example.yaml\n```\n",
      "csv_display_name": "F5 Ipam controller",
      "csv_metadata_description": "Operator to install F5 IPAM Controller.",
      "csv_name": "f5-ipam-controller-operator.v0.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:15:01.307000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.6",
      "organization": "certified-operators",
      "package": "f5-ipam-controller-operator",
      "provided_apis": [
        {
          "group": "fic.f5.com",
          "kind": "F5IpamCtlr",
          "version": "v1"
        }
      ],
      "provider": "F5 Inc.",
      "related_images": [
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "f5-ipam-controller-operator-c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790-annotation"
        },
        {
          "digest": "sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.6",
      "version": "0.0.2",
      "version_original": "0.0.2"
    },
    {
      "_id": "630603d2a96406a9dbe65d1a",
      "alm_examples": [
        {
          "api_version": "fic.f5.com/v1",
          "kind": "F5IpamCtlr",
          "metadata": {
            "name": "f5ipamctlr-sample"
          },
          "spec": {
            "args": {
              "infoblox_grid_host": "<IP Address of Server>",
              "infoblox_labels": "<CIDR Labels>",
              "infoblox_netview": "<NetView>",
              "infoblox_password": "<PASSWORD>",
              "infoblox_username": "<USERNAME>",
              "infoblox_wapi_version": "<VERSION>",
              "insecure": true,
              "ip_range": "<IP RANGE LABELS>",
              "log_level": "<DEBUG_OR_INFO>",
              "orchestration": "kubernetes",
              "provider": "<infoblox_or_f5-ip-provider>"
            },
            "image": {
              "pullPolicy": "Always",
              "repo": "f5-ipam-controller",
              "user": "f5networks",
              "version": "latest"
            },
            "namespace": "kube-system",
            "rbac": {
              "create": true
            },
            "serviceAccount": {
              "create": true
            }
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator-bundle@sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "bundle_path_digest": "sha256:4a81ea03e4394f48a126e5af811568a7eccd5825a40e9e36ea53e259b3e9f263",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T10:56:18.761000+00:00",
      "csv_description": "## Introduction\nThis Operator installs F5 IPAM Controller in your OpenShift Cluster. This enables to configure and deploy FIC using Helm Charts.\n## F5 IPAM Controller\nThe F5 IPAM Controller is a Docker container that runs in an orchestration environment and interfaces with an IPAM system. It allocates IP addresses from an IPAM system\u2019s address pool for hostnames in an orchestration environment. The F5 IPAM Controller watches orchestration-specific resources and consumes the hostnames within each resource.\n## Documentation\nRefer to F5 IPAM Controller documentation\n- https://github.com/F5Networks/f5-ipam-controller/blob/main/README.md\n## Prerequisites\n### Configure Infoblox with network and netview [Provider: Infoblox]\nRefer Infoblox documentation\n### Create persistent volume and persistent volume claim [Provider: f5-ip-provider]\nCreate persistent volume and persistent volume claim\n```\noc apply -f https://raw.githubusercontent.com/F5Networks/f5-ipam-controller/main/docs/config_examples/f5-ip-provider/localstorage-pv-pvc-example.yaml\n```\n",
      "csv_display_name": "F5 Ipam controller",
      "csv_metadata_description": "Operator to install F5 IPAM Controller.",
      "csv_name": "f5-ipam-controller-operator.v0.0.2",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:48:37.283000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.7",
      "organization": "certified-operators",
      "package": "f5-ipam-controller-operator",
      "provided_apis": [
        {
          "group": "fic.f5.com",
          "kind": "F5IpamCtlr",
          "version": "v1"
        }
      ],
      "provider": "F5 Inc.",
      "related_images": [
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "f5-ipam-controller-operator-c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790-annotation"
        },
        {
          "digest": "sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:076093ea58ca42a77839d911193977a7985728607187f31d3652ce189d79900d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "image": "registry.connect.redhat.com/f5networks/f5-ipam-controller-operator@sha256:c3861f997021a914fdcb8e21cb15cde0414bc22974b756ecc2d051c1a3adb790",
          "name": "manager"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.7",
      "version": "0.0.2",
      "version_original": "0.0.2"
    },
    {
      "_id": "63062ffcf7e1f4301c72071d",
      "alm_examples": [
        {
          "api_version": "locd.lenovo.com/v1alpha1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample",
            "namespace": "locd-operator-system"
          },
          "spec": {
            "affinity": {},
            "argocd": {
              "password": "xxxxxxxxxxx",
              "server": "argocd.svc.cluster.local",
              "user": "admin"
            },
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "core": {
              "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
              "pullPolicy": "Always"
            },
            "fullnameOverride": "",
            "global": {
              "managementNamespace": "loc-devops",
              "serviceEndpoint": "loc-devops.domain",
              "serviceaccount": {
                "annotations": {
                  "managedBy": "LOC DevOps"
                },
                "create": true,
                "name": "loc-devops-admin"
              }
            },
            "gui": {
              "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
              "pullPolicy": "Always"
            },
            "imagePullSecrets": [],
            "ingress": {
              "annotations": {},
              "className": "nginx",
              "enabled": false,
              "hosts": [
                {
                  "host": "loc-devops.domain",
                  "paths": [
                    {
                      "path": "/",
                      "pathType": "Prefix",
                      "servicePort": 8080
                    },
                    {
                      "path": "/api",
                      "pathType": "Prefix",
                      "servicePort": 8000
                    },
                    {
                      "path": "/swagger",
                      "pathType": "Prefix",
                      "servicePort": 8000
                    }
                  ]
                }
              ],
              "tls": []
            },
            "nameOverride": "",
            "nodeSelector": {},
            "podAnnotations": {},
            "podSecurityContext": {},
            "replicaCount": 1,
            "resources": {},
            "securityContext": {},
            "service": {
              "port": {
                "core": 8000,
                "gui": 8080
              },
              "type": "ClusterIP"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/loc-devops/operator-bundle@sha256:b9f7a678ceec2d367d55499a48a15a9dd729bc34b0e707989be23c97fc3c5f4e",
      "bundle_path_digest": "sha256:b9f7a678ceec2d367d55499a48a15a9dd729bc34b0e707989be23c97fc3c5f4e",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T14:04:44.256000+00:00",
      "csv_description": "This is operator used to manage components of Lenovo Open Cloud DevOps",
      "csv_display_name": "lenovo-locd-operator",
      "csv_metadata_description": "",
      "csv_name": "lenovo-locd-operator.v0.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:21:15.354000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "lenovo-locd-operator",
      "provided_apis": [
        {
          "group": "locd.lenovo.com",
          "kind": "Cluster",
          "version": "v1alpha1"
        }
      ],
      "provider": "Lenovo (Shanghai) Computer Technology Co., Ltd.",
      "related_images": [
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "image": "registry.connect.redhat.com/loc-devops/operator@sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "name": "operator"
        },
        {
          "digest": "sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "name": "core"
        },
        {
          "digest": "sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "name": "gui"
        },
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "image": "registry.connect.redhat.com/loc-devops/operator@sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "name": "manager"
        },
        {
          "digest": "sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "name": "gui-65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44-annotation"
        },
        {
          "digest": "sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "name": "core-f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "0.1.0",
      "version_original": "0.1.0"
    },
    {
      "_id": "630630458375780b95729a55",
      "alm_examples": [
        {
          "api_version": "locd.lenovo.com/v1alpha1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample",
            "namespace": "locd-operator-system"
          },
          "spec": {
            "affinity": {},
            "argocd": {
              "password": "xxxxxxxxxxx",
              "server": "argocd.svc.cluster.local",
              "user": "admin"
            },
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "core": {
              "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
              "pullPolicy": "Always"
            },
            "fullnameOverride": "",
            "global": {
              "managementNamespace": "loc-devops",
              "serviceEndpoint": "loc-devops.domain",
              "serviceaccount": {
                "annotations": {
                  "managedBy": "LOC DevOps"
                },
                "create": true,
                "name": "loc-devops-admin"
              }
            },
            "gui": {
              "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
              "pullPolicy": "Always"
            },
            "imagePullSecrets": [],
            "ingress": {
              "annotations": {},
              "className": "nginx",
              "enabled": false,
              "hosts": [
                {
                  "host": "loc-devops.domain",
                  "paths": [
                    {
                      "path": "/",
                      "pathType": "Prefix",
                      "servicePort": 8080
                    },
                    {
                      "path": "/api",
                      "pathType": "Prefix",
                      "servicePort": 8000
                    },
                    {
                      "path": "/swagger",
                      "pathType": "Prefix",
                      "servicePort": 8000
                    }
                  ]
                }
              ],
              "tls": []
            },
            "nameOverride": "",
            "nodeSelector": {},
            "podAnnotations": {},
            "podSecurityContext": {},
            "replicaCount": 1,
            "resources": {},
            "securityContext": {},
            "service": {
              "port": {
                "core": 8000,
                "gui": 8080
              },
              "type": "ClusterIP"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/loc-devops/operator-bundle@sha256:b9f7a678ceec2d367d55499a48a15a9dd729bc34b0e707989be23c97fc3c5f4e",
      "bundle_path_digest": "sha256:b9f7a678ceec2d367d55499a48a15a9dd729bc34b0e707989be23c97fc3c5f4e",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T14:05:57.587000+00:00",
      "csv_description": "This is operator used to manage components of Lenovo Open Cloud DevOps",
      "csv_display_name": "lenovo-locd-operator",
      "csv_metadata_description": "",
      "csv_name": "lenovo-locd-operator.v0.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:53:02.955000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "lenovo-locd-operator",
      "provided_apis": [
        {
          "group": "locd.lenovo.com",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1alpha1"
        }
      ],
      "provider": "Lenovo (Shanghai) Computer Technology Co., Ltd.",
      "related_images": [
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "image": "registry.connect.redhat.com/loc-devops/operator@sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "name": "operator"
        },
        {
          "digest": "sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "name": "core"
        },
        {
          "digest": "sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "name": "gui"
        },
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "image": "registry.connect.redhat.com/loc-devops/operator@sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "name": "manager"
        },
        {
          "digest": "sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "name": "gui-65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44-annotation"
        },
        {
          "digest": "sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "name": "core-f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "0.1.0",
      "version_original": "0.1.0"
    },
    {
      "_id": "630633258375780b95729b55",
      "alm_examples": [
        {
          "api_version": "locd.lenovo.com/v1alpha1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample",
            "namespace": "locd-operator-system"
          },
          "spec": {
            "affinity": {},
            "argocd": {
              "password": "xxxxxxxxxxx",
              "server": "argocd.svc.cluster.local",
              "user": "admin"
            },
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "core": {
              "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
              "pullPolicy": "Always"
            },
            "fullnameOverride": "",
            "global": {
              "managementNamespace": "loc-devops",
              "serviceEndpoint": "loc-devops.domain",
              "serviceaccount": {
                "annotations": {
                  "managedBy": "LOC DevOps"
                },
                "create": true,
                "name": "loc-devops-admin"
              }
            },
            "gui": {
              "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
              "pullPolicy": "Always"
            },
            "imagePullSecrets": [],
            "ingress": {
              "annotations": {},
              "className": "nginx",
              "enabled": false,
              "hosts": [
                {
                  "host": "loc-devops.domain",
                  "paths": [
                    {
                      "path": "/",
                      "pathType": "Prefix",
                      "servicePort": 8080
                    },
                    {
                      "path": "/api",
                      "pathType": "Prefix",
                      "servicePort": 8000
                    },
                    {
                      "path": "/swagger",
                      "pathType": "Prefix",
                      "servicePort": 8000
                    }
                  ]
                }
              ],
              "tls": []
            },
            "nameOverride": "",
            "nodeSelector": {},
            "podAnnotations": {},
            "podSecurityContext": {},
            "replicaCount": 1,
            "resources": {},
            "securityContext": {},
            "service": {
              "port": {
                "core": 8000,
                "gui": 8080
              },
              "type": "ClusterIP"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/loc-devops/operator-bundle@sha256:b9f7a678ceec2d367d55499a48a15a9dd729bc34b0e707989be23c97fc3c5f4e",
      "bundle_path_digest": "sha256:b9f7a678ceec2d367d55499a48a15a9dd729bc34b0e707989be23c97fc3c5f4e",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T14:18:13.717000+00:00",
      "csv_description": "This is operator used to manage components of Lenovo Open Cloud DevOps",
      "csv_display_name": "lenovo-locd-operator",
      "csv_metadata_description": "",
      "csv_name": "lenovo-locd-operator.v0.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:45:04.537000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.8",
      "organization": "certified-operators",
      "package": "lenovo-locd-operator",
      "provided_apis": [
        {
          "group": "locd.lenovo.com",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1alpha1"
        }
      ],
      "provider": "Lenovo (Shanghai) Computer Technology Co., Ltd.",
      "related_images": [
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "image": "registry.connect.redhat.com/loc-devops/operator@sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "name": "operator"
        },
        {
          "digest": "sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "name": "core"
        },
        {
          "digest": "sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "name": "gui"
        },
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "image": "registry.connect.redhat.com/loc-devops/operator@sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "name": "manager"
        },
        {
          "digest": "sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "name": "gui-65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44-annotation"
        },
        {
          "digest": "sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "name": "core-f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.8",
      "version": "0.1.0",
      "version_original": "0.1.0"
    },
    {
      "_id": "630634398375780b95729bbf",
      "alm_examples": [
        {
          "api_version": "locd.lenovo.com/v1alpha1",
          "kind": "Cluster",
          "metadata": {
            "name": "cluster-sample",
            "namespace": "locd-operator-system"
          },
          "spec": {
            "affinity": {},
            "argocd": {
              "password": "xxxxxxxxxxx",
              "server": "argocd.svc.cluster.local",
              "user": "admin"
            },
            "autoscaling": {
              "enabled": false,
              "maxReplicas": 100,
              "minReplicas": 1,
              "targetCPUUtilizationPercentage": 80
            },
            "core": {
              "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
              "pullPolicy": "Always"
            },
            "fullnameOverride": "",
            "global": {
              "managementNamespace": "loc-devops",
              "serviceEndpoint": "loc-devops.domain",
              "serviceaccount": {
                "annotations": {
                  "managedBy": "LOC DevOps"
                },
                "create": true,
                "name": "loc-devops-admin"
              }
            },
            "gui": {
              "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
              "pullPolicy": "Always"
            },
            "imagePullSecrets": [],
            "ingress": {
              "annotations": {},
              "className": "nginx",
              "enabled": false,
              "hosts": [
                {
                  "host": "loc-devops.domain",
                  "paths": [
                    {
                      "path": "/",
                      "pathType": "Prefix",
                      "servicePort": 8080
                    },
                    {
                      "path": "/api",
                      "pathType": "Prefix",
                      "servicePort": 8000
                    },
                    {
                      "path": "/swagger",
                      "pathType": "Prefix",
                      "servicePort": 8000
                    }
                  ]
                }
              ],
              "tls": []
            },
            "nameOverride": "",
            "nodeSelector": {},
            "podAnnotations": {},
            "podSecurityContext": {},
            "replicaCount": 1,
            "resources": {},
            "securityContext": {},
            "service": {
              "port": {
                "core": 8000,
                "gui": 8080
              },
              "type": "ClusterIP"
            },
            "tolerations": []
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/loc-devops/operator-bundle@sha256:b9f7a678ceec2d367d55499a48a15a9dd729bc34b0e707989be23c97fc3c5f4e",
      "bundle_path_digest": "sha256:b9f7a678ceec2d367d55499a48a15a9dd729bc34b0e707989be23c97fc3c5f4e",
      "capabilities": [
        "Basic Install"
      ],
      "channel_name": "alpha",
      "creation_date": "2022-08-24T14:22:49.439000+00:00",
      "csv_description": "This is operator used to manage components of Lenovo Open Cloud DevOps",
      "csv_display_name": "lenovo-locd-operator",
      "csv_metadata_description": "",
      "csv_name": "lenovo-locd-operator.v0.1.0",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": false,
          "type": "OwnNamespace"
        },
        {
          "supported": false,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:58:25.868000+00:00",
      "latest_in_channel": true,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "lenovo-locd-operator",
      "provided_apis": [
        {
          "group": "locd.lenovo.com",
          "kind": "Cluster",
          "plural": "clusters",
          "version": "v1alpha1"
        }
      ],
      "provider": "Lenovo (Shanghai) Computer Technology Co., Ltd.",
      "related_images": [
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "ose-kube-rbac-proxy"
        },
        {
          "digest": "sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "image": "registry.connect.redhat.com/loc-devops/operator@sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "name": "operator"
        },
        {
          "digest": "sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "name": "core"
        },
        {
          "digest": "sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "name": "gui"
        },
        {
          "digest": "sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "image": "registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:b5786bbbef725badf3dfcc2c2c7a86ead5ebb584c978c47aae8b9a62e241b80d",
          "name": "kube-rbac-proxy"
        },
        {
          "digest": "sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "image": "registry.connect.redhat.com/loc-devops/operator@sha256:4c17a3688ae3098e4b5c2c0530f49b1da8dedcc6fc1d3cd848513a1f189812a7",
          "name": "manager"
        },
        {
          "digest": "sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "image": "registry.connect.redhat.com/loc-devops/gui@sha256:65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44",
          "name": "gui-65142a36095188e879467038936b7b6b6ee39dcea6d42a437fa7066fb2bc0f44-annotation"
        },
        {
          "digest": "sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "image": "registry.connect.redhat.com/loc-devops/core@sha256:f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609",
          "name": "core-f771217b3f7849e1b63cc6abb93b8b0fc9636fd4a201e31807a15f773138c609-annotation"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "0.1.0",
      "version_original": "0.1.0"
    },
    {
      "_id": "63063a2c8375780b95729db1",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:9d154681aceb8b1e12056a3ff63591736e97580c43cd25d735cee6940bd05633",
      "bundle_path_digest": "sha256:9d154681aceb8b1e12056a3ff63591736e97580c43cd25d735cee6940bd05633",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-08-24T14:48:12.939000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:23:37.870000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.11",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "postgres-operator-1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec-annotation"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "operator"
        },
        {
          "digest": "sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.11",
      "version": "5.1.3",
      "version_original": "5.1.3"
    },
    {
      "_id": "63063bfa44c5b8e658fc8320",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:9d154681aceb8b1e12056a3ff63591736e97580c43cd25d735cee6940bd05633",
      "bundle_path_digest": "sha256:9d154681aceb8b1e12056a3ff63591736e97580c43cd25d735cee6940bd05633",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-08-24T14:55:54.005000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T12:03:45.857000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.9",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "postgres-operator-1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec-annotation"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "operator"
        },
        {
          "digest": "sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.9",
      "version": "5.1.3",
      "version_original": "5.1.3"
    },
    {
      "_id": "63063ca6f62197703e941b75",
      "alm_examples": [
        {
          "api_version": "postgres-operator.crunchydata.com/v1beta1",
          "kind": "PostgresCluster",
          "metadata": {
            "name": "example"
          },
          "spec": {
            "instances": [
              {
                "dataVolumeClaimSpec": {
                  "accessModes": [
                    "ReadWriteOnce"
                  ],
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                },
                "replicas": 1
              }
            ],
            "postgresVersion": 13
          }
        }
      ],
      "annotations": {
        "infrastructure_features": [],
        "valid_subscription": []
      },
      "architectures": [],
      "bundle_path": "registry.connect.redhat.com/crunchydata/postgres-operator-bundle@sha256:9d154681aceb8b1e12056a3ff63591736e97580c43cd25d735cee6940bd05633",
      "bundle_path_digest": "sha256:9d154681aceb8b1e12056a3ff63591736e97580c43cd25d735cee6940bd05633",
      "capabilities": [
        "Auto Pilot"
      ],
      "channel_name": "v5",
      "creation_date": "2022-08-24T14:58:46.542000+00:00",
      "csv_description": "[PGO](https://github.com/CrunchyData/postgres-operator), the\n[Postgres Operator](https://github.com/CrunchyData/postgres-operator) from\n[Crunchy Data](https://www.crunchydata.com), gives you a **declarative Postgres** solution that\nautomatically manages your [PostgreSQL](https://www.postgresql.org) clusters.\n\nDesigned for your GitOps workflows, it is [easy to get started](https://access.crunchydata.com/documentation/postgres-operator/v5/quickstart/)\nwith Postgres on Kubernetes with PGO. Within a few moments, you can have a production grade Postgres\ncluster complete with high availability, disaster recovery, and monitoring, all over secure TLS communications.\nEven better, PGO lets you easily customize your Postgres cluster to tailor it to your workload!\n\nWith conveniences like cloning Postgres clusters to using rolling updates to roll out disruptive\nchanges with minimal downtime, PGO is ready to support your Postgres data at every stage of your\nrelease pipeline. Built for resiliency and uptime, PGO will keep your desired Postgres in a desired\nstate so you do not need to worry about it.\n\nPGO is developed with many years of production experience in automating Postgres management on\nKubernetes, providing a seamless cloud native Postgres solution to keep your data always available.\n\n- **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],\n  while fully customizing your Pods and PostgreSQL configuration!\n- **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].\n  Uses [Pod Anti-Affinity][k8s-anti-affinity] to help resiliency; you can configure how aggressive this can be!\n  Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled\n  backups as well and set your backup retention policy\n- **Disaster Recovery**: [Backups][backups] and [restores][disaster-recovery] leverage the open source [pgBackRest][] utility and\n  [includes support for full, incremental, and differential backups as well as efficient delta restores][backups].\n  Set how long you want your backups retained for. Works great with very large databases!\n- **Monitoring**: [Track the health of your PostgreSQL clusters][monitoring] using the open source [pgMonitor][] library.\n- **Clone**: [Create new clusters from your existing clusters or backups][clone] with efficient data cloning.\n- **TLS**: All connections are over [TLS][tls]. You can also [bring your own TLS infrastructure][tls] if you do not want to use the provided defaults.\n- **Connection Pooling**: Advanced [connection pooling][pool] support using [pgBouncer][].\n- **Affinity and Tolerations**: Have your PostgreSQL clusters deployed to [Kubernetes Nodes][k8s-nodes] of your preference.\n  Set your [pod anti-affinity][k8s-anti-affinity], node affinity, Pod tolerations and more rules to customize your deployment topology!\n- **Full Customizability**: Crunchy PostgreSQL for Kubernetes makes it easy to get your own PostgreSQL-as-a-Service up and running\n  and fully customize your deployments, including:\n    - Choose the resources for your Postgres cluster: [container resources and storage size][resize-cluster]. [Resize at any time][resize-cluster] with minimal disruption.\n    - Use your own container image repository, including support `imagePullSecrets` and private repositories\n    - [Customize your PostgreSQL configuration][customize-cluster]\n\nand much more!\n\n[backups]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/backups/\n[clone]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/#clone-a-postgres-cluster\n[customize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/\n[disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/disaster-recovery/\n[high-availability]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/high-availability/\n[monitoring]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/monitoring/\n[pool]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[provisioning]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/create-cluster/\n[resize-cluster]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/resize-cluster/\n[tls]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/customize-cluster/#customize-tls\n\n[k8s-anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n[k8s-nodes]: https://kubernetes.io/docs/concepts/architecture/nodes/\n\n[pgBackRest]: https://www.pgbackrest.org\n[pgBouncer]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial/connection-pooling/\n[pgMonitor]: https://github.com/CrunchyData/pgmonitor\n\n\n## Post-Installation\n\n### Tutorial\n\nWant to [learn more about the PostgreSQL Operator][tutorial]? Browse through the [tutorial][] to learn more about what you can do!\n\n[tutorial]: https://access.crunchydata.com/documentation/postgres-operator/v5/tutorial",
      "csv_display_name": "Crunchy Postgres for Kubernetes",
      "csv_metadata_description": "Production Postgres Made Easy",
      "csv_name": "postgresoperator.v5.1.3",
      "in_index_img": true,
      "install_modes": [
        {
          "supported": true,
          "type": "OwnNamespace"
        },
        {
          "supported": true,
          "type": "SingleNamespace"
        },
        {
          "supported": false,
          "type": "MultiNamespace"
        },
        {
          "supported": true,
          "type": "AllNamespaces"
        }
      ],
      "is_default_channel": true,
      "last_update_date": "2022-09-19T11:58:18.378000+00:00",
      "latest_in_channel": false,
      "ocp_version": "4.10",
      "organization": "certified-operators",
      "package": "crunchy-postgres-operator",
      "provided_apis": [
        {
          "group": "postgres-operator.crunchydata.com",
          "kind": "PostgresCluster",
          "plural": "postgresclusters",
          "version": "v1beta1"
        }
      ],
      "provider": "Crunchy Data",
      "related_images": [
        {
          "digest": "sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "name": "PGADMIN"
        },
        {
          "digest": "sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "name": "PGBACKREST"
        },
        {
          "digest": "sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "name": "PGBOUNCER"
        },
        {
          "digest": "sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "name": "PGEXPORTER"
        },
        {
          "digest": "sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "name": "POSTGRES_13"
        },
        {
          "digest": "sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "name": "POSTGRES_14"
        },
        {
          "digest": "sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "name": "POSTGRES_13_GIS_3.0"
        },
        {
          "digest": "sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "name": "POSTGRES_13_GIS_3.1"
        },
        {
          "digest": "sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "name": "POSTGRES_14_GIS_3.1"
        },
        {
          "digest": "sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "name": "POSTGRES_14_GIS_3.2"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "postgres-operator"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "postgres-operator-1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec-annotation"
        },
        {
          "digest": "sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "image": "registry.connect.redhat.com/crunchydata/postgres-operator@sha256:1c3bab87a28a50c8314f51e13893839f895a41bea983bf20c46db7b30e3751ec",
          "name": "operator"
        },
        {
          "digest": "sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgadmin4@sha256:eb7c955a86eca26344811f045fca9ee1c69b62bba497b57f5a7292eab5599dad",
          "name": "pgadmin"
        },
        {
          "digest": "sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbackrest@sha256:ca99ce29ac0a4eb1a033d0d7a0f39e0f9d08573661ecec6641748ae4eb7fc00d",
          "name": "pgbackrest"
        },
        {
          "digest": "sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-pgbouncer@sha256:82e3fc35086f76d52c26d5c8c86427c5ecf0cfce42f971e4308ad7268fb688e0",
          "name": "pgbouncer"
        },
        {
          "digest": "sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-exporter@sha256:e09830db5cd48dddb6c2c5718115242d5c6a958f677c23bbfd6d822fe655ce58",
          "name": "pgexporter"
        },
        {
          "digest": "sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:988c8fb41d6667fcc4f05461f635b4109fffbb3e8f4a58f3e0a8db7e4f51201a",
          "name": "postgres_13"
        },
        {
          "digest": "sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres@sha256:4e506074f935bc6346fe2cb574ca15018ae901ef0365497965a2ca75d26d9ffc",
          "name": "postgres_14"
        },
        {
          "digest": "sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:246a1f978b7580921fb78d121d87754ba0e9be12902e4546823b15fe1da1a0b0",
          "name": "postgres_13_gis_3.0"
        },
        {
          "digest": "sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f29cdeaf84d51189d572450b702df82ad0970ac310c99888dbadc29d400b7d47",
          "name": "postgres_13_gis_3.1"
        },
        {
          "digest": "sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:f0c493dc681b8bdc54f012b05e87f143897e022210651de396ed1cd22851e9c1",
          "name": "postgres_14_gis_3.1"
        },
        {
          "digest": "sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "image": "registry.connect.redhat.com/crunchydata/crunchy-postgres-gis@sha256:8321084187156e90e7e08e0004717ecb14624573e76cec354b69b5cfd1222d89",
          "name": "postgres_14_gis_3.2"
        }
      ],
      "replaces": null,
      "skip_range": null,
      "skips": [],
      "source_index_container_path": "registry.redhat.io/redhat/certified-operator-index:v4.10",
      "version": "5.1.3",
      "version_original": "5.1.3"
    }
  ]
}
